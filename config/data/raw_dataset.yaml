# @package data.raw_dataset
generation:
  num_responses_per_problem: 5  # 각 문제당 8개 응답
  enable_thinking: false
  temperature: 0.7
  max_tokens: 10240 # max_model_len과 동일하게 설정
  logprobs: 5  # 성능 최적화: logprobs 비활성화 (필요시에만 활성화)
  top_p: 0.8
  top_k: 20
  min_p: 0.0
  presence_penalty: 0.0  # 0.0으로 완전히 비활성화하여 속도와 VRAM 활용률 개선
  
confidence:
  group_size: 512  # 토큰 그룹 크기
  methods:
    - "mean_group_confidence"
    - "bottom_10_percent_confidence" 
    - "tail_confidence"

vllm:
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.95  # 메모리 사용률 조정
  max_model_len: 12288 # 더 현실적인 길이로 조정 (18k → 12k)
  dtype: "auto"  # auto는 FP8 모델 사용 시 자동 처리, 일반 모델은 bfloat16
  kv_cache_dtype: "fp8"  # FP8 활성화 시 KV 캐시도 FP8로 명시적 설정
  trust_remote_code: true
  enforce_eager: false  # 메모리 효율성
  disable_custom_all_reduce: true  # 단일 GPU 최적화
  enable_prefix_caching: true  # 로그 분석: prefix caching이 활성되어 효율적 (동일 프롬프트 8개 응답)
  max_num_seqs: 100  # 동시 처리 성능 향상
  max_num_batched_tokens: 32768  # 배치 처리 최적화
  disable_log_stats: False  # vLLM 0.10.2에서는 지원 안 됨 (0.11.0+ 필요)

# Note: optimization 섹션은 vLLM이 지원하지 않으므로 제거됨
# 실제 메모리/성능 최적화는 위의 vllm 설정으로 충분히 달성됨:
# - kv_cache_dtype: fp8 (KV 캐시 FP8 압축)
# - max_model_len: 적절한 값 설정
# - logprobs: null (필요시에만 활성화)
