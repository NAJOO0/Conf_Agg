#!/bin/bash
# FlashInfer ì„¤ì¹˜ ë° vLLM í†µí•© ìŠ¤í¬ë¦½íŠ¸
# H100 GPUìš© ìµœì í™” (CUDA 12.8, SM 9.0)

set -e

echo "=========================================="
echo "FlashInfer + vLLM ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸"
echo "H100 GPU (SM 9.0) ìµœì í™” ë²„ì „"
echo "=========================================="
echo ""

# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo "1ï¸âƒ£ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •..."
export CUDA_HOME=/usr/local/cuda
export PATH="$CUDA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$CUDA_HOME/lib64:$LD_LIBRARY_PATH"

# FlashInfer ê´€ë ¨ í™˜ê²½ë³€ìˆ˜
export FLASHINFER_CUDA_ARCHITECTURES="90"  # H100 (SM 9.0)
export VLLM_ATTENTION_BACKEND="FLASHINFER"
export VLLM_USE_FLASHINFER=1

echo "   âœ“ CUDA_HOME: $CUDA_HOME"
echo "   âœ“ FlashInfer Architecture: SM 9.0 (H100)"

# 2. CUDA ë²„ì „ í™•ì¸
echo ""
echo "2ï¸âƒ£ CUDA í™˜ê²½ í™•ì¸..."
if command -v nvcc &> /dev/null; then
    CUDA_VERSION=$(nvcc --version | grep "release" | sed 's/.*release //' | sed 's/,.*//')
    echo "   âœ“ CUDA ë²„ì „: $CUDA_VERSION"
    
    # CUDA 12.x í™•ì¸
    if [[ ! "$CUDA_VERSION" =~ ^12\. ]]; then
        echo "   âš ï¸ ê²½ê³ : CUDA 12.xê°€ ê¶Œì¥ë©ë‹ˆë‹¤. í˜„ì¬: $CUDA_VERSION"
    fi
else
    echo "   âŒ nvccë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CUDAê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
    exit 1
fi

# 3. Python í™˜ê²½ í™œì„±í™” (ê¸°ì¡´ setup.sh ì°¸ê³ )
echo ""
echo "3ï¸âƒ£ Python í™˜ê²½ ì¤€ë¹„..."
cd /mnt/data1/projects/Conf_Agg

if [ -f ".venv/bin/activate" ]; then
    source .venv/bin/activate
    echo "   âœ“ ê¸°ì¡´ .venv í™œì„±í™”"
else
    echo "   - ìƒˆ .venv ìƒì„±..."
    uv venv --python 3.12
    source .venv/bin/activate
fi

PYTHON_VERSION=$(python --version | cut -d' ' -f2)
echo "   âœ“ Python ë²„ì „: $PYTHON_VERSION"

# 4. PyTorch ì„¤ì¹˜/í™•ì¸ (CUDA 12.8 ë²„ì „)
echo ""
echo "4ï¸âƒ£ PyTorch ì„¤ì¹˜/í™•ì¸..."
if python -c "import torch" 2>/dev/null; then
    TORCH_VERSION=$(python -c "import torch; print(torch.__version__)")
    echo "   âœ“ PyTorch ì´ë¯¸ ì„¤ì¹˜ë¨: $TORCH_VERSION"
else
    echo "   - PyTorch 2.5.1+cu128 ì„¤ì¹˜ ì¤‘..."
    uv pip install --index-url https://download.pytorch.org/whl/cu128 \
        torch==2.5.1+cu128 torchvision torchaudio
fi

# 5. FlashInfer ì„¤ì¹˜
echo ""
echo "5ï¸âƒ£ FlashInfer ì„¤ì¹˜..."

# FlashInfer ì˜ì¡´ì„±
echo "   - FlashInfer ì˜ì¡´ì„± ì„¤ì¹˜..."
uv pip install ninja packaging

# FlashInfer ë¹Œë“œ ë°©ë²• ì„ íƒ
echo ""
echo "   FlashInfer ì„¤ì¹˜ ë°©ë²• ì„ íƒ:"
echo "   1) Pre-built wheel ì‚¬ìš© (ë¹ ë¦„, ê¶Œì¥)"
echo "   2) ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ (ìµœì í™”, ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)"
read -p "   ì„ íƒ (1 ë˜ëŠ” 2): " INSTALL_METHOD

if [ "$INSTALL_METHOD" = "2" ]; then
    # ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ
    echo "   - FlashInferë¥¼ ì†ŒìŠ¤ì—ì„œ ë¹Œë“œí•©ë‹ˆë‹¤..."
    
    # ê¸°ì¡´ ì„¤ì¹˜ ì œê±°
    pip uninstall -y flashinfer 2>/dev/null || true
    
    # ì†ŒìŠ¤ í´ë¡ 
    if [ -d "/tmp/flashinfer" ]; then
        rm -rf /tmp/flashinfer
    fi
    
    git clone https://github.com/flashinfer-ai/flashinfer.git /tmp/flashinfer
    cd /tmp/flashinfer
    
    # H100 ìµœì í™” ë¹Œë“œ
    export TORCH_CUDA_ARCH_LIST="9.0"  # H100
    export MAX_JOBS=8  # ë³‘ë ¬ ì»´íŒŒì¼ ì‘ì—… ìˆ˜
    
    python setup.py install
    
    cd /mnt/data1/projects/Conf_Agg
    echo "   âœ“ FlashInfer ì†ŒìŠ¤ ë¹Œë“œ ì™„ë£Œ"
    
else
    # Pre-built wheel ì‚¬ìš©
    echo "   - Pre-built FlashInfer wheel ì„¤ì¹˜..."
    
    # CUDAì™€ Python ë²„ì „ì— ë§ëŠ” wheel ì„¤ì¹˜
    # FlashInferëŠ” íŠ¹ì • CUDA/Python ì¡°í•©ì˜ wheel ì œê³µ
    CUDA_VERSION_SHORT="cu128"  # CUDA 12.8
    PYTHON_VERSION_SHORT="cp312"  # Python 3.12
    
    # ì§ì ‘ wheel URL ë˜ëŠ” index ì‚¬ìš©
    uv pip install flashinfer -i https://flashinfer.ai/whl/cu128/torch2.5/
    
    echo "   âœ“ FlashInfer wheel ì„¤ì¹˜ ì™„ë£Œ"
fi

# 6. vLLM ì¬ì„¤ì¹˜ (FlashInfer ì§€ì› í¬í•¨)
echo ""
echo "6ï¸âƒ£ vLLM ì„¤ì¹˜ (FlashInfer ë°±ì—”ë“œ ì§€ì›)..."

# ê¸°ì¡´ vLLM ì œê±°
pip uninstall -y vllm vllm-flash-attn 2>/dev/null || true

# vLLM ì„¤ì¹˜ ì˜µì…˜
echo ""
echo "   vLLM ì„¤ì¹˜ ë°©ë²•:"
echo "   1) ê³µì‹ ë¦´ë¦¬ì¦ˆ (ì•ˆì •ì )"
echo "   2) ìµœì‹  ê°œë°œ ë²„ì „ (FlashInfer ìµœì‹  ì§€ì›)"
read -p "   ì„ íƒ (1 ë˜ëŠ” 2): " VLLM_METHOD

if [ "$VLLM_METHOD" = "2" ]; then
    # ê°œë°œ ë²„ì „
    echo "   - vLLM ìµœì‹  ë²„ì „ ì„¤ì¹˜..."
    uv pip install git+https://github.com/vllm-project/vllm.git
else
    # ê³µì‹ ë²„ì „
    echo "   - vLLM ê³µì‹ ë²„ì „ ì„¤ì¹˜..."
    uv pip install "vllm>=0.6.0"
fi

# 7. ì„¤ì¹˜ í™•ì¸
echo ""
echo "7ï¸âƒ£ ì„¤ì¹˜ í™•ì¸..."

# Python ìŠ¤í¬ë¦½íŠ¸ë¡œ í™•ì¸
cat > /tmp/test_flashinfer.py << 'EOF'
import sys
import torch

print("=" * 50)
print("FlashInfer + vLLM ì„¤ì¹˜ í™•ì¸")
print("=" * 50)

# PyTorch í™•ì¸
print(f"PyTorch: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# FlashInfer í™•ì¸
try:
    import flashinfer
    print(f"FlashInfer: ì„¤ì¹˜ë¨ (ë²„ì „: {flashinfer.__version__ if hasattr(flashinfer, '__version__') else 'unknown'})")
except ImportError as e:
    print(f"FlashInfer: ì„¤ì¹˜ ì‹¤íŒ¨ - {e}")

# vLLM í™•ì¸
try:
    import vllm
    print(f"vLLM: {vllm.__version__}")
    
    # vLLMì˜ attention backend í™•ì¸
    from vllm.attention.backends.flashinfer import FlashInferBackend
    print("vLLM FlashInfer Backend: ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"vLLM ë˜ëŠ” FlashInfer Backend ì˜¤ë¥˜: {e}")

print("=" * 50)

# ê°„ë‹¨í•œ ë²¤ì¹˜ë§ˆí¬
if torch.cuda.is_available():
    print("\nê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
    
    # FlashInfer í…ŒìŠ¤íŠ¸
    try:
        import flashinfer
        
        batch_size = 32
        seq_len = 2048
        num_heads = 32
        head_dim = 128
        
        # ëœë¤ í…ì„œ ìƒì„±
        q = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
        k = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
        v = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
        
        # Warmup
        for _ in range(3):
            with torch.no_grad():
                out = torch.nn.functional.scaled_dot_product_attention(q, k, v)
        
        torch.cuda.synchronize()
        
        print(f"í…ŒìŠ¤íŠ¸ ì„±ê³µ: batch={batch_size}, seq_len={seq_len}")
        print("FlashInferê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
EOF

python /tmp/test_flashinfer.py

# 8. vLLM ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
echo ""
echo "8ï¸âƒ£ vLLM ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±..."

cat > /mnt/data1/projects/Conf_Agg/run_vllm_flashinfer.sh << 'EOF'
#!/bin/bash
# vLLM with FlashInfer ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export CUDA_HOME=/usr/local/cuda
export PATH="$CUDA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$CUDA_HOME/lib64:$LD_LIBRARY_PATH"

# FlashInfer ë°±ì—”ë“œ ê°•ì œ ì‚¬ìš©
export VLLM_ATTENTION_BACKEND=FLASHINFER

# Python í™˜ê²½ í™œì„±í™”
source .venv/bin/activate

echo "=========================================="
echo "vLLM ì„œë²„ ì‹œì‘ (FlashInfer Backend)"
echo "=========================================="

# vLLM ì„œë²„ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
    --model $1 \
    --port ${2:-8000} \
    --host 0.0.0.0 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 12288 \
    --dtype float16 \
    --max-num-seqs 256 \
    --max-num-batched-tokens 65536 \
    --enable-prefix-caching \
    --enable-chunked-prefill \
    --trust-remote-code \
    --disable-log-requests \
    2>&1 | tee vllm_flashinfer.log
EOF

chmod +x /mnt/data1/projects/Conf_Agg/run_vllm_flashinfer.sh

# 9. ì™„ë£Œ ë©”ì‹œì§€
echo ""
echo "=========================================="
echo "âœ… FlashInfer + vLLM ì„¤ì¹˜ ì™„ë£Œ!"
echo "=========================================="
echo ""
echo "ğŸ“‹ ì‚¬ìš© ë°©ë²•:"
echo ""
echo "1. vLLM ì„œë²„ ì‹¤í–‰ (FlashInfer ë°±ì—”ë“œ):"
echo "   cd /mnt/data1/projects/Conf_Agg"
echo "   ./run_vllm_flashinfer.sh 'Qwen/Qwen2.5-Math-1.5B-Instruct' 8000"
echo ""
echo "2. Pythonì—ì„œ ì§ì ‘ ì‚¬ìš©:"
echo "   export VLLM_ATTENTION_BACKEND=FLASHINFER"
echo "   python your_script.py"
echo ""
echo "âš ï¸  ì£¼ì˜ì‚¬í•­:"
echo "- H100 GPUì—ì„œ ìµœì  ì„±ëŠ¥"
echo "- CUDA 12.x í•„ìš”"
echo "- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê¸°ë³¸ attentionë³´ë‹¤ ì ìŒ"
echo "- íŠ¹íˆ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ ì„±ëŠ¥ í–¥ìƒ"
echo ""
echo "ğŸ”§ ë¬¸ì œ í•´ê²°:"
echo "- FlashInfer import ì˜¤ë¥˜ ì‹œ: pip install flashinfer --upgrade"
echo "- vLLM backend ì˜¤ë¥˜ ì‹œ: export VLLM_ATTENTION_BACKEND=FLASH_ATTN"
echo ""