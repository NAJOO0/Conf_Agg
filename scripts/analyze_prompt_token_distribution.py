#!/usr/bin/env python3
"""
Train/Validation ë°ì´í„°ì˜ Prompt Token Count ë¶„í¬ ë¶„ì„

ì£¼ìš” ê¸°ëŠ¥:
1. Train ë°ì´í„°ì˜ prompt token count ë¶„í¬ ë¶„ì„
2. Validation ë°ì´í„°ì˜ prompt token count ë¶„í¬ ë¶„ì„
3. ì „ì²´ ë°ì´í„°ì˜ prompt token count ë¶„í¬ ë¶„ì„
4. ì‹œê°í™” ë° í†µê³„ ì¶œë ¥
"""
import os
import sys
import pandas as pd
import argparse
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging
import warnings
from typing import Optional, List, Tuple
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# ê²½ê³  ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings('ignore', category=UserWarning)

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from transformers import AutoTokenizer


# TRL imports (chat template ì ìš©ìš©)
try:
    from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE, maybe_apply_chat_template
    HAS_TRL = True
except ImportError:
    HAS_TRL = False
    logger.warning("TRLì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. chat templateì„ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# PyArrow ê°€ìš©ì„± í™•ì¸
try:
    import pyarrow.parquet as pq
    HAS_PYARROW = True
except Exception:
    HAS_PYARROW = False


def load_tokenizer(model_name: str):
    """
    Tokenizer ë¡œë“œ ë° chat template ì„¤ì •
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
        
    Returns:
        tokenizer
    """
    logger.info(f"Tokenizer ë¡œë“œ ì¤‘: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        logger.info("âœ… Tokenizer ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"Tokenizer ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    # Chat template ì„¤ì • (stage3_train_2.pyì™€ ë™ì¼í•˜ê²Œ)
    if tokenizer.chat_template is None:
        if HAS_TRL:
            tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE
            logger.info("ğŸ“ ê¸°ë³¸ chat template ì ìš© (SIMPLE_CHAT_TEMPLATE)")
        else:
            logger.warning("âš ï¸ TRLì´ ì—†ì–´ chat templateì„ ì„¤ì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        logger.info("ğŸ“ ëª¨ë¸ì— ì´ë¯¸ chat templateì´ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    
    return tokenizer


def count_tokens_with_tokenizer(tokenizer, text: str, apply_chat_template: bool = False) -> int:
    """
    Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
    
    Args:
        tokenizer: tokenizer ì¸ìŠ¤í„´ìŠ¤
        text: í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ì´ë¯¸ chat templateì´ ì ìš©ëœ í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
        apply_chat_template: chat template ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
                           curation.pyì—ì„œ ì´ë¯¸ ì ìš©í–ˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ì€ False
        
    Returns:
        í† í° ìˆ˜
    """
    if pd.isna(text) or not text:
        return 0
    
    try:
        text_str = str(text)
        
        # chat template ì ìš© (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
        if apply_chat_template:
            text_str = tokenizer.apply_chat_template(
                [{"role": "user", "content": text_str}],
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=False,
            )
        
        # í† í°í™”
        if hasattr(tokenizer, 'encode'):
            tokens = tokenizer.encode(text_str, add_special_tokens=False)
            return len(tokens)
        elif hasattr(tokenizer, 'tokenize'):
            tokens = tokenizer.tokenize(text_str)
            return len(tokens)
        else:
            return len(text_str.split())
    except Exception as e:
        logger.warning(f"í† í° ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return len(str(text).split())


def load_parquet_file(file_path: str) -> Optional[pd.DataFrame]:
    """
    Parquet íŒŒì¼ì„ ì•ˆì •ì ìœ¼ë¡œ ë¡œë“œ
    
    Args:
        file_path: Parquet íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„ ë˜ëŠ” None
    """
    if not os.path.exists(file_path):
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return None
    
    if not file_path.endswith('.parquet'):
        logger.error(f"Parquet íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {file_path}")
        return None
    
    logger.info(f"íŒŒì¼ ë¡œë“œ ì¤‘: {file_path}")
    try:
        if HAS_PYARROW:
            try:
                table = pq.read_table(file_path, memory_map=False)
                df = table.to_pandas(types_mapper=pd.ArrowDtype)
            except Exception as e:
                logger.warning(f"PyArrow memory_map=False ì‹¤íŒ¨: {e}, memory_map=Trueë¡œ ì¬ì‹œë„...")
                try:
                    table = pq.read_table(file_path, memory_map=True)
                    df = table.to_pandas(types_mapper=pd.ArrowDtype)
                except Exception as e2:
                    logger.warning(f"PyArrow types_mapper ì‚¬ìš© ì‹¤íŒ¨: {e2}, ê¸°ë³¸ ë³€í™˜ìœ¼ë¡œ ì¬ì‹œë„...")
                    table = pq.read_table(file_path, memory_map=False)
                    df = table.to_pandas()
        else:
            df = pd.read_parquet(file_path)

        logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
        logger.info(f"ì»¬ëŸ¼: {df.columns.to_list()}")
        
        # string íƒ€ì…ì„ large_stringìœ¼ë¡œ ë³€í™˜ (offset overflow ë°©ì§€)
        if HAS_PYARROW:
            try:
                logger.info("string íƒ€ì…ì„ large_stringìœ¼ë¡œ ë³€í™˜ ì¤‘...")
                string_cols = df.select_dtypes(include=['string[pyarrow]']).columns
                if not string_cols.empty:
                    for col in string_cols:
                        try:
                            df[col] = df[col].astype('large_string[pyarrow]')
                        except Exception as ce:
                            logger.warning(f"'{col}' ì»¬ëŸ¼ large_string ë³€í™˜ ì‹¤íŒ¨: {ce}")
                else:
                    object_cols = df.select_dtypes(include=['object']).columns
                    for col in object_cols:
                        try:
                            if not df[col].empty and isinstance(df[col].dropna().iloc[0], str):
                                df[col] = df[col].astype('large_string[pyarrow]')
                        except Exception as oe:
                            logger.warning(f"'{col}' (object) ì»¬ëŸ¼ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {oe}")
            except Exception as e:
                logger.warning(f"large_string ë³€í™˜ ë‹¨ê³„ì—ì„œ ê²½ê³ : {e}")

        return df
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {file_path}, ì˜¤ë¥˜: {e}")
        return None


def calculate_prompt_token_counts(df: pd.DataFrame, tokenizer, apply_chat_template: bool = False) -> pd.DataFrame:
    """
    Promptì˜ token count ê³„ì‚°
    
    Args:
        df: ë°ì´í„°í”„ë ˆì„
        tokenizer: tokenizer ì¸ìŠ¤í„´ìŠ¤
        apply_chat_template: chat template ì ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
                           curation.pyì—ì„œ ì´ë¯¸ chat templateì„ ì ìš©í–ˆìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ì€ False
        
    Returns:
        prompt_token_count ì»¬ëŸ¼ì´ ì¶”ê°€ëœ ë°ì´í„°í”„ë ˆì„
    """
    if 'prompt' not in df.columns:
        logger.error("'prompt' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df
    
    # ë¡œê¹…
    if apply_chat_template:
        logger.info("Prompt token count ê³„ì‚° ì¤‘... (chat template ì¬ì ìš©)")
    else:
        logger.info("Prompt token count ê³„ì‚° ì¤‘... (ì´ë¯¸ ì ìš©ëœ chat template ì‚¬ìš©)")
    
    prompt_token_counts = []
    
    total = len(df)
    for idx, prompt in enumerate(df['prompt']):
        if idx % 100 == 0 and idx > 0:
            logger.info(f"ì§„í–‰ ì¤‘: {idx}/{total} ({idx/total*100:.1f}%)")
        
        token_count = count_tokens_with_tokenizer(tokenizer, prompt, apply_chat_template=apply_chat_template)
        prompt_token_counts.append(token_count)
    
    df['prompt_token_count'] = prompt_token_counts
    logger.info("Prompt token count ê³„ì‚° ì™„ë£Œ")
    
    return df


def print_statistics(token_counts: np.ndarray, dataset_name: str):
    """
    Token count í†µê³„ ì¶œë ¥
    
    Args:
        token_counts: í† í° ìˆ˜ ë°°ì—´
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"=== {dataset_name} Prompt Token Count í†µê³„ ===")
    logger.info(f"{'='*60}")
    
    non_zero_mask = token_counts > 0
    non_zero_counts = token_counts[non_zero_mask]
    
    logger.info(f"ì´ ë°ì´í„° ìˆ˜: {len(token_counts)}")
    logger.info(f"í† í°ì´ ìˆëŠ” ë°ì´í„° ìˆ˜: {non_zero_mask.sum()} ({non_zero_mask.sum()/len(token_counts)*100:.1f}%)")
    
    if len(non_zero_counts) > 0:
        logger.info(f"\nê¸°ë³¸ í†µê³„:")
        logger.info(f"  í‰ê·  í† í° ìˆ˜ (ì „ì²´): {token_counts.mean():.2f}")
        logger.info(f"  í‰ê·  í† í° ìˆ˜ (í† í° ìˆìŒ): {non_zero_counts.mean():.2f}")
        logger.info(f"  ì¤‘ì•™ê°’ (í† í° ìˆìŒ): {np.median(non_zero_counts):.2f}")
        logger.info(f"  í‘œì¤€í¸ì°¨ (í† í° ìˆìŒ): {non_zero_counts.std():.2f}")
        logger.info(f"  ìµœì†Œê°’ (í† í° ìˆìŒ): {non_zero_counts.min():.2f}")
        logger.info(f"  ìµœëŒ€ê°’ (í† í° ìˆìŒ): {non_zero_counts.max():.2f}")
        
        # ë¶„ìœ„ìˆ˜
        percentiles = [25, 50, 75, 80, 85, 90, 95, 99]
        logger.info(f"\në¶„ìœ„ìˆ˜ (í† í° ìˆìŒ):")
        for p in percentiles:
            logger.info(f"  {p}%: {np.percentile(non_zero_counts, p):.2f}")
        
        # êµ¬ê°„ë³„ ë¶„í¬
        logger.info(f"\ní† í° ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬:")
        max_val = non_zero_counts.max()
        if max_val < 1000:
            bins = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900, float('inf')]
            labels = ['0-100', '100-200', '200-300', '300-400', '400-500', 
                     '500-600', '600-700', '700-800', '800-900', '900+']
        elif max_val < 5000:
            bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, float('inf')]
            labels = ['0-500', '500-1K', '1K-1.5K', '1.5K-2K', '2K-2.5K', 
                     '2.5K-3K', '3K-3.5K', '3.5K-4K', '4K+']
        else:
            bins = [0, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, float('inf')]
            labels = ['0-1K', '1K-2K', '2K-3K', '3K-4K', '4K-5K', 
                     '5K-6K', '6K-7K', '7K-8K', '8K+']
        
        for i in range(len(bins)-1):
            if i < len(bins) - 1:
                count = ((non_zero_counts >= bins[i]) & (non_zero_counts < bins[i+1])).sum()
            else:
                count = (non_zero_counts >= bins[i]).sum()
            percentage = count / len(non_zero_counts) * 100 if len(non_zero_counts) > 0 else 0
            logger.info(f"  {labels[i]}: {count}ê°œ ({percentage:.1f}%)")


def visualize_distribution(
    train_counts: np.ndarray,
    valid_counts: np.ndarray,
    all_counts: np.ndarray,
    output_dir: str
):
    """
    Token count ë¶„í¬ ì‹œê°í™”
    
    Args:
        train_counts: Train ë°ì´í„° í† í° ìˆ˜ ë°°ì—´
        valid_counts: Validation ë°ì´í„° í† í° ìˆ˜ ë°°ì—´
        all_counts: ì „ì²´ ë°ì´í„° í† í° ìˆ˜ ë°°ì—´
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
    
    try:
        # 1. íˆìŠ¤í† ê·¸ë¨ ë¹„êµ (Train vs Validation)
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # Train íˆìŠ¤í† ê·¸ë¨
        ax = axes[0, 0]
        train_non_zero = train_counts[train_counts > 0]
        if len(train_non_zero) > 0:
            ax.hist(train_non_zero, bins=50, alpha=0.7, color='#4C78A8', edgecolor='black')
            ax.set_xlabel('Prompt Token Count')
            ax.set_ylabel('Frequency')
            ax.set_title(f'Train Prompt Token Count Distribution\n(n={len(train_counts)}, mean={train_non_zero.mean():.1f})')
            ax.grid(True, alpha=0.3)
        
        # Validation íˆìŠ¤í† ê·¸ë¨
        ax = axes[0, 1]
        valid_non_zero = valid_counts[valid_counts > 0]
        if len(valid_non_zero) > 0:
            ax.hist(valid_non_zero, bins=50, alpha=0.7, color='#E45756', edgecolor='black')
            ax.set_xlabel('Prompt Token Count')
            ax.set_ylabel('Frequency')
            ax.set_title(f'Validation Prompt Token Count Distribution\n(n={len(valid_counts)}, mean={valid_non_zero.mean():.1f})')
            ax.grid(True, alpha=0.3)
        
        # ì „ì²´ íˆìŠ¤í† ê·¸ë¨
        ax = axes[1, 0]
        all_non_zero = all_counts[all_counts > 0]
        if len(all_non_zero) > 0:
            ax.hist(all_non_zero, bins=50, alpha=0.7, color='#54A24B', edgecolor='black')
            ax.set_xlabel('Prompt Token Count')
            ax.set_ylabel('Frequency')
            ax.set_title(f'All Prompt Token Count Distribution\n(n={len(all_counts)}, mean={all_non_zero.mean():.1f})')
            ax.grid(True, alpha=0.3)
        
        # ë¹„êµ Box Plot
        ax = axes[1, 1]
        data_to_plot = []
        labels = []
        if len(train_non_zero) > 0:
            data_to_plot.append(train_non_zero)
            labels.append('Train')
        if len(valid_non_zero) > 0:
            data_to_plot.append(valid_non_zero)
            labels.append('Validation')
        if len(all_non_zero) > 0:
            data_to_plot.append(all_non_zero)
            labels.append('All')
        
        if data_to_plot:
            bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True,
                          boxprops=dict(facecolor='lightblue', alpha=0.7))
            ax.set_ylabel('Prompt Token Count')
            ax.set_title('Prompt Token Count Comparison')
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, 'prompt_token_distribution.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"ì‹œê°í™” ì €ì¥: {plot_path}")
        
        # 2. CDF (Cumulative Distribution Function) ë¹„êµ
        fig, ax = plt.subplots(figsize=(10, 6))
        
        if len(train_non_zero) > 0:
            sorted_train = np.sort(train_non_zero)
            y_train = np.arange(1, len(sorted_train) + 1) / len(sorted_train)
            ax.plot(sorted_train, y_train, label='Train', linewidth=2, color='#4C78A8')
        
        if len(valid_non_zero) > 0:
            sorted_valid = np.sort(valid_non_zero)
            y_valid = np.arange(1, len(sorted_valid) + 1) / len(sorted_valid)
            ax.plot(sorted_valid, y_valid, label='Validation', linewidth=2, color='#E45756')
        
        if len(all_non_zero) > 0:
            sorted_all = np.sort(all_non_zero)
            y_all = np.arange(1, len(sorted_all) + 1) / len(sorted_all)
            ax.plot(sorted_all, y_all, label='All', linewidth=2, color='#54A24B')
        
        ax.set_xlabel('Prompt Token Count')
        ax.set_ylabel('Cumulative Probability')
        ax.set_title('Cumulative Distribution Function (CDF)')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = os.path.join(output_dir, 'prompt_token_cdf.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"CDF ì‹œê°í™” ì €ì¥: {plot_path}")
        
    except Exception as e:
        logger.warning(f"ì‹œê°í™” ìƒì„±/ì €ì¥ ì‹¤íŒ¨: {e}")


def main(
    train_path: str,
    validation_path: str,
    model_name: str,
    output_dir: Optional[str] = None,
    max_input_length: Optional[int] = None
):
    """
    ë©”ì¸ í•¨ìˆ˜: Prompt Token Count ë¶„í¬ ë¶„ì„
    
    Args:
        train_path: Train ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        validation_path: Validation ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        model_name: ëª¨ë¸ ì´ë¦„ (tokenizer ë¡œë“œìš©)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        max_input_length: ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì œí•œ (ì´ ê°’ì„ ë„˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì œê±°)
    """
    logger.info("\n" + "="*60)
    logger.info("=== Prompt Token Count ë¶„í¬ ë¶„ì„ ===")
    logger.info("="*60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_dir is None:
        output_dir = os.path.dirname(os.path.abspath(train_path))
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # 1. Tokenizer ë¡œë“œ
    try:
        tokenizer = load_tokenizer(model_name)
        # curation.pyì—ì„œ ì´ë¯¸ chat templateì„ ì ìš©í–ˆìœ¼ë¯€ë¡œ ì¬ì ìš©í•˜ì§€ ì•ŠìŒ
        logger.info("âœ… curation.pyì—ì„œ ì´ë¯¸ chat templateì´ ì ìš©ëœ promptë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        logger.info("   í† í° ìˆ˜ ê³„ì‚° ì‹œ chat templateì„ ì¬ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"Tokenizer ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # 2. ë°ì´í„° ë¡œë“œ
    logger.info("\në°ì´í„° ë¡œë“œ ì¤‘...")
    train_df = load_parquet_file(train_path)
    if train_df is None:
        logger.error("Train ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    valid_df = load_parquet_file(validation_path)
    if valid_df is None:
        logger.error("Validation ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # 3. Prompt token count ê³„ì‚°
    # curation.pyì—ì„œ ì´ë¯¸ chat templateì´ ì ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ ì¬ì ìš©í•˜ì§€ ì•ŠìŒ
    logger.info("\n" + "="*60)
    logger.info("Train ë°ì´í„° Prompt Token Count ê³„ì‚° ì¤‘...")
    train_df = calculate_prompt_token_counts(train_df, tokenizer, apply_chat_template=False)
    
    logger.info("\n" + "="*60)
    logger.info("Validation ë°ì´í„° Prompt Token Count ê³„ì‚° ì¤‘...")
    valid_df = calculate_prompt_token_counts(valid_df, tokenizer, apply_chat_template=False)
    
    # 3-1. max_input_length ì œí•œ ì ìš© (í•„í„°ë§)
    if max_input_length is not None:
        logger.info("\n" + "="*60)
        logger.info(f"Max Input Length ì œí•œ ì ìš©: {max_input_length} í† í°")
        logger.info("="*60)
        
        train_before = len(train_df)
        valid_before = len(valid_df)
        
        train_df = train_df[train_df['prompt_token_count'] <= max_input_length].copy()
        valid_df = valid_df[valid_df['prompt_token_count'] <= max_input_length].copy()
        
        train_removed = train_before - len(train_df)
        valid_removed = valid_before - len(valid_df)
        total_removed = train_removed + valid_removed
        
        logger.info(f"Train: {train_removed}ê°œ ì œê±° ({train_removed/train_before*100:.2f}%)")
        logger.info(f"Validation: {valid_removed}ê°œ ì œê±° ({valid_removed/valid_before*100:.2f}%)")
        logger.info(f"ì „ì²´: {total_removed}ê°œ ì œê±° ({(total_removed/(train_before+valid_before)*100):.2f}%)")
        logger.info(f"ë‚¨ì€ ì¸ìŠ¤í„´ìŠ¤: Train {len(train_df)}ê°œ, Validation {len(valid_df)}ê°œ")
        
        #filtered train_df and valid_df ì €ì¥
        dir_name = '/mnt/data1/datasets/nlp/conf_agg/curated/'
        train_df.to_parquet(os.path.join(dir_name, 'train_filtered.parquet'))
        valid_df.to_parquet(os.path.join(dir_name, 'valid_filtered.parquet'))
        
    # 4. í†µê³„ ì¶œë ¥
    train_counts = train_df['prompt_token_count'].values
    valid_counts = valid_df['prompt_token_count'].values
    all_counts = np.concatenate([train_counts, valid_counts])
    
    print_statistics(train_counts, "Train")
    print_statistics(valid_counts, "Validation")
    print_statistics(all_counts, "All (Train + Validation)")
    
    # 5. ì‹œê°í™”
    visualize_distribution(train_counts, valid_counts, all_counts, output_dir)
    
    # 6. ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    try:
        summary_data = {
            'dataset': ['Train', 'Validation', 'All'],
            'total_samples': [len(train_counts), len(valid_counts), len(all_counts)],
            'mean': [
                train_counts[train_counts > 0].mean() if len(train_counts[train_counts > 0]) > 0 else 0,
                valid_counts[valid_counts > 0].mean() if len(valid_counts[valid_counts > 0]) > 0 else 0,
                all_counts[all_counts > 0].mean() if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'median': [
                np.median(train_counts[train_counts > 0]) if len(train_counts[train_counts > 0]) > 0 else 0,
                np.median(valid_counts[valid_counts > 0]) if len(valid_counts[valid_counts > 0]) > 0 else 0,
                np.median(all_counts[all_counts > 0]) if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'std': [
                train_counts[train_counts > 0].std() if len(train_counts[train_counts > 0]) > 0 else 0,
                valid_counts[valid_counts > 0].std() if len(valid_counts[valid_counts > 0]) > 0 else 0,
                all_counts[all_counts > 0].std() if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'min': [
                train_counts[train_counts > 0].min() if len(train_counts[train_counts > 0]) > 0 else 0,
                valid_counts[valid_counts > 0].min() if len(valid_counts[valid_counts > 0]) > 0 else 0,
                all_counts[all_counts > 0].min() if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'max': [
                train_counts[train_counts > 0].max() if len(train_counts[train_counts > 0]) > 0 else 0,
                valid_counts[valid_counts > 0].max() if len(valid_counts[valid_counts > 0]) > 0 else 0,
                all_counts[all_counts > 0].max() if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'p95': [
                np.percentile(train_counts[train_counts > 0], 95) if len(train_counts[train_counts > 0]) > 0 else 0,
                np.percentile(valid_counts[valid_counts > 0], 95) if len(valid_counts[valid_counts > 0]) > 0 else 0,
                np.percentile(all_counts[all_counts > 0], 95) if len(all_counts[all_counts > 0]) > 0 else 0
            ],
            'p99': [
                np.percentile(train_counts[train_counts > 0], 99) if len(train_counts[train_counts > 0]) > 0 else 0,
                np.percentile(valid_counts[valid_counts > 0], 99) if len(valid_counts[valid_counts > 0]) > 0 else 0,
                np.percentile(all_counts[all_counts > 0], 99) if len(all_counts[all_counts > 0]) > 0 else 0
            ]
        }
        summary_df = pd.DataFrame(summary_data)
        summary_path = os.path.join(output_dir, 'prompt_token_statistics.csv')
        summary_df.to_csv(summary_path, index=False)
        logger.info(f"\ní†µê³„ ìš”ì•½ ì €ì¥: {summary_path}")
    except Exception as e:
        logger.warning(f"í†µê³„ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    logger.info("\n" + "="*60)
    logger.info("âœ… ë¶„ì„ ì™„ë£Œ!")
    logger.info("="*60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train/Validation ë°ì´í„°ì˜ Prompt Token Count ë¶„í¬ ë¶„ì„")
    parser.add_argument("--train-path", type=str, default="/mnt/data1/datasets/nlp/conf_agg/curated/train_curated.parquet",
                       help="Train ë°ì´í„° Parquet íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--validation-path", type=str, default="/mnt/data1/datasets/nlp/conf_agg/curated/validation_curated.parquet",
                       help="Validation ë°ì´í„° Parquet íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model-name", type=str, default="Qwen/Qwen3-1.7B",
                       help="ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ (tokenizer ë¡œë“œìš©)")
    parser.add_argument("--output-dir", type=str, default="/mnt/data1/datasets/nlp/conf_agg/curated",
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: train íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬)")
    parser.add_argument("--max-input-length", type=int, default=8092,
                       help="ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì œí•œ (ì´ ê°’ì„ ë„˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ì œê±°, ì˜ˆ: 8092)")
    
    args = parser.parse_args()
    
    main(
        train_path=args.train_path,
        validation_path=args.validation_path,
        model_name=args.model_name,
        output_dir=args.output_dir,
        max_input_length=args.max_input_length
    )

