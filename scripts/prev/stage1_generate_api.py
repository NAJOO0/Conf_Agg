"""
Stage 1: API ê¸°ë°˜ ì›ì‹œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
vLLM API ì„œë²„ì— ë¹„ë™ê¸° ìš”ì²­ì„ ë³´ë‚´ê³  ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì €ì¥
"""
import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import asyncio
import aiohttp
import aiofiles
import json
from datetime import datetime
from tqdm.asyncio import tqdm
import time
from dataclasses import dataclass
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data.confidence import ConfidenceCalculator
from src.data.dataset import RawDataset
from src.utils.logging import setup_logging
from transformers import AutoTokenizer

logger = logging.getLogger(__name__)


@dataclass
class RequestData:
    """ìš”ì²­ ë°ì´í„° í´ë˜ìŠ¤"""
    problem_id: str
    problem_text: str
    ground_truth: str
    messages: List[Dict[str, str]]
    server_url: str
    retry_count: int = 0


def simple_extract_topk(gen_logprobs: List[Dict], k: int) -> List[List[float]]:
    """API ì‘ë‹µì—ì„œ logprob ì¶”ì¶œ"""
    if not gen_logprobs:
        return []
    
    results = []
    for token_info in gen_logprobs:
        if not token_info or 'top_logprobs' not in token_info:
            results.append([])
            continue
        
        top_logprobs = token_info['top_logprobs']
        if isinstance(top_logprobs, list):
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ logprobs
            lps = [item.get('logprob', 0.0) for item in top_logprobs[:k]]
        elif isinstance(top_logprobs, dict):
            # ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ logprobs
            lps = list(top_logprobs.values())[:k]
        else:
            lps = []
        
        if lps:
            results.append(np.array(lps, dtype=np.float16).tolist())
        else:
            results.append([])
    
    return results


async def make_api_request(
    session: aiohttp.ClientSession,
    request_data: RequestData,
    sampling_params: Dict,
    timeout: int = 900
) -> Optional[Dict]:
    """vLLM API ì„œë²„ì— ë‹¨ì¼ ìš”ì²­"""
    
    payload = {
        "model": sampling_params.get("model", "default"),
        "messages": request_data.messages,
        "temperature": sampling_params["temperature"],
        "top_p": sampling_params["top_p"],
        "top_k": sampling_params["top_k"],
        "max_tokens": sampling_params["max_tokens"],
        "n": sampling_params["n"],
        "logprobs": sampling_params["logprobs"],
        "presence_penalty": sampling_params.get("presence_penalty", 0.0),
        "stream": False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™” (ì™„ë£Œëœ ì‘ë‹µë§Œ ë°›ê¸°)
    }
    
    try:
        async with session.post(
            f"{request_data.server_url}/v1/chat/completions",
            json=payload,
            timeout=aiohttp.ClientTimeout(total=timeout)
        ) as response:
            if response.status == 200:
                result = await response.json()
                return {
                    "request_data": request_data,
                    "response": result
                }
            else:
                logger.warning(f"API ì˜¤ë¥˜ (ìƒíƒœ {response.status}): {request_data.problem_id}")
                return None
                
    except asyncio.TimeoutError:
        logger.warning(f"ìš”ì²­ íƒ€ì„ì•„ì›ƒ: {request_data.problem_id}")
        return None
    except Exception as e:
        logger.error(f"ìš”ì²­ ì‹¤íŒ¨: {request_data.problem_id} - {e}")
        return None


async def request_worker(
    session: aiohttp.ClientSession,
    request_queue: asyncio.Queue,
    result_queue: asyncio.Queue,
    sampling_params: Dict,
    max_retries: int = 3
):
    """ìš”ì²­ ì›Œì»¤ - íì—ì„œ ìš”ì²­ì„ ê°€ì ¸ì™€ ì²˜ë¦¬"""
    
    while True:
        try:
            request_data = await request_queue.get()
            
            if request_data is None:  # ì¢…ë£Œ ì‹ í˜¸
                break
            
            # API ìš”ì²­
            result = await make_api_request(session, request_data, sampling_params)
            
            if result is None and request_data.retry_count < max_retries:
                # ì¬ì‹œë„
                request_data.retry_count += 1
                await asyncio.sleep(1)  # ì ì‹œ ëŒ€ê¸°
                await request_queue.put(request_data)
            elif result:
                # ì„±ê³µ
                await result_queue.put(result)
            else:
                # ìµœì¢… ì‹¤íŒ¨
                logger.error(f"ìµœì¢… ì‹¤íŒ¨: {request_data.problem_id}")
                await result_queue.put({
                    "request_data": request_data,
                    "response": None,
                    "error": True
                })
            
            request_queue.task_done()
            
        except Exception as e:
            logger.error(f"ì›Œì»¤ ì˜¤ë¥˜: {e}")


async def result_processor(
    result_queue: asyncio.Queue,
    output_file: aiofiles.threadpool.AsyncTextIOWrapper,
    confidence_calculator: ConfidenceCalculator,
    gen_cfg_logprobs: int,
    args: argparse.Namespace,
    total: int
):
    """ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥ ì›Œì»¤"""
    
    processed_count = 0
    buffer = []
    buffer_size = 100
    
    pbar = tqdm(total=total, desc=f"Shard {args.shard_id}")
    
    while True:
        try:
            result = await asyncio.wait_for(result_queue.get(), timeout=1.0)
        except asyncio.TimeoutError:
            # ë²„í¼ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥
            if buffer and processed_count >= total:
                for item in buffer:
                    await output_file.write(json.dumps(item) + '\n')
                await output_file.flush()
                buffer.clear()
                break
            continue
        
        if result is None:  # ì¢…ë£Œ ì‹ í˜¸
            break
        
        request_data = result["request_data"]
        response = result.get("response")
        
        if response and "choices" in response:
            # ê° ì‘ë‹µ ì²˜ë¦¬
            for i, choice in enumerate(response["choices"]):
                # logprobs ì¶”ì¶œ
                logprobs_data = choice.get("logprobs", {})
                if logprobs_data and "content" in logprobs_data:
                    topk = simple_extract_topk(logprobs_data["content"], gen_cfg_logprobs)
                else:
                    topk = []
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence_scores = confidence_calculator.calculate_all_confidence_scores(topk)
                
                # ê²°ê³¼ ìƒì„± (chat/completions í˜¸í™˜)
                output_record = {
                    "problem_id": request_data.problem_id,
                    "problem_text": request_data.problem_text,
                    "ground_truth": request_data.ground_truth,
                    "response_id": f"{request_data.problem_id}_resp_{i}",
                    "generated_text": choice.get("message", {}).get("content", "") or choice.get("text", ""),
                    "output_token_count": choice.get("usage", {}).get("completion_tokens", 0),
                    "logprobs": topk,
                    "worker_gpu": args.gpu_id,
                    "worker_replica": f"shard_{args.shard_id}",
                    **confidence_scores,
                }
                
                buffer.append(output_record)
        
        processed_count += 1
        pbar.update(1)
        
        # ë²„í¼ê°€ ì°¨ë©´ ì €ì¥
        if len(buffer) >= buffer_size:
            for item in buffer:
                await output_file.write(json.dumps(item) + '\n')
            await output_file.flush()
            logger.info(f"[Shard {args.shard_id}] {processed_count}/{total} ì²˜ë¦¬ ì™„ë£Œ")
            buffer.clear()
        
        result_queue.task_done()
    
    # ë‚¨ì€ ë²„í¼ ì €ì¥
    if buffer:
        for item in buffer:
            await output_file.write(json.dumps(item) + '\n')
        await output_file.flush()
    
    pbar.close()


async def api_generation_worker(cfg: DictConfig, args: argparse.Namespace):
    """API ê¸°ë°˜ ë¹„ë™ê¸° ìƒì„± ì›Œì»¤"""
    
    # ë¡œê¹… ë””ë ‰í† ë¦¬ (ENV ìš°ì„ , ê¸°ë³¸ì€ config)
    log_dir = os.environ.get("LOG_DIR", getattr(cfg.paths, "log_dir", "/workspace/outputs/logs"))
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, f"stage1_api_shard_{args.shard_id}.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    logger = logging.getLogger("conf_agg_llm")
    logger.info(f"ğŸš€ [Shard {args.shard_id}] Stage 1: API ê¸°ë°˜ ë°ì´í„° ìƒì„± ì‹œì‘")
    
    try:
        # vLLM ì„œë²„ ì •ë³´ ë¡œë“œ (ì ˆëŒ€ ê²½ë¡œ ìš°ì„ )
        servers_json_path = os.environ.get("VLLM_SERVERS_JSON", "/workspace/vllm_servers.json")
        if not os.path.exists(servers_json_path):
            # í˜¸í™˜: í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ë„ í™•ì¸
            if os.path.exists('vllm_servers.json'):
                servers_json_path = 'vllm_servers.json'
            else:
                raise FileNotFoundError(f"vllm_servers.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {servers_json_path}")
        
        with open(servers_json_path, 'r') as f:
            server_info = json.load(f)
        
        servers = server_info['servers']
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë²„: {len(servers)}ê°œ")
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ (ENV ìš°ì„ )
        base_data_dir = os.environ.get("DATA_DIR", getattr(cfg.paths, "data_dir", "/workspace/outputs/data"))
        output_dir = os.path.join(base_data_dir, "generated")
        sample_limit_env = os.environ.get("SAMPLE_LIMIT", "")
        if sample_limit_env:
            output_dir = os.path.join(output_dir, f"sample_{sample_limit_env}")
        os.makedirs(output_dir, exist_ok=True)
        
        # ì›ë³¸ ë°ì´í„° ê²½ë¡œ (ENV RAW_DATA_PATHë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
        raw_data_path = os.environ.get("RAW_DATA_PATH", os.path.join(base_data_dir, "raw", "deepscaler.jsonl"))
        if not os.path.exists(raw_data_path):
            logger = logging.getLogger("conf_agg_llm")
            logger.error(f"ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_data_path}")
            raise FileNotFoundError(f"ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_data_path}")
        raw_dataset = RawDataset(raw_data_path)
        logger.info(f"ì „ì²´ ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # ìƒ˜í”Œë§ ë° ìƒ¤ë”© (ê¸°ì¡´ê³¼ ë™ì¼)
        sample_limit = int(sample_limit_env) if sample_limit_env and sample_limit_env.isdigit() else 0
        if sample_limit > 0 and sample_limit < len(raw_dataset):
            np.random.seed(42)
            selected_indices = np.random.choice(len(raw_dataset), size=sample_limit, replace=False)
            selected_indices = sorted(selected_indices)
        else:
            selected_indices = list(range(len(raw_dataset)))
        
        tokenizer = AutoTokenizer.from_pretrained(
            cfg.model.base_model,
            trust_remote_code=cfg.data.raw_dataset.vllm.trust_remote_code
        )
        
        instruction = "Please reason step by step, and put your final answer within \\boxed{}."
        
        # ë°ì´í„° ì¤€ë¹„
        problems = []
        texts = []
        for idx in selected_indices:
            problem_data = raw_dataset[idx]
            problem_id = problem_data.get("id", f"problem_{idx}")
            problem_text = problem_data.get("problem", "")
            ground_truth = problem_data.get("answer", "")
            messages = [{"role": "user", "content": f"{problem_text}\n\n{instruction}"}]
            # ì„œë²„ì—ì„œ ì±„íŒ… í…œí”Œë¦¿/í† í¬ë‚˜ì´ì¦ˆë¥¼ ì²˜ë¦¬í•˜ë„ë¡ messagesë§Œ ì „ë‹¬
            text = None
            problems.append({
                "problem_id": problem_id,
                "problem_text": problem_text,
                "ground_truth": ground_truth,
            })
            texts.append(messages)
        
        # ìƒ¤ë“œ ë¶„í• 
        my_problems = problems[args.shard_id::args.total_shards]
        my_texts = texts[args.shard_id::args.total_shards]
        logger.info(f"[Shard {args.shard_id}] {len(my_texts)}ê°œ ë¬¸ì œ ì²˜ë¦¬")
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
        gen_cfg = cfg.data.raw_dataset.generation
        sampling_params = {
            "model": cfg.model.base_model,
            "temperature": gen_cfg.temperature,
            "top_p": gen_cfg.top_p,
            "top_k": gen_cfg.top_k,
            "max_tokens": gen_cfg.max_tokens,
            "n": gen_cfg.num_responses_per_problem,
            "logprobs": gen_cfg.logprobs,
            "presence_penalty": gen_cfg.presence_penalty,
        }
        
        # ì‹ ë¢°ë„ ê³„ì‚°ê¸°
        confidence_calculator = ConfidenceCalculator(
            group_size=cfg.data.raw_dataset.confidence.group_size
        )
        
        # í ìƒì„±
        request_queue = asyncio.Queue(maxsize=1000)
        result_queue = asyncio.Queue(maxsize=1000)
        
        # ìš”ì²­ ë°ì´í„° ìƒì„± ë° íì— ì¶”ê°€
        for i, (problem, messages) in enumerate(zip(my_problems, my_texts)):
            # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ ì„œë²„ í• ë‹¹
            server = servers[i % len(servers)]
            request_data = RequestData(
                problem_id=problem["problem_id"],
                problem_text=problem["problem_text"],
                ground_truth=problem["ground_truth"],
                messages=messages,
                server_url=server["url"]
            )
            await request_queue.put(request_data)
        
        # ì¶œë ¥ íŒŒì¼
        temp_jsonl = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}_temp.jsonl")
        final_parquet = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}.parquet")
        
        async with aiofiles.open(temp_jsonl, 'a') as output_file:
            # HTTP ì„¸ì…˜ ìƒì„±
            connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)
            async with aiohttp.ClientSession(connector=connector) as session:
                
                # ì›Œì»¤ íƒœìŠ¤í¬ ìƒì„±
                num_request_workers = min(50, len(my_texts))  # ë™ì‹œ ìš”ì²­ ì›Œì»¤ ìˆ˜
                
                # ìš”ì²­ ì›Œì»¤ë“¤
                request_workers = [
                    asyncio.create_task(
                        request_worker(
                            session, 
                            request_queue, 
                            result_queue, 
                            sampling_params
                        )
                    )
                    for _ in range(num_request_workers)
                ]
                
                # ê²°ê³¼ ì²˜ë¦¬ ì›Œì»¤
                processor_task = asyncio.create_task(
                    result_processor(
                        result_queue,
                        output_file,
                        confidence_calculator,
                        gen_cfg.logprobs,
                        args,
                        len(my_texts)
                    )
                )
                
                # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
                await request_queue.join()
                
                # ì›Œì»¤ ì¢…ë£Œ
                for _ in request_workers:
                    await request_queue.put(None)
                await asyncio.gather(*request_workers)
                
                # ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
                await result_queue.join()
                await result_queue.put(None)
                await processor_task
        
        # JSONL â†’ Parquet ë³€í™˜
        logger.info(f"[Shard {args.shard_id}] Parquet ë³€í™˜ ì¤‘...")
        df = pd.read_json(temp_jsonl, lines=True)
        df.to_parquet(final_parquet, index=False, compression="zstd")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_jsonl)
        
        logger.info(f"âœ… [Shard {args.shard_id}] ì™„ë£Œ: {len(df)}ê°œ ê²°ê³¼ ì €ì¥")
        logger.info(f"Parquet ìœ„ì¹˜: {final_parquet}")
        
        # í†µê³„
        if len(df) > 0:
            logger.info(f"ìƒì„±ëœ ì‘ë‹µ ìˆ˜: {len(df)}")
            logger.info(f"ë¬¸ì œ ìˆ˜: {df['problem_id'].nunique()}")
            logger.info(f"ë¬¸ì œë‹¹ í‰ê·  ì‘ë‹µ ìˆ˜: {len(df) / df['problem_id'].nunique():.1f}")
        
    except Exception as e:
        logger.error(f"[Shard {args.shard_id}] ì˜¤ë¥˜: {e}", exc_info=True)
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--config-path", type=str, required=True)
    parser.add_argument("--config-name", type=str, required=True)
    parser.add_argument("--gpu-id", type=str, default="0")  # API ë°©ì‹ì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨
    parser.add_argument("--shard-id", type=int, required=True)
    parser.add_argument("--total-shards", type=int, default=4)
    args = parser.parse_args()
    
    # Hydra ì´ˆê¸°í™”
    config_dir = Path(args.config_path).resolve()
    hydra.core.global_hydra.GlobalHydra.instance().clear()
    hydra.initialize_config_dir(config_dir=str(config_dir), version_base=None)
    cfg = hydra.compose(config_name=args.config_name)
    
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(api_generation_worker(cfg, args))


if __name__ == "__main__":
    main()