"""
Stage 1: ì›ì‹œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Async ë²„ì „ - ê°œì„ íŒ)
- vLLM API í´ë°± ì§€ì› (generate/get_next_response)
- ë°±í”„ë ˆì…” ì œì–´ (Semaphore)
- I/O ìµœì í™” (íŒŒì¼ í•¸ë“¤ ì¬ì‚¬ìš©, ë°°ì¹˜ flush)
- Parquet ì•ˆì •ì„± í–¥ìƒ (JSON ì§ë ¬í™”)
- Graceful shutdown ì§€ì›
- ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
"""
import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import json
import asyncio
import signal

# PyArrow ì„í¬íŠ¸
import pyarrow as pa
import pyarrow.parquet as pq

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
import psutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data.confidence import ConfidenceCalculator
from src.data.dataset import RawDataset
from src.utils.logging import setup_logging
from transformers import AutoTokenizer

# vLLM Async ì—”ì§„
from vllm import SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine


logger = logging.getLogger(__name__)

# Graceful shutdownì„ ìœ„í•œ ì „ì—­ ì´ë²¤íŠ¸
shutdown_event = asyncio.Event()


def setup_environment_defaults(cfg: DictConfig):
    """
    í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì • (config ê¸°ë°˜)
    ì‚¬ìš©ìê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œí•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì ìš©
    """
    # HuggingFace ìºì‹œ ì„¤ì •
    if "TRANSFORMERS_CACHE" not in os.environ:
        os.environ["TRANSFORMERS_CACHE"] = cfg.paths.huggingface_cache
    if "HF_HOME" not in os.environ:
        os.environ["HF_HOME"] = cfg.paths.huggingface_cache
    
    # vLLM ë¡œê¹… ë ˆë²¨
    if "VLLM_LOGGING_LEVEL" not in os.environ:
        os.environ["VLLM_LOGGING_LEVEL"] = "INFO"
    
    # ê¸°ë³¸ SNAPSHOT_EVERY (configì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤ë©´)
    if "SNAPSHOT_EVERY" not in os.environ:
        os.environ.setdefault("SNAPSHOT_EVERY", "50")
    
    # ê¸°ë³¸ FLUSH_EVERY
    if "FLUSH_EVERY" not in os.environ:
        os.environ.setdefault("FLUSH_EVERY", "100")
    
    # ì¬ì‹œì‘ ê¸°ëŠ¥ ê¸°ë³¸ í™œì„±í™”
    if "RESUME" not in os.environ:
        os.environ.setdefault("RESUME", "true")


def signal_handler(sig, frame):
    """SIGINT/SIGTERM ì‹œê·¸ë„ í•¸ë“¤ëŸ¬"""
    logger.warning(f"âš ï¸  Shutdown ì‹ í˜¸ ìˆ˜ì‹  (signal={sig})")
    shutdown_event.set()


def simple_extract_topk(gen_logprobs: List[Dict[int, Any]], k: int) -> List[List[float]]:
    """ìµœì í™”ëœ logprob ì¶”ì¶œ í•¨ìˆ˜ (float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)"""
    if not gen_logprobs:
        return []
    
    results = []
    for token_step_dict in gen_logprobs:
        if not token_step_dict:
            results.append([])
            continue
        
        lps = []
        for i, entry in enumerate(token_step_dict.values()):
            if i >= k:
                break
            if hasattr(entry, "logprob"):
                lps.append(float(entry.logprob))
            elif isinstance(entry, dict) and "logprob" in entry:
                lps.append(float(entry["logprob"]))
        
        if lps:
            # float32 ë³€í™˜ (float16ë³´ë‹¤ ì•ˆì •ì ì´ê³  Parquet í˜¸í™˜)
            results.append(np.array(lps, dtype=np.float32).tolist())
        else:
            results.append([])
    
    return results


def compute_prompt_token_counts(
    tokenizer: AutoTokenizer, 
    texts: List[str], 
    batch_size: int = 100
) -> List[int]:
    """
    í”„ë¡¬í”„íŠ¸ í† í° ì¹´ìš´íŠ¸ ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬, ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    """
    prompt_token_counts = []
    
    # return_length ì§€ì› ì—¬ë¶€ í™•ì¸
    supports_return_length = False
    try:
        test_result = tokenizer(
            ["test"], 
            add_special_tokens=False, 
            return_length=True
        )
        if "length" in test_result:
            supports_return_length = True
    except (TypeError, KeyError):
        pass
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        
        try:
            if supports_return_length:
                enc = tokenizer(
                    batch, 
                    add_special_tokens=False, 
                    return_length=True
                )
                prompt_token_counts.extend([int(x) for x in enc["length"]])
            else:
                enc = tokenizer(batch, add_special_tokens=False)
                prompt_token_counts.extend([len(ids) for ids in enc.input_ids])
        except Exception as e:
            logger.warning(f"í† í° ì¹´ìš´íŠ¸ ê³„ì‚° ì‹¤íŒ¨ (ë°°ì¹˜ {i//batch_size}): {e}")
            # í´ë°±: ê°œë³„ ì²˜ë¦¬
            for text in batch:
                try:
                    enc = tokenizer([text], add_special_tokens=False)
                    prompt_token_counts.append(len(enc.input_ids[0]))
                except Exception:
                    prompt_token_counts.append(0)
    
    return prompt_token_counts


def apply_chat_template_safe(
    tokenizer: AutoTokenizer,
    messages: List[Dict[str, str]],
) -> str:
    """
    ì•ˆì „í•œ chat template ì ìš© (enable_thinking í˜¸í™˜ì„± ì²˜ë¦¬)
    """
    try:
        # Qwen2.5 ë“±ì—ì„œ enable_thinking ì§€ì›
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=True,
        )
    except TypeError:
        # enable_thinking ë¯¸ì§€ì› í† í¬ë‚˜ì´ì €
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
    
    return text


def create_engine_args(cfg: DictConfig) -> AsyncEngineArgs:
    """
    AsyncEngineArgs ìƒì„± (ë²„ì „ í˜¸í™˜ì„± ê³ ë ¤)
    Config íŒŒì¼ì˜ ëª¨ë“  ì„¤ì •ì„ ë°˜ì˜
    """
    vllm_config = cfg.data.raw_dataset.vllm
    
    # í•„ìˆ˜ í•„ë“œ
    engine_kwargs = {
        "model": cfg.model.base_model,
        "tensor_parallel_size": vllm_config.tensor_parallel_size,
        "gpu_memory_utilization": vllm_config.gpu_memory_utilization,
        "max_model_len": vllm_config.max_model_len,
        "dtype": vllm_config.dtype,
        "trust_remote_code": vllm_config.trust_remote_code,
        "max_num_batched_tokens": vllm_config.max_num_batched_tokens,
        "max_num_seqs": vllm_config.max_num_seqs,
        "enforce_eager": vllm_config.enforce_eager,
    }
    
    # ì„ íƒì  í•„ë“œ (ë²„ì „ë³„ë¡œ ìˆì„ ìˆ˜ë„ ì—†ì„ ìˆ˜ë„ ìˆìŒ)
    optional_fields = [
        "disable_custom_all_reduce",
        "disable_log_stats",
        "kv_cache_dtype",
        "enable_prefix_caching",
    ]
    
    for field in optional_fields:
        if field in vllm_config and vllm_config[field] is not None:
            engine_kwargs[field] = vllm_config[field]
    
    return AsyncEngineArgs(**engine_kwargs)


def create_parquet_schema() -> pa.Schema:
    """
    ëª…ì‹œì  Parquet ìŠ¤í‚¤ë§ˆ ì •ì˜ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
    
    Note: Confidence scoresëŠ” ConfidenceCalculatorê°€ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ
    ì—¬ê¸°ì„œëŠ” ê¸°ë³¸ í•„ë“œë§Œ ì •ì˜í•˜ê³ , ì‹¤ì œ ìŠ¤í‚¤ë§ˆëŠ” ì²« ë°°ì¹˜ì—ì„œ ì¶”ë¡ 
    """
    return pa.schema([
        ("problem_id", pa.string()),
        ("problem_text", pa.string()),
        ("ground_truth", pa.string()),
        ("response_id", pa.string()),
        ("generated_text", pa.string()),
        ("output_token_count", pa.int32()),
        ("prompt_token_count", pa.int32()),
        ("total_token_count", pa.int32()),
        ("logprobs", pa.string()),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
        ("worker_gpu", pa.string()),
        ("worker_replica", pa.string()),
        # Confidence scoresëŠ” ë™ì ìœ¼ë¡œ ì¶”ê°€ë¨
        # ConfidenceCalculator.calculate_all_confidence_scores()ì˜ ë°˜í™˜ê°’ ì°¸ì¡°
    ])


class FileHandlers:
    """
    íŒŒì¼ í•¸ë“¤ëŸ¬ ê´€ë¦¬ í´ë˜ìŠ¤ (ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬)
    """
    def __init__(self, jsonl_path: str, parquet_path: str, flush_every: int = 100):
        self.jsonl_path = jsonl_path
        self.parquet_path = parquet_path
        self.flush_every = flush_every
        
        self.jsonl_f: Optional[Any] = None
        self.jsonl_fd: Optional[int] = None
        self.parquet_writer: Optional[pq.ParquetWriter] = None
        self.parquet_schema: Optional[pa.Schema] = None
        
        self.since_flush = 0
        self.is_closed = False
    
    def open_jsonl(self):
        """JSONL íŒŒì¼ ì—´ê¸°"""
        if self.jsonl_f is None:
            self.jsonl_f = open(
                self.jsonl_path, 
                "a", 
                encoding="utf-8",
                buffering=8192  # 8KB ë²„í¼
            )
            self.jsonl_fd = self.jsonl_f.fileno()
    
    def write_jsonl(self, row_str: str):
        """JSONLì— í•œ ì¤„ ì“°ê¸° (ë²„í¼ë§)"""
        self.open_jsonl()
        self.jsonl_f.write(row_str + "\n")
        self.since_flush += 1
        
        # ì£¼ê¸°ì  flush/fsync
        if self.since_flush >= self.flush_every:
            self.flush_jsonl()
    
    def flush_jsonl(self):
        """JSONL ë²„í¼ ê°•ì œ í”ŒëŸ¬ì‹œ"""
        if self.jsonl_f is not None:
            self.jsonl_f.flush()
            os.fsync(self.jsonl_fd)
            self.since_flush = 0
    
    def open_parquet(self):
        """Parquet writer ì´ˆê¸°í™” (ìŠ¤í‚¤ë§ˆëŠ” ì²« ë°°ì¹˜ì—ì„œ ì¶”ë¡ )"""
        # ìŠ¤í‚¤ë§ˆê°€ ì•„ì§ ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ì²« ë°°ì¹˜ì—ì„œ ì¶”ë¡ 
        pass
    
    def write_parquet_batch(self, rows: List[Dict[str, Any]]):
        """Parquetì— ë°°ì¹˜ ì“°ê¸° (ì²« ë°°ì¹˜ì—ì„œ ìŠ¤í‚¤ë§ˆ ìë™ ì¶”ë¡ )"""
        if not rows:
            return
        
        try:
            # DataFrame ìƒì„±
            df = pd.DataFrame(rows)
            
            # ì²« ë°°ì¹˜ì—ì„œ ìŠ¤í‚¤ë§ˆ ì¶”ë¡  ë° writer ìƒì„±
            if self.parquet_writer is None:
                self.parquet_schema = pa.Table.from_pandas(df).schema
                self.parquet_writer = pq.ParquetWriter(
                    self.parquet_path, 
                    self.parquet_schema, 
                    compression="zstd"
                )
                logger.info(f"Parquet ìŠ¤í‚¤ë§ˆ ì¶”ë¡  ì™„ë£Œ: {len(self.parquet_schema)} ì»¬ëŸ¼")
            
            # Arrow Table ìƒì„± ë° ì“°ê¸°
            table = pa.Table.from_pandas(df, schema=self.parquet_schema)
            self.parquet_writer.write_table(table)
        
        except Exception as e:
            logger.error(f"Parquet ì“°ê¸° ì‹¤íŒ¨: {e}")
            if rows:
                logger.error(f"ì‹¤íŒ¨ ë°ì´í„° ìƒ˜í”Œ: {json.dumps(rows[0], indent=2, ensure_ascii=False)[:300]}")
    
    def close(self):
        """ëª¨ë“  íŒŒì¼ í•¸ë“¤ëŸ¬ ì•ˆì „í•˜ê²Œ ë‹«ê¸°"""
        if self.is_closed:
            return
        
        # JSONL ë‹«ê¸°
        if self.jsonl_f is not None:
            self.flush_jsonl()
            self.jsonl_f.close()
            self.jsonl_f = None
        
        # Parquet ë‹«ê¸°
        if self.parquet_writer is not None:
            self.parquet_writer.close()
            self.parquet_writer = None
        
        self.is_closed = True
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()


async def feed_requests(
    engine: AsyncLLMEngine,
    texts: List[str],
    sampling_params: SamplingParams,
    shard_id: int,
    semaphore: asyncio.Semaphore,
) -> None:
    """
    ë¹„ë™ê¸°ë¡œ ëª¨ë“  ìš”ì²­ì„ ì—”ì§„ì— ë“±ë¡ (ë°±í”„ë ˆì…” ì œì–´)
    """
    logger.info(f"[Shard {shard_id}] ë¹„ë™ê¸° ìš”ì²­ ë“±ë¡ ì‹œì‘...")
    
    for i, prompt in enumerate(texts):
        # Shutdown ì²´í¬
        if shutdown_event.is_set():
            logger.warning(f"[Shard {shard_id}] Shutdown ì‹ í˜¸ ê°ì§€, ìš”ì²­ ë“±ë¡ ì¤‘ë‹¨")
            break
        
        request_id = f"req_{shard_id}_{i}"
        
        # Semaphore íšë“ (in-flight ìš”ì²­ ìˆ˜ ì œí•œ)
        await semaphore.acquire()
        
        try:
            await engine.add_request(
                request_id=request_id,
                prompt=prompt,
                sampling_params=sampling_params,
            )
        except Exception as e:
            logger.error(f"[Shard {shard_id}] ìš”ì²­ ì¶”ê°€ ì‹¤íŒ¨ (Req ID: {request_id}): {e}")
            semaphore.release()  # ì‹¤íŒ¨ ì‹œ ìŠ¬ë¡¯ ë°˜í™˜
    
    logger.info(f"[Shard {shard_id}] ìš”ì²­ ë“±ë¡ ì™„ë£Œ: {len(texts)} ê±´")


async def collect_results(
    engine: AsyncLLMEngine,
    request_id_to_index: Dict[str, int],
    problems: List[Dict],
    prompt_token_counts: List[int],
    confidence_calculator: ConfidenceCalculator,
    file_handlers: FileHandlers,
    existing_response_ids: set,
    gen_cfg_logprobs: int,
    shard_id: int,
    gpu_id: str,
    semaphore: asyncio.Semaphore,
    snapshot_every: int,
) -> Dict[str, int]:
    """
    ë¹„ë™ê¸°ë¡œ ê²°ê³¼ ìˆ˜ì§‘ ë° ì‹¤ì‹œê°„ ì €ì¥
    """
    finished_requests = 0
    total_appended = 0
    total_skipped = 0
    
    # vLLM API ë²„ì „ í˜¸í™˜ì„± ì²´í¬
    use_generate = hasattr(engine, "generate") and callable(getattr(engine, "generate"))
    
    logger.info(f"[Shard {shard_id}] ê²°ê³¼ ìˆ˜ì§‘ ì‹œì‘ (API: {'generate' if use_generate else 'get_next_response'})")
    
    try:
        if use_generate:
            # vLLM 0.6.x ìŠ¤íƒ€ì¼: async for loop
            async for request_output in engine.generate():
                if shutdown_event.is_set():
                    logger.warning(f"[Shard {shard_id}] Shutdown ì‹ í˜¸ ê°ì§€, ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                # ìš”ì²­ ì²˜ë¦¬
                stats = await process_request_output(
                    request_output=request_output,
                    request_id_to_index=request_id_to_index,
                    problems=problems,
                    prompt_token_counts=prompt_token_counts,
                    confidence_calculator=confidence_calculator,
                    file_handlers=file_handlers,
                    existing_response_ids=existing_response_ids,
                    gen_cfg_logprobs=gen_cfg_logprobs,
                    shard_id=shard_id,
                    gpu_id=gpu_id,
                )
                
                finished_requests += 1
                total_appended += stats["appended"]
                total_skipped += stats["skipped"]
                
                # Semaphore ë°˜í™˜ (in-flight ìŠ¬ë¡¯ íšŒìˆ˜)
                semaphore.release()
                
                # ì£¼ê¸°ì  ì§„í–‰ ìƒí™© ë¡œê¹…
                if snapshot_every > 0 and finished_requests % snapshot_every == 0:
                    await log_progress(
                        shard_id=shard_id,
                        finished_requests=finished_requests,
                        total_requests=len(request_id_to_index),
                        total_appended=total_appended,
                        total_skipped=total_skipped,
                    )
        
        else:
            # vLLM 0.10.x ìŠ¤íƒ€ì¼: get_next_response()
            while True:
                if shutdown_event.is_set():
                    logger.warning(f"[Shard {shard_id}] Shutdown ì‹ í˜¸ ê°ì§€, ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
                
                request_output = await engine.get_next_response()
                
                if request_output is None:
                    break
                
                # ìš”ì²­ ì²˜ë¦¬
                stats = await process_request_output(
                    request_output=request_output,
                    request_id_to_index=request_id_to_index,
                    problems=problems,
                    prompt_token_counts=prompt_token_counts,
                    confidence_calculator=confidence_calculator,
                    file_handlers=file_handlers,
                    existing_response_ids=existing_response_ids,
                    gen_cfg_logprobs=gen_cfg_logprobs,
                    shard_id=shard_id,
                    gpu_id=gpu_id,
                )
                
                finished_requests += 1
                total_appended += stats["appended"]
                total_skipped += stats["skipped"]
                
                # Semaphore ë°˜í™˜
                semaphore.release()
                
                # ì£¼ê¸°ì  ì§„í–‰ ìƒí™© ë¡œê¹…
                if snapshot_every > 0 and finished_requests % snapshot_every == 0:
                    await log_progress(
                        shard_id=shard_id,
                        finished_requests=finished_requests,
                        total_requests=len(request_id_to_index),
                        total_appended=total_appended,
                        total_skipped=total_skipped,
                    )
    
    except Exception as e:
        logger.error(f"[Shard {shard_id}] ê²°ê³¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
    
    finally:
        # ë§ˆì§€ë§‰ flush
        file_handlers.flush_jsonl()
    
    return {
        "finished_requests": finished_requests,
        "total_appended": total_appended,
        "total_skipped": total_skipped,
    }


async def process_request_output(
    request_output,
    request_id_to_index: Dict[str, int],
    problems: List[Dict],
    prompt_token_counts: List[int],
    confidence_calculator: ConfidenceCalculator,
    file_handlers: FileHandlers,
    existing_response_ids: set,
    gen_cfg_logprobs: int,
    shard_id: int,
    gpu_id: str,
) -> Dict[str, int]:
    """
    ë‹¨ì¼ ìš”ì²­ ì¶œë ¥ ì²˜ë¦¬
    """
    req_id = request_output.request_id
    
    if req_id not in request_id_to_index:
        logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” request_id: {req_id}")
        return {"appended": 0, "skipped": 0}
    
    idx = request_id_to_index[req_id]
    base_meta = problems[idx]
    prompt_tokens = prompt_token_counts[idx] if idx < len(prompt_token_counts) else 0
    
    # ë°°ì¹˜ ê²°ê³¼ ì¤€ë¹„
    batch_results_json = []
    batch_results_arrow = []
    
    appended = 0
    skipped = 0
    
    # ê° ì¶œë ¥ ì²˜ë¦¬
    for oi, gen in enumerate(request_output.outputs):
        # logprob ì¶”ì¶œ
        topk = simple_extract_topk(gen.logprobs, gen_cfg_logprobs)
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence_scores = confidence_calculator.calculate_all_confidence_scores(topk)
        
        response_id = f"{base_meta['problem_id']}_resp_{oi}"
        
        # ì¬ì‹œì‘ ì‹œ ê¸°ì¡´ ê²°ê³¼ ìŠ¤í‚µ
        if response_id in existing_response_ids:
            skipped += 1
            continue
        
        # í† í° ì¹´ìš´íŠ¸
        output_token_count = len(gen.token_ids) if hasattr(gen, "token_ids") else 0
        total_token_count = prompt_tokens + output_token_count
        
        # ê²°ê³¼ í–‰ ìƒì„±
        row = {
            "problem_id": base_meta["problem_id"],
            "problem_text": base_meta["problem_text"],
            "ground_truth": base_meta["ground_truth"],
            "response_id": response_id,
            "generated_text": gen.text,
            "output_token_count": output_token_count,
            "prompt_token_count": prompt_tokens,
            "total_token_count": total_token_count,
            "logprobs": json.dumps(topk, ensure_ascii=False),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
            "worker_gpu": gpu_id,
            "worker_replica": f"shard_{shard_id}",
            **confidence_scores,
        }
        
        batch_results_json.append(json.dumps(row, ensure_ascii=False))
        batch_results_arrow.append(row)
        appended += 1
        
        # ì¬ì‹œì‘ ì„¸íŠ¸ì— ì¦‰ì‹œ ì¶”ê°€
        existing_response_ids.add(response_id)
    
    # JSONL ì‹¤ì‹œê°„ ì €ì¥
    if batch_results_json:
        for row_str in batch_results_json:
            file_handlers.write_jsonl(row_str)
    
    # Parquet ì‹¤ì‹œê°„ ì €ì¥
    if batch_results_arrow:
        file_handlers.write_parquet_batch(batch_results_arrow)
    
    return {"appended": appended, "skipped": skipped}


async def log_progress(
    shard_id: int,
    finished_requests: int,
    total_requests: int,
    total_appended: int,
    total_skipped: int,
):
    """
    ì§„í–‰ ìƒí™© ë¡œê¹… (ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í¬í•¨)
    """
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    process = psutil.Process()
    mem_info = process.memory_info()
    mem_gb = mem_info.rss / 1024**3
    
    logger.info(
        f"[Shard {shard_id}] ì§„í–‰: {finished_requests}/{total_requests} ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ "
        f"(ì €ì¥: {total_appended}ê±´, ìŠ¤í‚µ: {total_skipped}ê±´) | "
        f"ë©”ëª¨ë¦¬: {mem_gb:.2f} GB"
    )


def load_existing_response_ids(
    parquet_path: str,
    jsonl_path: str,
    shard_id: int,
    resume_enabled: bool,
) -> set:
    """
    ì¬ì‹œì‘ì„ ìœ„í•œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
    """
    existing_response_ids = set()
    
    if not resume_enabled:
        return existing_response_ids
    
    # 1. Parquet ìš°ì„  ì‹œë„ (ë” ë¹ ë¦„)
    if os.path.exists(parquet_path):
        try:
            logger.info(f"[Shard {shard_id}] ê¸°ì¡´ Parquet ë°œê²¬, ì¬ì‹œì‘ í™œì„±í™”: {parquet_path}")
            df_existing = pd.read_parquet(parquet_path, columns=["response_id"])
            existing_response_ids = set(df_existing["response_id"])
            logger.info(f"Parquetì—ì„œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ: {len(existing_response_ids)}ê°œ")
            return existing_response_ids
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ Parquet ë¡œë“œ ì‹¤íŒ¨, JSONLë¡œ ëŒ€ì²´: {e}")
            existing_response_ids.clear()
    
    # 2. JSONL í´ë°±
    if os.path.exists(jsonl_path):
        logger.info(f"[Shard {shard_id}] ê¸°ì¡´ JSONL ë¡œë“œ: {jsonl_path}")
        try:
            with open(jsonl_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                        if isinstance(obj, dict) and "response_id" in obj:
                            existing_response_ids.add(obj["response_id"])
                    except json.JSONDecodeError:
                        continue
            logger.info(f"JSONLì—ì„œ ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ: {len(existing_response_ids)}ê°œ")
        except Exception as e:
            logger.warning(f"ê¸°ì¡´ JSONL ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return existing_response_ids


async def main_worker_async(cfg: DictConfig, args: argparse.Namespace) -> None:
    """
    Async ë²„ì „ ë©”ì¸ ì›Œì»¤ í•¨ìˆ˜ (ê°œì„ íŒ)
    """
    # í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì • (config ê¸°ë°˜)
    setup_environment_defaults(cfg)
    
    # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # ë¡œê¹… ì„¤ì • (config ê¸°ë°˜)
    log_file = os.path.join(
        cfg.paths.log_dir, 
        f"stage1_generate_async_shard_{args.shard_id}.log"
    )
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags + [f"shard_{args.shard_id}"]  # ìƒ¤ë“œ íƒœê·¸ ì¶”ê°€
    )
    
    app_logger = logging.getLogger("conf_agg_llm")
    vllm_logger = logging.getLogger("vllm")
    vllm_logger.setLevel(logging.INFO)
    for h in app_logger.handlers:
        vllm_logger.addHandler(h)
    vllm_logger.propagate = False
    vllm_logger.info("[DIAG] vLLM async logger ì—°ê²° ì™„ë£Œ.")
    
    app_logger.info(f"ğŸš€ [Shard {args.shard_id} | GPU {args.gpu_id}] Stage 1 (Async ê°œì„ íŒ) ì‹œì‘")
    app_logger.info(f"=" * 80)
    app_logger.info(f"í”„ë¡œì íŠ¸: {cfg.project.name} v{cfg.project.version}")
    app_logger.info(f"ëª¨ë¸: {cfg.model.base_model}")
    app_logger.info(f"ë°ì´í„° ê²½ë¡œ: {cfg.paths.data_dir}")
    app_logger.info(f"ì¶œë ¥ ê²½ë¡œ: {cfg.paths.output_dir}")
    app_logger.info(f"=" * 80)
    app_logger.info(f"vLLM ì„¤ì •:")
    app_logger.info(f"  - max_model_len: {cfg.data.raw_dataset.vllm.max_model_len}")
    app_logger.info(f"  - max_num_seqs: {cfg.data.raw_dataset.vllm.max_num_seqs}")
    app_logger.info(f"  - gpu_memory_utilization: {cfg.data.raw_dataset.vllm.gpu_memory_utilization}")
    app_logger.info(f"  - dtype: {cfg.data.raw_dataset.vllm.dtype}")
    app_logger.info(f"  - kv_cache_dtype: {cfg.data.raw_dataset.vllm.get('kv_cache_dtype', 'N/A')}")
    app_logger.info(f"ìƒì„± ì„¤ì •:")
    app_logger.info(f"  - num_responses_per_problem: {cfg.data.raw_dataset.generation.num_responses_per_problem}")
    app_logger.info(f"  - temperature: {cfg.data.raw_dataset.generation.temperature}")
    app_logger.info(f"  - max_tokens: {cfg.data.raw_dataset.generation.max_tokens}")
    app_logger.info(f"=" * 80)
    if cfg.experiment.get("log_level") != "INFO":
        app_logger.debug(f"ì „ì²´ ì„¤ì •:\n{OmegaConf.to_yaml(cfg)}")
    
    try:
        # 1. AsyncLLMEngine ì´ˆê¸°í™”
        app_logger.info(f"[Shard {args.shard_id}] AsyncLLMEngine ë¡œë“œ ì¤‘: {cfg.model.base_model}")
        
        engine_args = create_engine_args(cfg)
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        
        tokenizer = AutoTokenizer.from_pretrained(
            cfg.model.base_model,
            trust_remote_code=cfg.model.trust_remote_code
        )
        app_logger.info(f"[Shard {args.shard_id}] AsyncLLMEngine ë¡œë“œ ì™„ë£Œ.")
        
        # 2. ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(cfg.paths.data_dir, "generated")
        sample_limit_env = os.environ.get("SAMPLE_LIMIT")
        if sample_limit_env:
            output_dir = os.path.join(output_dir, f"sample_{sample_limit_env}")
        os.makedirs(output_dir, exist_ok=True)
        
        # 3. ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        raw_data_path = os.path.join(cfg.paths.data_dir, "raw", "deepscaler.jsonl")
        if not os.path.exists(raw_data_path):
            app_logger.error(f"ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_data_path}")
            return
        
        raw_dataset = RawDataset(raw_data_path)
        app_logger.info(f"ì „ì²´ ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # 4. ìƒ˜í”Œë§ ì„¤ì •
        sample_limit = int(sample_limit_env) if sample_limit_env and sample_limit_env.isdigit() else 0
        
        if sample_limit > 0 and sample_limit < len(raw_dataset):
            np.random.seed(42)
            selected_indices = np.random.choice(
                len(raw_dataset), 
                size=sample_limit, 
                replace=False
            )
            selected_indices = sorted(selected_indices)
            app_logger.info(f"SAMPLE_LIMIT ì ìš©: ì „ì²´ {len(raw_dataset)}ê°œ ì¤‘ {sample_limit}ê°œ ì„ íƒ")
            app_logger.info(f"ì„ íƒëœ ì¸ë±ìŠ¤ ë²”ìœ„: {min(selected_indices)} ~ {max(selected_indices)}")
        else:
            selected_indices = list(range(len(raw_dataset)))
            app_logger.info(f"ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # 5. ë¬¸ì œ/í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        problems: List[Dict] = []
        texts: List[str] = []
        instruction = "Please reason step by step, and put your final answer within \\boxed{}."
        
        app_logger.info("ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        for idx in selected_indices:
            problem_data = raw_dataset[idx]
            problem_id = problem_data.get("id", f"problem_{idx}")
            problem_text = problem_data.get("problem", "")
            ground_truth = problem_data.get("answer", "")
            
            messages = [{"role": "user", "content": f"{problem_text}\n\n{instruction}"}]
            text = apply_chat_template_safe(tokenizer, messages)
            
            problems.append({
                "problem_id": problem_id,
                "problem_text": problem_text,
                "ground_truth": ground_truth,
            })
            texts.append(text)
        
        app_logger.info(f"ì´ {len(texts)}ê°œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")
        
        # 6. ì‘ì—… ë¶„í•  (Sharding)
        my_problems = problems[args.shard_id::args.total_shards]
        my_texts = texts[args.shard_id::args.total_shards]
        app_logger.info(
            f"[Shard {args.shard_id}] ì‘ì—… ë¶„í•  ì™„ë£Œ: "
            f"{len(my_texts)}ê°œ (1/{args.total_shards})"
        )
        
        # 7. í”„ë¡¬í”„íŠ¸ í† í° ì¹´ìš´íŠ¸ ê³„ì‚° (ë°°ì¹˜ ì²˜ë¦¬)
        app_logger.info(f"[Shard {args.shard_id}] í”„ë¡¬í”„íŠ¸ í† í° ì¹´ìš´íŠ¸ ê³„ì‚° ì¤‘...")
        prompt_token_counts = compute_prompt_token_counts(tokenizer, my_texts)
        app_logger.info(f"í† í° ì¹´ìš´íŠ¸ ê³„ì‚° ì™„ë£Œ.")
        
        # 8. ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° êµ¬ì„± (config ê¸°ë°˜)
        gen_cfg = cfg.data.raw_dataset.generation
        sampling_params = SamplingParams(
            n=gen_cfg.num_responses_per_problem,
            temperature=gen_cfg.temperature,
            top_p=gen_cfg.top_p,
            top_k=gen_cfg.top_k,
            min_p=gen_cfg.min_p,
            max_tokens=gen_cfg.max_tokens,
            logprobs=gen_cfg.logprobs if gen_cfg.logprobs > 0 else None,  # 0ì´ë©´ None
            presence_penalty=gen_cfg.presence_penalty,
        )
        gen_cfg_logprobs = gen_cfg.logprobs if gen_cfg.logprobs > 0 else 0
        
        app_logger.info(f"ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°: n={gen_cfg.num_responses_per_problem}, "
                       f"temp={gen_cfg.temperature}, top_p={gen_cfg.top_p}, "
                       f"max_tokens={gen_cfg.max_tokens}, logprobs={gen_cfg_logprobs}")
        
        # 9. ì €ì¥ ê²½ë¡œ ë° ì˜µì…˜ ì„¤ì •
        jsonl_path = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}.jsonl")
        parquet_path = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}.parquet")
        snapshot_every = int(os.environ.get("SNAPSHOT_EVERY", "50"))
        flush_every = int(os.environ.get("FLUSH_EVERY", "100"))
        resume_enabled = os.environ.get("RESUME", "true").lower() in ("1", "true", "yes")
        
        # 10. ì¬ì‹œì‘ ë¡œì§: ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
        existing_response_ids = load_existing_response_ids(
            parquet_path=parquet_path,
            jsonl_path=jsonl_path,
            shard_id=args.shard_id,
            resume_enabled=resume_enabled,
        )
        
        # 11. ìš”ì²­ ì‹ë³„ì ë§µí•‘ ìƒì„±
        request_id_to_index: Dict[str, int] = {}
        for i in range(len(my_texts)):
            request_id = f"req_{args.shard_id}_{i}"
            request_id_to_index[request_id] = i
        
        # 12. ë°±í”„ë ˆì…” ì œì–´ (Semaphore)
        # configì˜ max_num_seqsë¥¼ ê¸°ì¤€ìœ¼ë¡œ MAX_INFLIGHT ê³„ì‚°
        max_num_seqs = cfg.data.raw_dataset.vllm.max_num_seqs
        # í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥, ê¸°ë³¸ê°’ì€ max_num_seqsì˜ 2ë°°
        max_inflight = int(os.environ.get("MAX_INFLIGHT", str(max_num_seqs * 2)))
        semaphore = asyncio.Semaphore(max_inflight)
        
        app_logger.info(f"ë°±í”„ë ˆì…” ì„¤ì •: MAX_INFLIGHT={max_inflight}, max_num_seqs={max_num_seqs}")
        
        # 13. ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™” (config ê¸°ë°˜)
        confidence_calculator = ConfidenceCalculator(
            group_size=cfg.data.raw_dataset.confidence.group_size
        )
        app_logger.info(f"ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”: group_size={cfg.data.raw_dataset.confidence.group_size}")
        
        # 14. íŒŒì¼ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
        with FileHandlers(jsonl_path, parquet_path, flush_every) as file_handlers:
            # 15. Feederì™€ Collector ë™ì‹œ ì‹¤í–‰
            feeder_task = asyncio.create_task(
                feed_requests(
                    engine=engine,
                    texts=my_texts,
                    sampling_params=sampling_params,
                    shard_id=args.shard_id,
                    semaphore=semaphore,
                )
            )
            
            collector_task = asyncio.create_task(
                collect_results(
                    engine=engine,
                    request_id_to_index=request_id_to_index,
                    problems=my_problems,
                    prompt_token_counts=prompt_token_counts,
                    confidence_calculator=confidence_calculator,
                    file_handlers=file_handlers,
                    existing_response_ids=existing_response_ids,
                    gen_cfg_logprobs=gen_cfg_logprobs,
                    shard_id=args.shard_id,
                    gpu_id=args.gpu_id,
                    semaphore=semaphore,
                    snapshot_every=snapshot_every,
                )
            )
            
            # ë‘ íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            app_logger.info(f"[Shard {args.shard_id}] Feederì™€ Collector ì‹œì‘...")
            feeder_result, collector_result = await asyncio.gather(
                feeder_task, 
                collector_task,
                return_exceptions=True
            )
            
            # ì˜ˆì™¸ ì²˜ë¦¬
            if isinstance(feeder_result, Exception):
                app_logger.error(f"Feeder ì‹¤íŒ¨: {feeder_result}", exc_info=feeder_result)
            if isinstance(collector_result, Exception):
                app_logger.error(f"Collector ì‹¤íŒ¨: {collector_result}", exc_info=collector_result)
            
            app_logger.info(f"[Shard {args.shard_id}] Feederì™€ Collector ì™„ë£Œ.")
        
        # 16. ìµœì¢… í†µê³„ ì¶œë ¥
        if isinstance(collector_result, dict):
            finished_requests = collector_result["finished_requests"]
            total_appended = collector_result["total_appended"]
            total_skipped = collector_result["total_skipped"]
            
            app_logger.info(f"âœ… [Shard {args.shard_id}] Async Stage 1 ì™„ë£Œ")
            app_logger.info(f"ì €ì¥ ìœ„ì¹˜: {parquet_path}")
            app_logger.info(
                f"ì²˜ë¦¬ ì™„ë£Œ: {finished_requests}ê°œ ìš”ì²­, "
                f"{total_appended}í–‰ ì €ì¥, {total_skipped}í–‰ ìŠ¤í‚µ"
            )
        
        # 17. ìµœì¢… ë°ì´í„°í”„ë ˆì„ í†µê³„
        if os.path.exists(parquet_path):
            df = pd.read_parquet(parquet_path)
            app_logger.info(f"ìµœì¢… ë°ì´í„°í”„ë ˆì„ í¬ê¸°: {len(df)}í–‰")
            app_logger.info(f"ë¬¸ì œ ìˆ˜: {df['problem_id'].nunique()}")
            
            if len(df) > 0:
                app_logger.info(
                    f"ë¬¸ì œë‹¹ í‰ê·  ì‘ë‹µ ìˆ˜: "
                    f"{len(df) / df['problem_id'].nunique():.1f}"
                )
                
                # í† í° í†µê³„
                if 'total_token_count' in df.columns:
                    try:
                        df_tokens = df['total_token_count'].fillna(0)
                        total_tokens = int(df_tokens.sum())
                        mean_tokens = float(df_tokens.mean())
                        min_tokens = int(df_tokens.min())
                        max_tokens = int(df_tokens.max())
                        
                        app_logger.info(f"ì „ì²´ í† í° ìˆ˜: {total_tokens:,}")
                        app_logger.info(f"í‰ê·  í† í° ìˆ˜: {mean_tokens:.1f}")
                        app_logger.info(f"í† í° ìˆ˜ ë²”ìœ„: {min_tokens:,} ~ {max_tokens:,}")
                        
                        # max_tokens ë„ë‹¬ ì—¬ë¶€ (config ê¸°ë°˜)
                        # prompt + generated tokensê°€ max_model_lenì— ê·¼ì ‘í–ˆëŠ”ì§€ ì²´í¬
                        max_model_len = cfg.data.raw_dataset.vllm.max_model_len
                        near_limit_threshold = max_model_len * 0.95  # 95% ë„ë‹¬
                        near_limit_count = int((df_tokens >= near_limit_threshold).sum())
                        app_logger.info(
                            f"í† í° í•œê³„ ê·¼ì ‘ ì¸ìŠ¤í„´ìŠ¤ (>={near_limit_threshold:.0f}): {near_limit_count}ê±´"
                        )
                    except Exception as e:
                        app_logger.warning(f"í† í° í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                
                # ìƒ˜í”Œ ì¶œë ¥
                app_logger.info("=" * 80)
                app_logger.info("ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥ (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤):")
                app_logger.info("=" * 80)
                sample = df.iloc[0]
                app_logger.info(f"Problem ID: {sample['problem_id']}")
                app_logger.info(f"Generated Text: {sample['generated_text'][:500]}...")
                app_logger.info(f"Total Token Count: {sample.get('total_token_count', 'N/A')}")
                
                # Confidence scores (configì— ì •ì˜ëœ ë©”ì„œë“œ ê¸°ì¤€)
                conf_cols = [col for col in df.columns if col.startswith("confidence_")]
                if conf_cols:
                    app_logger.info("Confidence Scores:")
                    for col in conf_cols:
                        val = sample.get(col, 'N/A')
                        if isinstance(val, (int, float)):
                            app_logger.info(f"  {col}: {val:.4f}")
                        else:
                            app_logger.info(f"  {col}: {val}")
                    
                    # ì„¤ì •ëœ ë©”ì„œë“œ í™•ì¸
                    expected_methods = cfg.data.raw_dataset.confidence.methods
                    app_logger.info(f"Configì— ì •ì˜ëœ ì‹ ë¢°ë„ ë©”ì„œë“œ: {expected_methods}")
                
                app_logger.info("=" * 80)
    
    except Exception as e:
        app_logger.error(
            f"[Shard {args.shard_id}] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", 
            exc_info=True
        )
        raise


if __name__ == "__main__":
    # argparseë¡œ ëŸ°ì²˜ì˜ ì¸ìˆ˜ë¥¼ ë°›ìŒ
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--config-path", 
        type=str, 
        required=True, 
        help="Hydra config directory (e.g., ../config)"
    )
    parser.add_argument(
        "--config-name", 
        type=str, 
        required=True, 
        help="Hydra config name (e.g., config)"
    )
    parser.add_argument(
        "--gpu-id", 
        type=str, 
        required=True, 
        help="GPU ID (e.g., '0')"
    )
    parser.add_argument(
        "--shard-id", 
        type=int, 
        required=True, 
        help="Data shard index (0, 1, 2, 3)"
    )
    parser.add_argument(
        "--total-shards", 
        type=int, 
        default=4, 
        help="Total number of shards"
    )
    args = parser.parse_args()
    
    # GPU ê²©ë¦¬
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id
    
    # Hydra ì´ˆê¸°í™”
    config_dir = Path(args.config_path).resolve()
    hydra.core.global_hydra.GlobalHydra.instance().clear()
    hydra.initialize_config_dir(config_dir=str(config_dir), version_base=None)
    
    cfg = hydra.compose(config_name=args.config_name)
    
    # ë¹„ë™ê¸° ë©”ì¸ ì›Œì»¤ ì‹¤í–‰
    try:
        asyncio.run(main_worker_async(cfg, args))
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë¹„ë™ê¸° ë©”ì¸ ì›Œì»¤ ì‹¤í–‰ ì‹¤íŒ¨: {e}", exc_info=True)
        sys.exit(1)