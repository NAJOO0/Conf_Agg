#!/usr/bin/env python
"""
vLLM API ì„œë²„ ëŸ°ì²˜ ìŠ¤í¬ë¦½íŠ¸
ê° GPUë§ˆë‹¤ ë…ë¦½ì ì¸ vLLM ì„œë²„ë¥¼ ì‹¤í–‰
"""
import os
import sys
import subprocess
import argparse
import time
from pathlib import Path
import signal
import json

def launch_vllm_server(gpu_id: int, port: int, config_path: str):
    """ë‹¨ì¼ GPUì—ì„œ vLLM ì„œë²„ ì‹¤í–‰"""
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ
    with open(config_path, 'r') as f:
        config = json.load(f)
    
    model_name = config['model']['base_model']
    vllm_config = config['data']['raw_dataset']['vllm']
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    # ë¡œê·¸ ë ˆë²¨ ì œì–´ (INFO/DEBUG/WARNING)
    env.setdefault('VLLM_LOG_LEVEL', env.get('VLLM_LOG_LEVEL', 'INFO'))
    
    # vLLM ì„œë²„ ì‹¤í–‰ ëª…ë ¹
    cmd = [
        sys.executable, '-m', 'vllm.entrypoints.openai.api_server',
        '--model', model_name,
        '--port', str(port),
        '--host', '0.0.0.0',
        '--gpu-memory-utilization', str(vllm_config['gpu_memory_utilization']),
        '--max-model-len', str(vllm_config.get('max_model_len', 32768)),
        '--dtype', vllm_config['dtype'],
        '--generation-config', 'vllm',
        '--max-num-batched-tokens', str(vllm_config.get('max_num_batched_tokens', 16384)),
        '--max-num-seqs', str(vllm_config.get('max_num_seqs', 256)),
        '--disable-log-stats' if vllm_config.get('disable_log_stats', False) else '',
        '--trust-remote-code' if vllm_config.get('trust_remote_code', False) else '',
        '--enable-prefix-caching',  # í”„ë¦¬í”½ìŠ¤ ìºì‹± í™œì„±í™”
        # ìš”ì²­ ë¡œê·¸ëŠ” ê¸°ë³¸ í™œì„± (í•„ìš”ì‹œ VLLM_LOG_LEVELë¡œ ì œì–´)
    ]
    
    # KV ìºì‹œ dtype ì„¤ì •
    if 'kv_cache_dtype' in vllm_config:
        cmd.extend(['--kv-cache-dtype', vllm_config['kv_cache_dtype']])
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    cmd = [c for c in cmd if c]
    
    print(f"ğŸš€ GPU {gpu_id}ì—ì„œ vLLM ì„œë²„ ì‹œì‘ (í¬íŠ¸: {port})")
    print(f"ëª…ë ¹ì–´: {' '.join(cmd)}")

    # ì„œë²„ ë¡œê·¸ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    workspace = os.environ.get('WORKSPACE', '/mnt/data1/projects/Conf_Agg')
    log_dir = Path(os.environ.get('SERVER_LOG_DIR', str(Path(workspace) / 'outputs/logs/vllm')))
    log_dir.mkdir(parents=True, exist_ok=True)
    log_path = log_dir / f'server_gpu_{gpu_id}.log'
    print(f"ë¡œê·¸ íŒŒì¼: {log_path}")
    
    # ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    log_file = open(log_path, 'a')
    process = subprocess.Popen(
        cmd,
        env=env,
        stdout=log_file,
        stderr=log_file,
        text=True
    )
    
    return process

def wait_for_server(port: int, max_retries: int = 30):
    """ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    import requests
    
    url = f"http://localhost:{port}/health"
    for i in range(max_retries):
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                print(f"âœ… í¬íŠ¸ {port}ì˜ ì„œë²„ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
        except:
            pass
        
        print(f"ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì¤‘... ({i+1}/{max_retries})")
        time.sleep(5)
    
    return False

def main():
    parser = argparse.ArgumentParser(description="vLLM API ì„œë²„ ëŸ°ì²˜")
    parser.add_argument('--num-gpus', type=int, default=4, help='ì‚¬ìš©í•  GPU ìˆ˜')
    parser.add_argument('--base-port', type=int, default=8000, help='ì‹œì‘ í¬íŠ¸ ë²ˆí˜¸')
    parser.add_argument('--config-path', type=str, required=True, help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    args = parser.parse_args()
    
    processes = []
    ports = []
    
    try:
        # ê° GPUë§ˆë‹¤ ì„œë²„ ì‹¤í–‰
        for gpu_id in range(args.num_gpus):
            port = args.base_port + gpu_id
            process = launch_vllm_server(gpu_id, port, args.config_path)
            processes.append(process)
            ports.append(port)
            
            # ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            if not wait_for_server(port):
                print(f"âŒ GPU {gpu_id}ì˜ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
                raise Exception(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: GPU {gpu_id}")
        
        print("\n" + "="*50)
        print("âœ… ëª¨ë“  vLLM ì„œë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"í¬íŠ¸: {ports}")
        print("="*50 + "\n")
        
        # ì„œë²„ ì •ë³´ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (WORKSPACE ê¸°ì¤€)
        server_info = {
            'servers': [
                {'gpu_id': i, 'port': ports[i], 'url': f'http://localhost:{ports[i]}'}
                for i in range(args.num_gpus)
            ]
        }
        workspace = os.environ.get('WORKSPACE', '/mnt/data1/projects/Conf_Agg')
        out_path = Path(workspace) / 'vllm_servers.json'
        with open(out_path, 'w') as f:
            json.dump(server_info, f, indent=2)
        print(f"ì„œë²„ ì •ë³´ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {out_path}")
        print("ì„œë²„ë¥¼ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”...")
        
        # í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§: ì¬ì‹œì‘ ì—†ì´ ì¢…ë£Œ ê°ì§€ ì‹œ ì¦‰ì‹œ ì—ëŸ¬ ë°˜í™˜
        while True:
            for i, process in enumerate(processes):
                ret = process.poll()
                if ret is not None:
                    print(f"âŒ GPU {i}ì˜ ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (return code: {ret})")
                    sys.exit(1)
            time.sleep(10)
            
    except KeyboardInterrupt:
        print("\nì„œë²„ ì¢…ë£Œ ì¤‘...")
        for process in processes:
            process.terminate()
            process.wait()
        print("ëª¨ë“  ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        for process in processes:
            process.terminate()
        sys.exit(1)

if __name__ == "__main__":
    main()