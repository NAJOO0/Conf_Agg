#!/usr/bin/env python3
"""
Parquet íŒŒì¼ì˜ logprobsë¥¼ ì‚¬ìš©í•˜ì—¬ confidence ì ìˆ˜ë¥¼ ì¬ê³„ì‚°í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ìµœì í™” ë²„ì „ v2)
"""
import os
import sys
import pandas as pd
import argparse
import numpy as np
import logging
from pathlib import Path
from typing import List, Optional, Dict
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import partial

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.data.confidence import ConfidenceCalculator

# PyArrow ê°€ìš©ì„± í™•ì¸
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    HAS_PYARROW = True
except Exception:
    pa = None
    pq = None
    HAS_PYARROW = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def convert_logprobs_to_list(logprobs_value) -> Optional[List[List[float]]]:
    """logprobs ê°’ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (ìµœì í™”)"""
    if logprobs_value is None:
        return None
    
    # numpy array ìš°ì„  ì²˜ë¦¬
    if isinstance(logprobs_value, np.ndarray):
        if logprobs_value.size == 0:
            return None
        return logprobs_value.tolist()
    
    # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
    if isinstance(logprobs_value, list):
        if len(logprobs_value) == 0:
            return None
        return logprobs_value
    
    # NaN ì²´í¬ (ìŠ¤ì¹¼ë¼ë§Œ)
    try:
        if pd.isna(logprobs_value):
            return None
    except (ValueError, TypeError):
        pass
    
    return None


def process_batch(batch_data: List[tuple], group_size: int) -> List[Optional[Dict]]:
    """
    ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—¬ëŸ¬ rowë¥¼ ì²˜ë¦¬ (í”„ë¡œì„¸ìŠ¤ë‹¹ í•œ ë²ˆë§Œ Calculator ìƒì„±)
    """
    calculator = ConfidenceCalculator(group_size=group_size)
    results = []
    
    for idx, logprobs_value in batch_data:
        logprobs_list = convert_logprobs_to_list(logprobs_value)
        
        if logprobs_list is None:
            results.append((idx, None))
            continue
        
        try:
            confidence_scores = calculator.calculate_all_confidence_scores(logprobs_list)
            results.append((idx, confidence_scores))
        except Exception as e:
            logger.warning(f"Row {idx} confidence ê³„ì‚° ì‹¤íŒ¨: {e}")
            results.append((idx, None))
    
    return results


def recalculate_confidence_scores_vectorized(
    df: pd.DataFrame,
    group_size: int = 512,
    chunk_offset: int = 0,
    num_workers: int = 4,
    batch_size: int = 100
) -> tuple:
    """
    ë²¡í„°í™” + ë°°ì¹˜ ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ confidence ì ìˆ˜ ê³„ì‚° (ìµœì í™”)
    """
    if 'logprobs' not in df.columns:
        logger.error("logprobs ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return df, 0, len(df)
    
    column_names = [
        'mean_group_confidence',
        'bottom_10_percent_confidence',
        'tail_confidence',
        'head_confidence',
        'highest_group_confidence',
        'lowest_group_confidence',
        'top_10_percent_confidence',
    ]
    
    n_rows = len(df)
    
    # logprobsë¥¼ numpy arrayë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ)
    logprobs_values = df['logprobs'].values
    
    # ì¸ë±ìŠ¤ì™€ ë°ì´í„°ë¥¼ íŠœí”Œë¡œ ì¤€ë¹„
    data_with_idx = [(i, logprobs_values[i]) for i in range(n_rows)]
    
    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = []
    for i in range(0, n_rows, batch_size):
        batches.append(data_with_idx[i:i + batch_size])
    
    logger.info(f"  â†’ {n_rows:,}ê°œ rowë¥¼ {len(batches):,}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")
    
    # ê²°ê³¼ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ (ì¸ë±ìŠ¤ â†’ confidence_scores)
    results_dict = {}
    valid_count = 0
    invalid_count = 0
    
    # ë©€í‹°í”„ë¡œì„¸ì‹±
    if num_workers > 1:
        process_func = partial(process_batch, group_size=group_size)
        
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            futures = {executor.submit(process_func, batch): batch_idx 
                      for batch_idx, batch in enumerate(batches)}
            
            completed_batches = 0
            for future in as_completed(futures):
                batch_results = future.result()
                
                for idx, confidence_scores in batch_results:
                    results_dict[idx] = confidence_scores
                    if confidence_scores is not None:
                        valid_count += 1
                    else:
                        invalid_count += 1
                
                completed_batches += 1
                if completed_batches % 50 == 0 or completed_batches == len(batches):
                    logger.info(
                        f"  â†’ ì§„í–‰: {completed_batches}/{len(batches)} ë°°ì¹˜ "
                        f"(ìœ íš¨: {valid_count:,}, ë¬´íš¨: {invalid_count:,})"
                    )
        
        # â­ ê³„ì‚° ì™„ë£Œ í›„ ì¦‰ì‹œ logprobs ê´€ë ¨ ë©”ëª¨ë¦¬ í•´ì œ
        del logprobs_values
        del data_with_idx
        
        # DataFrameì—ì„œë„ logprobs ì»¬ëŸ¼ ì œê±°
        if 'logprobs' in df.columns:
            df = df.drop(columns=['logprobs'])
            logger.info(f"  â†’ logprobs ì»¬ëŸ¼ ì œê±° ì™„ë£Œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
    
    else:
        # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬
        calculator = ConfidenceCalculator(group_size=group_size)
        for idx, logprobs_value in data_with_idx:
            logprobs_list = convert_logprobs_to_list(logprobs_value)
            
            if logprobs_list is None:
                results_dict[idx] = None
                invalid_count += 1
            else:
                try:
                    confidence_scores = calculator.calculate_all_confidence_scores(logprobs_list)
                    results_dict[idx] = confidence_scores
                    valid_count += 1
                except Exception as e:
                    logger.warning(f"Row {idx} ê³„ì‚° ì‹¤íŒ¨: {e}")
                    results_dict[idx] = None
                    invalid_count += 1
            
            if (idx + 1) % 1000 == 0:
                logger.info(f"  â†’ ì§„í–‰: {idx + 1}/{n_rows} (ìœ íš¨: {valid_count:,}, ë¬´íš¨: {invalid_count:,})")
        
        # â­ ê³„ì‚° ì™„ë£Œ í›„ ì¦‰ì‹œ logprobs ê´€ë ¨ ë©”ëª¨ë¦¬ í•´ì œ
        del logprobs_values
        del data_with_idx
        
        # DataFrameì—ì„œë„ logprobs ì»¬ëŸ¼ ì œê±°
        if 'logprobs' in df.columns:
            df = df.drop(columns=['logprobs'])
            logger.info(f"  â†’ logprobs ì»¬ëŸ¼ ì œê±° ì™„ë£Œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
    
    # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ (ë²¡í„°í™”)
    for col_name in column_names:
        values = [
            results_dict[i].get(col_name, np.nan) if results_dict[i] is not None else np.nan
            for i in range(n_rows)
        ]
        df[col_name] = values
    
    # ë©”ëª¨ë¦¬ ëª…ì‹œì  í•´ì œ
    del results_dict
    
    return df, valid_count, invalid_count


def read_parquet_chunk_by_rowgroups(
    parquet_file: pq.ParquetFile,
    start_row: int,
    chunk_size: int
) -> pd.DataFrame:
    """
    Row group ë‹¨ìœ„ë¡œ ì •í™•íˆ chunk_sizeë§Œí¼ë§Œ ì½ê¸°
    """
    num_row_groups = parquet_file.num_row_groups
    dfs = []
    current_global_row = 0
    rows_read = 0
    
    for rg_idx in range(num_row_groups):
        rg_metadata = parquet_file.metadata.row_group(rg_idx)
        rg_num_rows = rg_metadata.num_rows
        rg_start = current_global_row
        rg_end = current_global_row + rg_num_rows
        
        # ì´ row groupì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë²”ìœ„ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if rg_end <= start_row:
            # ì•„ì§ ì‹œì‘ ì „
            current_global_row = rg_end
            continue
        
        if rg_start >= start_row + chunk_size:
            # ì´ë¯¸ ì¶©ë¶„íˆ ì½ìŒ
            break
        
        # ì´ row group ì½ê¸°
        table = parquet_file.read_row_group(rg_idx)
        df_rg = table.to_pandas()
        
        # í•„ìš”í•œ ë¶€ë¶„ë§Œ ìŠ¬ë¼ì´ì‹±
        if rg_start < start_row:
            # row group ì‹œì‘ì´ ìš°ë¦¬ ë²”ìœ„ ì´ì „ â†’ ì•ë¶€ë¶„ ì˜ë¼ë‚´ê¸°
            skip_rows = start_row - rg_start
            df_rg = df_rg.iloc[skip_rows:]
        
        if rg_end > start_row + chunk_size:
            # row group ëì´ ìš°ë¦¬ ë²”ìœ„ ì´í›„ â†’ ë’·ë¶€ë¶„ ì˜ë¼ë‚´ê¸°
            take_rows = (start_row + chunk_size) - max(rg_start, start_row)
            df_rg = df_rg.iloc[:take_rows]
        
        dfs.append(df_rg)
        rows_read += len(df_rg)
        current_global_row = rg_end
        
        # ì¶©ë¶„íˆ ì½ì—ˆìœ¼ë©´ ì¤‘ë‹¨
        if rows_read >= chunk_size:
            break
    
    if not dfs:
        return pd.DataFrame()
    
    return pd.concat(dfs, ignore_index=True)


def process_parquet_in_chunks_v2(
    input_path: str,
    output_path: str,
    group_size: int = 512,
    chunk_size: int = 20000,
    num_workers: int = 4,
    batch_size: int = 100
) -> None:
    """
    Parquet íŒŒì¼ì„ ì •í™•íˆ chunk_size ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (v2)
    """
    if os.path.exists(output_path):
        os.remove(output_path)
        logger.info(f"ê¸°ì¡´ ì¶œë ¥ íŒŒì¼ ì‚­ì œ: {output_path}")
    
    if not HAS_PYARROW:
        logger.error("PyArrowê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install pyarrow")
        return
    
    # íŒŒì¼ ì •ë³´ ì¡°íšŒ
    try:
        parquet_file = pq.ParquetFile(input_path, memory_map=False)
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {e}")
        return
    
    num_rows = parquet_file.metadata.num_rows
    num_row_groups = parquet_file.num_row_groups
    size_mb = os.path.getsize(input_path) / (1024 ** 2)
    
    logger.info(f"\n{'='*70}")
    logger.info("ğŸ“Š Parquet íŒŒì¼ ì •ë³´")
    logger.info(f"{'='*70}")
    logger.info(f"  íŒŒì¼ í¬ê¸°: {size_mb:.2f} MB")
    logger.info(f"  ì´ í–‰ ìˆ˜: {num_rows:,}")
    logger.info(f"  Row groups: {num_row_groups}")
    logger.info(f"\nâš™ï¸  ì²˜ë¦¬ ì„¤ì •")
    logger.info(f"  ì²­í¬ í¬ê¸°: {chunk_size:,} rows")
    logger.info(f"  ì›Œì»¤ ìˆ˜: {num_workers}")
    logger.info(f"  ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"  ì˜ˆìƒ ì²­í¬ ìˆ˜: {(num_rows + chunk_size - 1) // chunk_size}")
    logger.info(f"{'='*70}\n")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    # ìŠ¤í‚¤ë§ˆ ì¤€ë¹„
    first_table = parquet_file.read_row_group(0)
    first_df = first_table.to_pandas()
    
    if 'logprobs' in first_df.columns:
        first_df = first_df.drop(columns=['logprobs'])
    
    # confidence ì»¬ëŸ¼ ì¶”ê°€
    column_names = [
        'mean_group_confidence',
        'bottom_10_percent_confidence',
        'tail_confidence',
        'head_confidence',
        'highest_group_confidence',
        'lowest_group_confidence',
        'top_10_percent_confidence',
    ]
    for col_name in column_names:
        first_df[col_name] = 0.0
    
    schema = pa.Schema.from_pandas(first_df)
    
    # Writer ìƒì„±
    writer = pq.ParquetWriter(
        output_path,
        schema=schema,
        compression='zstd',
        write_statistics=True
    )
    
    total_valid = 0
    total_invalid = 0
    processed_rows = 0
    
    try:
        # chunk_size ë‹¨ìœ„ë¡œ ì •í™•íˆ ë‚˜ëˆ ì„œ ì²˜ë¦¬
        chunk_idx = 0
        
        for start_row in range(0, num_rows, chunk_size):
            end_row = min(start_row + chunk_size, num_rows)
            expected_rows = end_row - start_row
            
            logger.info(f"\n{'='*70}")
            logger.info(f"ğŸ”„ ì²­í¬ {chunk_idx + 1} / {(num_rows + chunk_size - 1) // chunk_size}")
            logger.info(f"{'='*70}")
            logger.info(f"  ë²”ìœ„: row {start_row:,} ~ {end_row:,} (ì´ {expected_rows:,}ê°œ)")
            
            # ì²­í¬ ì½ê¸°
            df_chunk = read_parquet_chunk_by_rowgroups(
                parquet_file,
                start_row,
                chunk_size
            )
            
            if len(df_chunk) == 0:
                logger.warning(f"  âš ï¸  ì²­í¬ {chunk_idx + 1}ì—ì„œ ë°ì´í„°ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                chunk_idx += 1
                continue
            
            logger.info(f"  âœ“ {len(df_chunk):,}ê°œ row ë¡œë“œ ì™„ë£Œ")
            
            # Confidence ê³„ì‚°
            df_chunk, valid, invalid = recalculate_confidence_scores_vectorized(
                df_chunk,
                group_size=group_size,
                chunk_offset=start_row,
                num_workers=num_workers,
                batch_size=batch_size
            )
            
            total_valid += valid
            total_invalid += invalid
            processed_rows += len(df_chunk)
            
            # í…Œì´ë¸”ë¡œ ë³€í™˜ í›„ ì €ì¥ (logprobsëŠ” ì´ë¯¸ ì œê±°ë¨)
            table = pa.Table.from_pandas(df_chunk, schema=schema)
            writer.write_table(table)
            
            logger.info(f"  âœ… ì €ì¥ ì™„ë£Œ (ìœ íš¨: {valid:,}, ë¬´íš¨: {invalid:,})")
            logger.info(f"  ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {processed_rows:,}/{num_rows:,} ({processed_rows/num_rows*100:.1f}%)")
            
            del df_chunk, table
            chunk_idx += 1
    
    finally:
        writer.close()
    
    output_size_mb = os.path.getsize(output_path) / (1024 ** 2)
    
    logger.info(f"\n{'='*70}")
    logger.info("âœ… ì²˜ë¦¬ ì™„ë£Œ!")
    logger.info(f"{'='*70}")
    logger.info(f"  ì´ ì²˜ë¦¬: {processed_rows:,} / {num_rows:,} rows")
    logger.info(f"  ìœ íš¨: {total_valid:,} ({total_valid/processed_rows*100:.1f}%)")
    logger.info(f"  ë¬´íš¨: {total_invalid:,} ({total_invalid/processed_rows*100:.1f}%)")
    logger.info(f"  ì¶œë ¥ íŒŒì¼: {output_path}")
    logger.info(f"  ì¶œë ¥ í¬ê¸°: {output_size_mb:.2f} MB")
    logger.info(f"{'='*70}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Parquet íŒŒì¼ì˜ logprobsë¥¼ ì‚¬ìš©í•˜ì—¬ confidence ì ìˆ˜ë¥¼ ì¬ê³„ì‚° (ìµœì í™” ë²„ì „ v2)"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="ì…ë ¥ Parquet íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ì¶œë ¥ Parquet íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--group-size",
        type=int,
        default=512,
        help="í† í° ê·¸ë£¹ í¬ê¸° (ê¸°ë³¸ê°’: 512)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=40000,
        help="ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 20000)"
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=1,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=400,
        help="ê° ì›Œì»¤ì˜ ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)"
    )
    
    args = parser.parse_args()
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ìë™ ìƒì„±
    if args.output is None:
        input_path = Path(args.input)
        output_filename = input_path.stem + "_recalculated" + input_path.suffix
        args.output = str(input_path.parent / output_filename)
        logger.info(f"ì¶œë ¥ íŒŒì¼: {args.output}\n")
    
    # ì²˜ë¦¬ ì‹œì‘
    process_parquet_in_chunks_v2(
        input_path=args.input,
        output_path=args.output,
        group_size=args.group_size,
        chunk_size=args.chunk_size,
        num_workers=args.num_workers,
        batch_size=args.batch_size
    )
    
    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()