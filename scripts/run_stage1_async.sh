#!/bin/bash
# Stage 1 ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (2ê°œ GPU ìµœì í™”)
# ê°œì„ íŒ: ëª¨ë“  ì„±ëŠ¥/ì•ˆì •ì„± í™˜ê²½ë³€ìˆ˜ í¬í•¨

# =============================================================================
# ê¸°ë³¸ í™˜ê²½ ì„¤ì •
# =============================================================================
export PATH="$HOME/.local/bin:$PATH"
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export HF_HOME=/mnt/data1/models/nlp/huggingface_cache
export TRANSFORMERS_CACHE=/mnt/data1/models/nlp/huggingface_cache
export HF_DATASETS_CACHE=/mnt/data1/datasets/nlp/cache
export VLLM_USE_FLASHINFER=1
export PYTHONPATH=/mnt/data1/projects/Conf_Agg
export UV_CACHE_DIR=/mnt/data1/.uv-cache

# =============================================================================
# Stage 1 ì „ìš© ì„¤ì •
# =============================================================================

# ìƒ˜í”Œë§ ì œí•œ (0 = ì „ì²´ ë°ì´í„° ì‚¬ìš©)
# - 0: ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
# - 100: ì²˜ìŒ 100ê°œ ë¬¸ì œë§Œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ìš©)
# - 1000: ì²˜ìŒ 1000ê°œ ë¬¸ì œë§Œ ì²˜ë¦¬
export SAMPLE_LIMIT=0

# ë°±í”„ë ˆì…” ì œì–´ (ê¸°ë³¸: configì˜ max_num_seqs * 1.5)
# - vLLM ì—”ì§„ì— ë™ì‹œ ë“±ë¡ ê°€ëŠ¥í•œ ìµœëŒ€ ìš”ì²­ ìˆ˜
# - ë„ˆë¬´ ë†’ìœ¼ë©´: ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM) ìœ„í—˜
# - ë„ˆë¬´ ë‚®ìœ¼ë©´: GPU í™œìš©ë¥  ì €í•˜
# - ê¶Œì¥ê°’: max_num_seqsì˜ 1.5~2ë°°
# - configì˜ max_num_seqs=112 ê¸°ì¤€ â†’ 168 (1.5ë°°)
# - V1 APIì—ì„œëŠ” ê° completionì´ ë³„ë„ ì¹´ìš´íŠ¸ë˜ë¯€ë¡œ ë‚®ê²Œ ì„¤ì •
export MAX_INFLIGHT=168

# ì§„í–‰ ìƒí™© ë¡œê¹… ì£¼ê¸° (ê¸°ë³¸: 10)
# - Nê°œ completion ì²˜ë¦¬ë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
# - V1 API: ê° ë¬¸ì œë‹¹ nê°œ completion ìƒì„± (ì˜ˆ: n=8, 5ë¬¸ì œ â†’ 40 completions)
# - ë¡œê·¸ íŒŒì¼ í¬ê¸°ì™€ ê°€ì‹œì„±ì˜ ê· í˜•
# - ë„ˆë¬´ ë‚®ìœ¼ë©´: ë¡œê·¸ íŒŒì¼ ë¹„ëŒ€í™”
# - ë„ˆë¬´ ë†’ìœ¼ë©´: ì§„í–‰ ìƒí™© íŒŒì•… ì–´ë ¤ì›€
# - ê¶Œì¥ê°’: 10~50
export SNAPSHOT_EVERY=10

# JSONL flush ì£¼ê¸° (ê¸°ë³¸: 100)
# - Nê°œ í–‰ ì“°ê¸°ë§ˆë‹¤ ë””ìŠ¤í¬ì— ê°•ì œ ë™ê¸°í™” (fsync)
# - ë„ˆë¬´ ë‚®ìœ¼ë©´: I/O ë³‘ëª© (ì„±ëŠ¥ ì €í•˜)
# - ë„ˆë¬´ ë†’ìœ¼ë©´: ë¹„ì •ìƒ ì¢…ë£Œ ì‹œ ë°ì´í„° ì†ì‹¤ ìœ„í—˜
# - ê¶Œì¥ê°’: 100~500
export FLUSH_EVERY=100

# ì¬ì‹œì‘ ê¸°ëŠ¥ í™œì„±í™” (ê¸°ë³¸: true)
# - true: ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ë¡œë“œ, ì¤‘ë³µ ì‘ë‹µ ìë™ ìŠ¤í‚µ
# - false: ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œ, ì²˜ìŒë¶€í„° ì¬ìƒì„±
# - ë¹„ì •ìƒ ì¢…ë£Œ í›„ ì¬ì‹¤í–‰ ì‹œ ë°˜ë“œì‹œ true ìœ ì§€
export RESUME=true

# vLLM ë¡œê¹… ë ˆë²¨ (ê¸°ë³¸: INFO)
# - ERROR: ì—ëŸ¬ë§Œ ì¶œë ¥
# - WARNING: ê²½ê³  ì´ìƒ ì¶œë ¥
# - INFO: ì¼ë°˜ ì •ë³´ ì¶œë ¥ (ê¶Œì¥)
# - DEBUG: ìƒì„¸ ë””ë²„ê·¸ ì •ë³´ (ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥)
export VLLM_LOGGING_LEVEL=INFO

# =============================================================================
# ì‹¤í–‰ ì„¤ì •
# =============================================================================
# íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ë¡œê·¸ ë””ë ‰í† ë¦¬ êµ¬ë¶„ìš©)
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_DIR="outputs/logs/sample_${SAMPLE_LIMIT}_${TIMESTAMP}"
mkdir -p "$LOG_DIR"

CONFIG_PATH="./config" 
CONFIG_NAME="config"
TOTAL_SHARDS=2  # GPU 2ê°œ ì‚¬ìš©

# =============================================================================
# ì‹œì‘ ë¡œê·¸
# =============================================================================
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "Stage 1 Data Generation (Async Improved Version)" | tee -a "$LOG_DIR/stage1_background.log"
echo "ì‹œì‘ ì‹œê°: $(date)" | tee -a "$LOG_DIR/stage1_background.log"
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

# í™˜ê²½ë³€ìˆ˜ ì¶œë ¥
echo "í™˜ê²½ ì„¤ì •:" | tee -a "$LOG_DIR/stage1_background.log"
echo "  SAMPLE_LIMIT: $SAMPLE_LIMIT" | tee -a "$LOG_DIR/stage1_background.log"
echo "  MAX_INFLIGHT: $MAX_INFLIGHT" | tee -a "$LOG_DIR/stage1_background.log"
echo "  SNAPSHOT_EVERY: $SNAPSHOT_EVERY" | tee -a "$LOG_DIR/stage1_background.log"
echo "  FLUSH_EVERY: $FLUSH_EVERY" | tee -a "$LOG_DIR/stage1_background.log"
echo "  RESUME: $RESUME" | tee -a "$LOG_DIR/stage1_background.log"
echo "  TOTAL_SHARDS: $TOTAL_SHARDS" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

# =============================================================================
# ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
# =============================================================================
echo "ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ í™•ì¸ ë° ì •ë¦¬..." | tee -a "$LOG_DIR/stage1_background.log"
pkill -f "stage1_generate_async.py" && echo "ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œë¨" || echo "ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œì„¸ìŠ¤ ì—†ìŒ"
sleep 3

# =============================================================================
# GPU ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì‹œì‘
# =============================================================================
# ê° GPUì— ë…ë¦½ì ì¸ torch compile ìºì‹œ ë””ë ‰í† ë¦¬ í• ë‹¹
TORCH_COMPILE_GPU0="/tmp/torch_compile_gpu0_$$"
TORCH_COMPILE_GPU1="/tmp/torch_compile_gpu1_$$"

# ë””ë ‰í† ë¦¬ ìƒì„± ë° ê¶Œí•œ ì„¤ì •
mkdir -p "$TORCH_COMPILE_GPU0" "$TORCH_COMPILE_GPU1"
chmod 755 "$TORCH_COMPILE_GPU0" "$TORCH_COMPILE_GPU1"

echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "Starting $TOTAL_SHARDS parallel vLLM worker processes..." | tee -a "$LOG_DIR/stage1_background.log"
echo "Torch compile directories:" | tee -a "$LOG_DIR/stage1_background.log"
echo "  GPU 0: $TORCH_COMPILE_GPU0" | tee -a "$LOG_DIR/stage1_background.log"
echo "  GPU 1: $TORCH_COMPILE_GPU1" | tee -a "$LOG_DIR/stage1_background.log"
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

# GPU 0 (Shard 0)
echo "[GPU 0] Shard 0 ì‹œì‘..." | tee -a "$LOG_DIR/stage1_background.log"
CUDA_VISIBLE_DEVICES=0 \
VLLM_WORKER_MULTIPROC_METHOD=spawn \
TORCH_COMPILE_DIR="$TORCH_COMPILE_GPU0" \
nohup uv run python scripts/stage1_generate_async.py \
    --config-path $CONFIG_PATH \
    --config-name $CONFIG_NAME \
    --gpu-id "0" \
    --shard-id 0 \
    --total-shards $TOTAL_SHARDS \
> "$LOG_DIR/stage1_shard_0.log" 2>&1 &
PID_0=$!
echo $PID_0 > "$LOG_DIR/stage1_shard_0_pid.txt"
echo "  PID: $PID_0" | tee -a "$LOG_DIR/stage1_background.log"

sleep 5

# GPU 1 (Shard 1)
echo "[GPU 1] Shard 1 ì‹œì‘..." | tee -a "$LOG_DIR/stage1_background.log"
CUDA_VISIBLE_DEVICES=1 \
VLLM_WORKER_MULTIPROC_METHOD=spawn \
TORCH_COMPILE_DIR="$TORCH_COMPILE_GPU1" \
nohup uv run python scripts/stage1_generate_async.py \
    --config-path $CONFIG_PATH \
    --config-name $CONFIG_NAME \
    --gpu-id "1" \
    --shard-id 1 \
    --total-shards $TOTAL_SHARDS \
> "$LOG_DIR/stage1_shard_1.log" 2>&1 &
PID_1=$!
echo $PID_1 > "$LOG_DIR/stage1_shard_1_pid.txt"
echo "  PID: $PID_1" | tee -a "$LOG_DIR/stage1_background.log"

echo "" | tee -a "$LOG_DIR/stage1_background.log"

# =============================================================================
# ì‹¤í–‰ ì™„ë£Œ ì•ˆë‚´
# =============================================================================
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "âœ… Stage 1 ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œì‘ ì™„ë£Œ" | tee -a "$LOG_DIR/stage1_background.log"
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "ğŸ“ ë¡œê·¸ íŒŒì¼:" | tee -a "$LOG_DIR/stage1_background.log"
echo "  Shard 0: $LOG_DIR/stage1_shard_0.log" | tee -a "$LOG_DIR/stage1_background.log"
echo "  Shard 1: $LOG_DIR/stage1_shard_1.log" | tee -a "$LOG_DIR/stage1_background.log"
echo "  Background: $LOG_DIR/stage1_background.log" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "ğŸ” ëª¨ë‹ˆí„°ë§ ëª…ë ¹ì–´:" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # ì „ì²´ ë¡œê·¸ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  tail -f $LOG_DIR/stage1_shard_*.log" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # Shard 0ë§Œ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  tail -f $LOG_DIR/stage1_shard_0.log" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # ì§„í–‰ë¥ ë§Œ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  tail -f $LOG_DIR/stage1_shard_*.log | grep 'ì§„í–‰:'" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # ë©”ëª¨ë¦¬ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  tail -f $LOG_DIR/stage1_shard_*.log | grep 'ë©”ëª¨ë¦¬:'" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # GPU ìƒíƒœ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  nvidia-smi -l 1" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "âš™ï¸  í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬:" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # í”„ë¡œì„¸ìŠ¤ í™•ì¸" | tee -a "$LOG_DIR/stage1_background.log"
echo "  ps aux | grep stage1_generate_async.py" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # íŠ¹ì • ìƒ¤ë“œ ì¢…ë£Œ" | tee -a "$LOG_DIR/stage1_background.log"
echo "  kill \$(cat $LOG_DIR/stage1_shard_0_pid.txt)" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"
echo "  # ëª¨ë“  ìƒ¤ë“œ ì¢…ë£Œ" | tee -a "$LOG_DIR/stage1_background.log"
echo "  pkill -f stage1_generate_async.py" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "ğŸ“Š ì¶œë ¥ íŒŒì¼ ìœ„ì¹˜:" | tee -a "$LOG_DIR/stage1_background.log"
if [ "$SAMPLE_LIMIT" -eq 0 ]; then
    OUTPUT_PATH="/mnt/data1/datasets/nlp/conf_agg/generated"
else
    OUTPUT_PATH="/mnt/data1/datasets/nlp/conf_agg/generated/sample_${SAMPLE_LIMIT}"
fi
echo "  $OUTPUT_PATH/" | tee -a "$LOG_DIR/stage1_background.log"
echo "  â”œâ”€â”€ raw_generated_shard_0.parquet" | tee -a "$LOG_DIR/stage1_background.log"
echo "  â”œâ”€â”€ raw_generated_shard_0.jsonl" | tee -a "$LOG_DIR/stage1_background.log"
echo "  â”œâ”€â”€ raw_generated_shard_1.parquet" | tee -a "$LOG_DIR/stage1_background.log"
echo "  â””â”€â”€ raw_generated_shard_1.jsonl" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "ğŸ§¹ Cleanup (ì‘ì—… ì™„ë£Œ í›„ ìˆ˜ë™ ì‹¤í–‰):" | tee -a "$LOG_DIR/stage1_background.log"
echo "  rm -rf $TORCH_COMPILE_GPU0 $TORCH_COMPILE_GPU1" | tee -a "$LOG_DIR/stage1_background.log"
echo "" | tee -a "$LOG_DIR/stage1_background.log"

echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"
echo "ì‹¤í–‰ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”." | tee -a "$LOG_DIR/stage1_background.log"
echo "=============================================================================" | tee -a "$LOG_DIR/stage1_background.log"