"""
Stage 1: ì›ì‹œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ë‹¨ìˆœ ë°ì´í„° ë³‘ë ¬ ìµœì í™”)
"""
import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import torch
from transformers import AutoTokenizer
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data.confidence import ConfidenceCalculator
from src.data.dataset import RawDataset
from src.utils.logging import setup_logging
from vllm import LLM, SamplingParams

logger = logging.getLogger(__name__)


def simple_extract_topk(gen_logprobs: List[Dict[int, Any]], k: int) -> List[List[float]]:
    """ìµœì í™”ëœ logprob ì¶”ì¶œ í•¨ìˆ˜ (float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)"""
    if not gen_logprobs:
        return []
    
    results = []
    for token_step_dict in gen_logprobs:
        if not token_step_dict:
            results.append([])
            continue
        
        lps = []
        for i, entry in enumerate(token_step_dict.values()):
            if i >= k:
                break
            if hasattr(entry, "logprob"):
                lps.append(float(entry.logprob))
            elif isinstance(entry, dict) and "logprob" in entry:
                lps.append(float(entry["logprob"]))
        
        if lps:
            # float16 ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
            results.append(np.array(lps, dtype=np.float16).tolist())
        else:
            results.append([])
    
    return results


def main_worker(cfg: DictConfig, args: argparse.Namespace) -> None:
    """
    ì˜¤í”„ë¼ì¸ ë°°ì¹˜ ì›Œì»¤ ë©”ì¸ í•¨ìˆ˜
    - argsë¡œë¶€í„° gpu_idì™€ shard_idë¥¼ ë°›ì•„ ì‘ì—…ì„ ë¶„í•  ì²˜ë¦¬
    """
    
    # ë¡œê¹… ì„¤ì • (ìƒ¤ë“œë³„ íŒŒì¼ êµ¬ë¶„)
    log_file = os.path.join(cfg.paths.log_dir, f"stage1_generate_shard_{args.shard_id}.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    # ì˜¬ë°”ë¥¸ ë¡œê±° ì‚¬ìš©
    logger = logging.getLogger("conf_agg_llm")
    
    logger.info(f"ğŸš€ [Shard {args.shard_id} | GPU {args.gpu_id}] Stage 1: ì›ì‹œ ë°ì´í„° ìƒì„± ì‹œì‘")
    logger.info(f"ì „ì²´ ì„¤ì •: {OmegaConf.to_yaml(cfg)}")
    
    try:
        # 1. vLLM ëª¨ë¸ ë¡œë“œ (Ray Serve ëŒ€ì‹  ì§ì ‘ ë¡œë“œ)
        logger.info(f"[Shard {args.shard_id}] vLLM ëª¨ë¸ ë¡œë“œ ì¤‘: {cfg.model.base_model}")
        vllm_config = cfg.data.raw_dataset.vllm
        llm = LLM(
            model=cfg.model.base_model,
            tensor_parallel_size=1,  # TP=1 (ë‹¨ì¼ GPU)
            gpu_memory_utilization=vllm_config.gpu_memory_utilization,
            max_model_len=vllm_config.max_model_len,
            dtype=vllm_config.dtype,
            trust_remote_code=vllm_config.trust_remote_code,
            max_num_batched_tokens=vllm_config.get("max_num_batched_tokens", 16384),
            max_num_seqs=vllm_config.get("max_num_seqs", 256),
            enforce_eager=vllm_config.get("enforce_eager", False),
            disable_custom_all_reduce=vllm_config.get("disable_custom_all_reduce", True),
        )
        tokenizer = AutoTokenizer.from_pretrained(
            cfg.model.base_model,
            trust_remote_code=vllm_config.trust_remote_code
        )
        logger.info(f"[Shard {args.shard_id}] vLLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(cfg.paths.data_dir, f"generated")
        output_dir = os.path.join(output_dir, f"sample_{os.environ.get("SAMPLE_LIMIT")}")
        os.makedirs(output_dir, exist_ok=True)
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ (ì „ì²´ 40K)
        raw_data_path = os.path.join(cfg.paths.data_dir, "raw", "deepscaler.jsonl")
        if not os.path.exists(raw_data_path):
            logger.error(f"ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_data_path}")
            return
        
        raw_dataset = RawDataset(raw_data_path)
        logger.info(f"ì „ì²´ ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ (ëœë¤ ìƒ˜í”Œë§)
        sample_limit_env = os.environ.get("SAMPLE_LIMIT")
        sample_limit = int(sample_limit_env) if sample_limit_env and sample_limit_env.isdigit() else 0
        
        if sample_limit > 0 and sample_limit < len(raw_dataset):
            # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ì¸ë±ìŠ¤ ì„ íƒ
            np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ëœë¤ ìƒ˜í”Œë§ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
            selected_indices = np.random.choice(len(raw_dataset), size=sample_limit, replace=False)
            selected_indices = sorted(selected_indices)  # ì •ë ¬í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
            logger.info(f"SAMPLE_LIMIT ì ìš©: ì „ì²´ {len(raw_dataset)}ê°œ ì¤‘ ëœë¤ìœ¼ë¡œ {sample_limit}ê°œ ë¬¸ì œ ì„ íƒ")
            logger.info(f"ì„ íƒëœ ì¸ë±ìŠ¤ ë²”ìœ„: {min(selected_indices)} ~ {max(selected_indices)}")
        else:
            selected_indices = list(range(len(raw_dataset)))
            logger.info(f"ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        total_items = len(selected_indices)
        
        # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        confidence_calculator = ConfidenceCalculator(
            group_size=cfg.data.raw_dataset.confidence.group_size
        )
        
        instruction = "Please reason step by step, and put your final answer within \\boxed{}."
        
        problems: List[Dict] = []
        texts: List[str] = []
        
        logger.info("ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        for idx in selected_indices:
            problem_data = raw_dataset[idx]
            problem_id = problem_data.get("id", f"problem_{idx}")
            problem_text = problem_data.get("problem", "")
            ground_truth = problem_data.get("answer", "")
            messages = [{"role": "user", "content": f"{problem_text}\n\n{instruction}"}]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True,  # Qwen ê³„ì—´ ëª¨ë¸ì— í•„ìš”í•  ìˆ˜ ìˆìŒ
            )
            problems.append({
                "problem_id": problem_id,
                "problem_text": problem_text,
                "ground_truth": ground_truth,
            })
            texts.append(text)
        logger.info(f"ì´ {len(texts)}ê°œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")

        # 2. ì‘ì—… ë¶„í•  (Sharding)
        # ì´ ì›Œì»¤(ìƒ¤ë“œ)ì— í• ë‹¹ëœ ì‘ì—…ë§Œ í•„í„°ë§
        my_problems = problems[args.shard_id::args.total_shards]
        my_texts = texts[args.shard_id::args.total_shards]
        
        logger.info(f"[Shard {args.shard_id}] ì‘ì—… ë¶„í•  ì™„ë£Œ. ì´ ìƒ¤ë“œì—ì„œ {len(my_texts)}ê°œ ë¬¸ì œ ì²˜ë¦¬ (1/{args.total_shards})")

        # 3. ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° êµ¬ì„±
        gen_cfg = cfg.data.raw_dataset.generation
        sampling_params = SamplingParams(
            n=gen_cfg.num_responses_per_problem,  # ê° ë¬¸ì œë‹¹ ì‘ë‹µ ìˆ˜
            temperature=gen_cfg.temperature,
            top_p=gen_cfg.top_p,
            top_k=gen_cfg.top_k,
            min_p=gen_cfg.min_p,
            max_tokens=gen_cfg.max_tokens,
            logprobs=gen_cfg.logprobs,  # top-k logprob ì €ì¥
        )
        gen_cfg_logprobs = gen_cfg.logprobs

        # 4. vLLM ì¼ê´„ ì¶”ë¡  ì‹¤í–‰ (ëª¨ë“  ìš”ì²­ì„ í•œ ë²ˆì— ë˜ì§€ê³  vLLMì´ ìë™ ë°°ì¹˜ ì²˜ë¦¬)
        logger.info(f"[Shard {args.shard_id}] vLLM ì¶”ë¡  ì‹œì‘... (ì…ë ¥ {len(my_texts)}ê°œ ë¬¸ì œ, ê° ë¬¸ì œë‹¹ {gen_cfg.num_responses_per_problem}ê°œ ì‘ë‹µ)")
        outputs = llm.generate(my_texts, sampling_params)
        logger.info(f"[Shard {args.shard_id}] vLLM ì¶”ë¡  ì™„ë£Œ. ì´ {len(outputs)}ê°œ í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ìˆ˜ì‹ .")
        
        # 5. í›„ì²˜ë¦¬ ë° ê²°ê³¼ ì·¨í•© (CPU ì‘ì—…)
        all_results = []
        logger.info(f"[Shard {args.shard_id}] ê²°ê³¼ í›„ì²˜ë¦¬ ì‹œì‘...")
        
        pbar = tqdm(total=len(outputs), desc=f"Shard {args.shard_id} Post-processing", ncols=100)
        for pi, req_out in enumerate(outputs):
            base_meta = my_problems[pi]  # ìƒ¤ë“œì— í• ë‹¹ëœ ë¬¸ì œ ë©”íƒ€ë°ì´í„°
            
            for oi, gen in enumerate(req_out.outputs):
                
                # ìµœì í™”ëœ logprob ì¶”ì¶œ
                topk = simple_extract_topk(gen.logprobs, gen_cfg_logprobs)
                
                # ì‹ ë¢°ë„ ê³„ì‚°
                confidence_scores = confidence_calculator.calculate_all_confidence_scores(topk)
                
                all_results.append({
                    "problem_id": base_meta["problem_id"],
                    "problem_text": base_meta["problem_text"],
                    "ground_truth": base_meta["ground_truth"],
                    "response_id": f"{base_meta['problem_id']}_resp_{oi}",
                    "generated_text": gen.text,
                    "output_token_count": len(gen.token_ids) if hasattr(gen, "token_ids") else 0,
                    "logprobs": topk,
                    "worker_gpu": args.gpu_id,  # GPU ID ì €ì¥
                    "worker_replica": f"shard_{args.shard_id}",  # Shard ID ì €ì¥
                    **confidence_scores,
                })
            pbar.update(1)
        pbar.close()

        # 6. ê²°ê³¼ ì €ì¥ (ìƒ¤ë“œë³„ íŒŒì¼)
        df = pd.DataFrame(all_results)
        parquet_path = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}.parquet")
        df.to_parquet(parquet_path, index=False, compression="zstd")
        
        logger.info(f"âœ… [Shard {args.shard_id}] Stage 1 ì™„ë£Œ: {len(df)}ê°œ ê²°ê³¼ ì €ì¥")
        logger.info(f"Parquet ì €ì¥ ìœ„ì¹˜: {parquet_path}")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        logger.info(f"ìƒì„±ëœ ì‘ë‹µ ìˆ˜: {len(df)}")
        logger.info(f"ë¬¸ì œ ìˆ˜: {df['problem_id'].nunique()}")
        logger.info(f"ë¬¸ì œë‹¹ í‰ê·  ì‘ë‹µ ìˆ˜: {len(df) / df['problem_id'].nunique():.1f}")
        
        if 'output_token_count' in df.columns:
            try:
                total_tokens = int(df['output_token_count'].fillna(0).sum())
                mean_tokens = float(df['output_token_count'].fillna(0).mean())
                min_tokens = int(df['output_token_count'].fillna(0).min()) if len(df) > 0 else 0
                max_tokens = int(df['output_token_count'].fillna(0).max()) if len(df) > 0 else 0
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ í•©ê³„: {total_tokens}")
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ í‰ê· : {mean_tokens:.1f}")
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ ìµœì†Œ/ìµœëŒ€: {min_tokens}/{max_tokens}")
                
                # max_tokensì™€ ê°™ì€ í† í° ìˆ˜ë¥¼ ê°€ì§„ ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜ ì¶œë ¥
                max_tokens_limit = gen_cfg.max_tokens
                max_tokens_count = int((df['output_token_count'].fillna(0) == max_tokens_limit).sum())
                logger.info(f"ìµœëŒ€ í† í° ìˆ˜({max_tokens_limit})ì— ë„ë‹¬í•œ ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜: {max_tokens_count}")
            except Exception:
                pass
        
        # ìƒ˜í”Œ 1ê°œ ì „ì²´ ì¶œë ¥
        if len(df) > 0:
            logger.info("=" * 80)
            logger.info("ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥ (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤):")
            logger.info("=" * 80)
            sample = df.iloc[0]
            logger.info(f"Problem ID: {sample['problem_id']}")
            logger.info(f"Problem Text: {sample['problem_text']}")
            logger.info(f"Ground Truth: {sample['ground_truth']}")
            logger.info(f"Generated Text: {sample['generated_text']}")
            logger.info(f"Output Token Count: {sample['output_token_count']}")
            logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"[Shard {args.shard_id}] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    # argparseë¡œ ëŸ°ì²˜ì˜ ì¸ìˆ˜ë¥¼ ë°›ìŒ
    parser = argparse.ArgumentParser()
    parser.add_argument("--config-path", type=str, required=True, help="Hydra config directory (e.g., ../config)")
    parser.add_argument("--config-name", type=str, required=True, help="Hydra config name (e.g., config)")
    parser.add_argument("--gpu-id", type=str, required=True, help="GPU ID (e.g., '0')")
    parser.add_argument("--shard-id", type=int, required=True, help="Data shard index (0, 1, 2, 3)")
    parser.add_argument("--total-shards", type=int, default=4, help="Total number of shards")
    args = parser.parse_args()

    # 1. GPU ê²©ë¦¬
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id

    # 2. Hydra ìˆ˜ë™ ì´ˆê¸°í™”
    # config_pathëŠ” ë””ë ‰í† ë¦¬ì´ë¯€ë¡œ Path ê°ì²´ë¡œ ë³€í™˜
    config_dir = Path(args.config_path).resolve()
    # hydra.initialize_config_dirì€ ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
    hydra.core.global_hydra.GlobalHydra.instance().clear()  # Hydra ì´ˆê¸°í™”
    hydra.initialize_config_dir(config_dir=str(config_dir), version_base=None)
    
    cfg = hydra.compose(config_name=args.config_name)

    # 3. ë©”ì¸ ì›Œì»¤ í•¨ìˆ˜ ì‹¤í–‰
    main_worker(cfg, args)