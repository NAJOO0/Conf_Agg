"""
Stage 1: ì›ì‹œ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (AsyncLLMEngineìœ¼ë¡œ continuous batching + ìŠ¤íŠ¸ë¦¬ë° ì €ì¥)
"""
import os
import sys
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, AsyncGenerator
import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import torch
from transformers import AutoTokenizer
from tqdm.asyncio import tqdm
import asyncio
import aiofiles
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data.confidence import ConfidenceCalculator
from src.data.dataset import RawDataset
from src.utils.logging import setup_logging
from vllm import AsyncLLMEngine, AsyncEngineArgs, SamplingParams
from vllm.outputs import RequestOutput

logger = logging.getLogger(__name__)


def simple_extract_topk(gen_logprobs: List[Dict[int, Any]], k: int) -> List[List[float]]:
    """ìµœì í™”ëœ logprob ì¶”ì¶œ í•¨ìˆ˜ (float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)"""
    if not gen_logprobs:
        return []
    
    results = []
    for token_step_dict in gen_logprobs:
        if not token_step_dict:
            results.append([])
            continue
        
        lps = []
        for i, entry in enumerate(token_step_dict.values()):
            if i >= k:
                break
            if hasattr(entry, "logprob"):
                lps.append(float(entry.logprob))
            elif isinstance(entry, dict) and "logprob" in entry:
                lps.append(float(entry["logprob"]))
        
        if lps:
            # float16 ë³€í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
            results.append(np.array(lps, dtype=np.float16).tolist())
        else:
            results.append([])
    
    return results


def process_single_output(
    problem_meta: Dict,
    completion: Any,
    response_idx: int,
    confidence_calculator: ConfidenceCalculator,
    gen_cfg_logprobs: int,
    args: argparse.Namespace
) -> Dict:
    """ë‹¨ì¼ ì¶œë ¥ì„ ì²˜ë¦¬í•˜ì—¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
    
    # ìµœì í™”ëœ logprob ì¶”ì¶œ
    topk = simple_extract_topk(completion.logprobs, gen_cfg_logprobs)
    
    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence_scores = confidence_calculator.calculate_all_confidence_scores(topk)
    
    return {
        "problem_id": problem_meta["problem_id"],
        "problem_text": problem_meta["problem_text"],
        "ground_truth": problem_meta["ground_truth"],
        "response_id": f"{problem_meta['problem_id']}_resp_{response_idx}",
        "generated_text": completion.text,
        "output_token_count": len(completion.token_ids) if hasattr(completion, "token_ids") else 0,
        "logprobs": topk,
        "worker_gpu": args.gpu_id,
        "worker_replica": f"shard_{args.shard_id}",
        **confidence_scores,
    }


async def async_generation_worker(cfg: DictConfig, args: argparse.Namespace):
    """ë¹„ë™ê¸° ì—”ì§„ìœ¼ë¡œ continuous batching ìœ ì§€ + ìŠ¤íŠ¸ë¦¬ë° ì €ì¥"""
    
    # ë¡œê¹… ì„¤ì • (ìƒ¤ë“œë³„ íŒŒì¼ êµ¬ë¶„)
    log_file = os.path.join(cfg.paths.log_dir, f"stage1_generate_shard_{args.shard_id}.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    # ì˜¬ë°”ë¥¸ ë¡œê±° ì‚¬ìš©
    logger = logging.getLogger("conf_agg_llm")
    
    logger.info(f"ğŸš€ [Shard {args.shard_id} | GPU {args.gpu_id}] Stage 1: ì›ì‹œ ë°ì´í„° ìƒì„± ì‹œì‘ (Async Mode)")
    logger.info(f"ì „ì²´ ì„¤ì •: {OmegaConf.to_yaml(cfg)}")
    
    try:
        # 1. CUDA ì»¨í…ìŠ¤íŠ¸ ì¬í™•ì¸ (í”„ë¡œì„¸ìŠ¤ ê°„ ê²©ë¦¬ ë³´ì¥)
        if torch.cuda.is_available():
            # CUDA_VISIBLE_DEVICESë¡œ ì¸í•´ 0ë²ˆì´ ì‹¤ì œ GPU
            current_device = torch.cuda.current_device()
            logger.info(f"[Shard {args.shard_id}] CUDA ì»¨í…ìŠ¤íŠ¸ í™•ì¸: device={current_device}, GPU={torch.cuda.get_device_name(current_device)}")
        
        # 2. AsyncLLMEngine ì„¤ì •
        logger.info(f"[Shard {args.shard_id}] AsyncLLMEngine ì´ˆê¸°í™” ì¤‘: {cfg.model.base_model}")
        vllm_config = cfg.data.raw_dataset.vllm
        
        engine_args = AsyncEngineArgs(
            model=cfg.model.base_model,
            tensor_parallel_size=1,  # TP=1 (ë‹¨ì¼ GPU)
            gpu_memory_utilization=vllm_config.gpu_memory_utilization,
            max_model_len=vllm_config.max_model_len,
            dtype=vllm_config.dtype,
            trust_remote_code=vllm_config.trust_remote_code,
            max_num_batched_tokens=vllm_config.get("max_num_batched_tokens", 16384),
            max_num_seqs=vllm_config.get("max_num_seqs", 256),
            enforce_eager=vllm_config.get("enforce_eager", False),
            disable_custom_all_reduce=vllm_config.get("disable_custom_all_reduce", True),
            disable_log_stats=vllm_config.get("disable_log_stats", False),
            enable_prefix_caching=vllm_config.get("enable_prefix_caching", True),  # í”„ë¦¬í”½ìŠ¤ ìºì‹±
        )
        
        # kv_cache_dtypeì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì¶”ê°€
        if "kv_cache_dtype" in vllm_config:
            engine_args.kv_cache_dtype = vllm_config.kv_cache_dtype
        
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        
        tokenizer = AutoTokenizer.from_pretrained(
            cfg.model.base_model,
            trust_remote_code=vllm_config.trust_remote_code
        )
        logger.info(f"[Shard {args.shard_id}] AsyncLLMEngine ì´ˆê¸°í™” ì™„ë£Œ.")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = os.path.join(cfg.paths.data_dir, "generated")
        sample_limit_env = os.environ.get("SAMPLE_LIMIT", "")
        if sample_limit_env:
            output_dir = os.path.join(output_dir, f"sample_{sample_limit_env}")
        os.makedirs(output_dir, exist_ok=True)
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ
        raw_data_path = os.path.join(cfg.paths.data_dir, "raw", "deepscaler.jsonl")
        if not os.path.exists(raw_data_path):
            logger.error(f"ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_data_path}")
            return
        
        raw_dataset = RawDataset(raw_data_path)
        logger.info(f"ì „ì²´ ì›ë³¸ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # ìƒ˜í”Œ ìˆ˜ ì œí•œ
        sample_limit = int(sample_limit_env) if sample_limit_env and sample_limit_env.isdigit() else 0
        
        if sample_limit > 0 and sample_limit < len(raw_dataset):
            np.random.seed(42)
            selected_indices = np.random.choice(len(raw_dataset), size=sample_limit, replace=False)
            selected_indices = sorted(selected_indices)
            logger.info(f"SAMPLE_LIMIT ì ìš©: ì „ì²´ {len(raw_dataset)}ê°œ ì¤‘ ëœë¤ìœ¼ë¡œ {sample_limit}ê°œ ë¬¸ì œ ì„ íƒ")
        else:
            selected_indices = list(range(len(raw_dataset)))
            logger.info(f"ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©: {len(raw_dataset)}ê°œ ë¬¸ì œ")
        
        # ì‹ ë¢°ë„ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        confidence_calculator = ConfidenceCalculator(
            group_size=cfg.data.raw_dataset.confidence.group_size
        )
        
        instruction = "Please reason step by step, and put your final answer within \\boxed{}."
        
        # ì „ì²´ ë°ì´í„° ì¤€ë¹„
        problems: List[Dict] = []
        texts: List[str] = []
        
        logger.info("ì „ì²´ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        for idx in selected_indices:
            problem_data = raw_dataset[idx]
            problem_id = problem_data.get("id", f"problem_{idx}")
            problem_text = problem_data.get("problem", "")
            ground_truth = problem_data.get("answer", "")
            messages = [{"role": "user", "content": f"{problem_text}\n\n{instruction}"}]
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=cfg.data.raw_dataset.generation.enable_thinking,
            )
            problems.append({
                "problem_id": problem_id,
                "problem_text": problem_text,
                "ground_truth": ground_truth,
            })
            texts.append(text)
        
        # ì‘ì—… ë¶„í•  (Sharding)
        my_problems = problems[args.shard_id::args.total_shards]
        my_texts = texts[args.shard_id::args.total_shards]
        
        logger.info(f"[Shard {args.shard_id}] ì‘ì—… ë¶„í•  ì™„ë£Œ. ì´ ìƒ¤ë“œì—ì„œ {len(my_texts)}ê°œ ë¬¸ì œ ì²˜ë¦¬")
        
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° êµ¬ì„±
        gen_cfg = cfg.data.raw_dataset.generation
        sampling_params = SamplingParams(
            n=gen_cfg.num_responses_per_problem,
            temperature=gen_cfg.temperature,
            top_p=gen_cfg.top_p,
            top_k=gen_cfg.top_k,
            min_p=gen_cfg.min_p,
            max_tokens=gen_cfg.max_tokens,
            logprobs=gen_cfg.logprobs,
            presence_penalty=gen_cfg.presence_penalty,
        )
        gen_cfg_logprobs = gen_cfg.logprobs
        
        # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        temp_jsonl_path = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}_temp.jsonl")
        final_parquet_path = os.path.join(output_dir, f"raw_generated_shard_{args.shard_id}.parquet")
        checkpoint_path = os.path.join(output_dir, f"checkpoint_shard_{args.shard_id}.json")
        
        # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        start_idx = 0
        processed_problems = set()
        if os.path.exists(checkpoint_path):
            with open(checkpoint_path, 'r') as f:
                checkpoint = json.load(f)
                start_idx = checkpoint.get('last_index', 0)
                processed_problems = set(checkpoint.get('processed_problems', []))
            logger.info(f"ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘: {start_idx}ë²ˆì§¸ ë¬¸ì œë¶€í„°, ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì œ {len(processed_problems)}ê°œ")
        
        # ë¹„ë™ê¸° íŒŒì¼ í•¸ë“¤ëŸ¬
        async with aiofiles.open(temp_jsonl_path, 'a') as f:
            # ìš”ì²­ ìƒì„±ê¸°
            async def request_generator() -> AsyncGenerator:
                """ëª¨ë“  ìš”ì²­ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±"""
                for i in range(start_idx, len(my_texts)):
                    problem = my_problems[i]
                    
                    # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì œëŠ” ê±´ë„ˆë›°ê¸°
                    if problem['problem_id'] in processed_problems:
                        continue
                    
                    text = my_texts[i]
                    request_id = f"shard_{args.shard_id}_req_{i}"
                    yield (request_id, text, problem, i)
            
            # ë™ì‹œ ì²˜ë¦¬ ì¤‘ì¸ ìš”ì²­ ì¶”ì 
            pending_requests = {}
            results_buffer = []
            buffer_size = cfg.data.raw_dataset.get("buffer_size", 400)  # ë²„í¼ í¬ê¸°
            save_interval = cfg.data.raw_dataset.get("save_interval", 100)  # Nê°œ ë°°ì¹˜ë§ˆë‹¤ ì €ì¥
            max_pending = cfg.data.raw_dataset.get("max_pending_requests", 400)  # ìµœëŒ€ ë™ì‹œ ìš”ì²­ ìˆ˜
            
            processed_count = 0
            total_saved = 0
            last_checkpoint_time = datetime.now()
            
            # ìš”ì²­ ì œì¶œ íƒœìŠ¤í¬
            async def submit_requests():
                """ìš”ì²­ì„ ì—”ì§„ì— ì œì¶œ"""
                async for request_id, text, problem, idx in request_generator():
                    # ëŒ€ê¸°ì—´ì´ ë„ˆë¬´ í¬ë©´ ì ì‹œ ëŒ€ê¸°
                    while len(pending_requests) > max_pending:
                        await asyncio.sleep(0.1)
                    
                    # ì—”ì§„ì— ìš”ì²­ ì œì¶œ
                    results_generator = engine.generate(
                        prompt=text,
                        sampling_params=sampling_params,
                        request_id=request_id
                    )
                    
                    pending_requests[request_id] = {
                        'generator': results_generator,
                        'problem': problem,
                        'index': idx
                    }
                    
                    # ì§„í–‰ ìƒí™© ë¡œê·¸
                    if len(pending_requests) % 100 == 0:
                        logger.info(f"[Shard {args.shard_id}] {len(pending_requests)}ê°œ ìš”ì²­ ì œì¶œë¨")
                
                logger.info(f"[Shard {args.shard_id}] ëª¨ë“  ìš”ì²­ ì œì¶œ ì™„ë£Œ")
            
            # ê²°ê³¼ ìˆ˜ì§‘ íƒœìŠ¤í¬
            async def collect_results():
                """ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¦‰ì‹œ ì €ì¥"""
                nonlocal processed_count, total_saved, last_checkpoint_time
                
                pbar = tqdm(total=len(my_texts) - start_idx, desc=f"Shard {args.shard_id}", position=args.shard_id)
                
                while pending_requests or not submit_task.done():
                    if not pending_requests:
                        await asyncio.sleep(0.1)
                        continue
                    
                    # ì™„ë£Œëœ ìš”ì²­ ì²˜ë¦¬
                    completed = []
                    
                    for request_id, req_data in list(pending_requests.items()):
                        try:
                            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ë…¼ë¸”ë¡œí‚¹)
                            output = await anext(req_data['generator'])
                            
                            if output is not None and output.finished:
                                # ê° ì‘ë‹µ ì²˜ë¦¬
                                problem = req_data['problem']
                                
                                for j, completion in enumerate(output.outputs):
                                    result = process_single_output(
                                        problem,
                                        completion,
                                        j,
                                        confidence_calculator,
                                        gen_cfg_logprobs,
                                        args
                                    )
                                    results_buffer.append(result)
                                
                                completed.append(request_id)
                                processed_count += 1
                                processed_problems.add(problem['problem_id'])
                                pbar.update(1)
                                
                                # ë²„í¼ê°€ ì°¨ë©´ ì €ì¥
                                if len(results_buffer) >= buffer_size:
                                    for r in results_buffer:
                                        await f.write(json.dumps(r) + '\n')
                                    await f.flush()
                                    total_saved += len(results_buffer)
                                    logger.info(f"[Shard {args.shard_id}] {total_saved}ê°œ ê²°ê³¼ ì €ì¥ ì™„ë£Œ ({processed_count}/{len(my_texts)}ê°œ ë¬¸ì œ ì²˜ë¦¬)")
                                    results_buffer.clear()
                                
                                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (1ë¶„ë§ˆë‹¤)
                                if (datetime.now() - last_checkpoint_time).seconds > 60:
                                    checkpoint = {
                                        'last_index': req_data['index'],
                                        'processed_problems': list(processed_problems),
                                        'timestamp': datetime.now().isoformat()
                                    }
                                    async with aiofiles.open(checkpoint_path, 'w') as ckpt_f:
                                        await ckpt_f.write(json.dumps(checkpoint))
                                    last_checkpoint_time = datetime.now()
                                    logger.info(f"[Shard {args.shard_id}] ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
                                
                        except StopAsyncIteration:
                            # ìƒì„± ì™„ë£Œ
                            completed.append(request_id)
                        except Exception as e:
                            logger.error(f"[Shard {args.shard_id}] ìš”ì²­ {request_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            completed.append(request_id)
                    
                    # ì™„ë£Œëœ ìš”ì²­ ì œê±°
                    for req_id in completed:
                        del pending_requests[req_id]
                    
                    # CPU ì–‘ë³´
                    await asyncio.sleep(0.01)
                
                pbar.close()
                
                # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
                if results_buffer:
                    for r in results_buffer:
                        await f.write(json.dumps(r) + '\n')
                    await f.flush()
                    total_saved += len(results_buffer)
                    logger.info(f"[Shard {args.shard_id}] ìµœì¢… {len(results_buffer)}ê°œ ê²°ê³¼ ì €ì¥")
                    results_buffer.clear()
            
            # ë™ì‹œ ì‹¤í–‰
            submit_task = asyncio.create_task(submit_requests())
            collect_task = asyncio.create_task(collect_results())
            
            await asyncio.gather(submit_task, collect_task)
        
        # JSONL â†’ Parquet ë³€í™˜
        logger.info(f"[Shard {args.shard_id}] JSONLì„ Parquetìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        df = pd.read_json(temp_jsonl_path, lines=True)
        df.to_parquet(final_parquet_path, index=False, compression="zstd")
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_jsonl_path)
        if os.path.exists(checkpoint_path):
            os.remove(checkpoint_path)
        
        logger.info(f"âœ… [Shard {args.shard_id}] Stage 1 ì™„ë£Œ: {len(df)}ê°œ ê²°ê³¼ ì €ì¥")
        logger.info(f"Parquet ì €ì¥ ìœ„ì¹˜: {final_parquet_path}")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        logger.info(f"ìƒì„±ëœ ì‘ë‹µ ìˆ˜: {len(df)}")
        logger.info(f"ë¬¸ì œ ìˆ˜: {df['problem_id'].nunique()}")
        logger.info(f"ë¬¸ì œë‹¹ í‰ê·  ì‘ë‹µ ìˆ˜: {len(df) / df['problem_id'].nunique():.1f}")
        
        if 'output_token_count' in df.columns:
            try:
                total_tokens = int(df['output_token_count'].fillna(0).sum())
                mean_tokens = float(df['output_token_count'].fillna(0).mean())
                min_tokens = int(df['output_token_count'].fillna(0).min()) if len(df) > 0 else 0
                max_tokens = int(df['output_token_count'].fillna(0).max()) if len(df) > 0 else 0
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ í•©ê³„: {total_tokens}")
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ í‰ê· : {mean_tokens:.1f}")
                logger.info(f"ì‘ë‹µ í† í° ìˆ˜ ìµœì†Œ/ìµœëŒ€: {min_tokens}/{max_tokens}")
                
                max_tokens_limit = gen_cfg.max_tokens
                max_tokens_count = int((df['output_token_count'].fillna(0) == max_tokens_limit).sum())
                logger.info(f"ìµœëŒ€ í† í° ìˆ˜({max_tokens_limit})ì— ë„ë‹¬í•œ ì¸ìŠ¤í„´ìŠ¤ ê°œìˆ˜: {max_tokens_count}")
            except Exception:
                pass
        
        # ìƒ˜í”Œ ì¶œë ¥
        if len(df) > 0:
            logger.info("=" * 80)
            logger.info("ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥ (ì²« ë²ˆì§¸ ì¸ìŠ¤í„´ìŠ¤):")
            logger.info("=" * 80)
            sample = df.iloc[0]
            logger.info(f"Problem ID: {sample['problem_id']}")
            logger.info(f"Problem Text: {sample['problem_text'][:200]}...")
            logger.info(f"Ground Truth: {sample['ground_truth']}")
            logger.info(f"Generated Text: {sample['generated_text'][:500]}...")
            logger.info(f"Output Token Count: {sample.get('output_token_count', 'N/A')}")
            logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"[Shard {args.shard_id}] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ë™ê¸° ë˜í¼"""
    # argparseë¡œ ëŸ°ì²˜ì˜ ì¸ìˆ˜ë¥¼ ë°›ìŒ
    parser = argparse.ArgumentParser()
    parser.add_argument("--config-path", type=str, required=True, help="Hydra config directory")
    parser.add_argument("--config-name", type=str, required=True, help="Hydra config name")
    parser.add_argument("--gpu-id", type=str, required=True, help="GPU ID")
    parser.add_argument("--shard-id", type=int, required=True, help="Data shard index")
    parser.add_argument("--total-shards", type=int, default=4, help="Total number of shards")
    args = parser.parse_args()
    
    # 1. GPU ê²©ë¦¬ (ê°€ì¥ ë¨¼ì € ì„¤ì •)
    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_id
    
    # 2. CUDA ì»¨í…ìŠ¤íŠ¸ ëª…ì‹œì  ì´ˆê¸°í™” ë° GPU ì„¤ì •
    # CUDA_VISIBLE_DEVICES ì„¤ì • í›„ì—ëŠ” í•­ìƒ 0ë²ˆì´ ì‹¤ì œ GPUê°€ ë¨
    if torch.cuda.is_available():
        # ëª…ì‹œì ìœ¼ë¡œ GPUë¥¼ ì„¤ì •í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ê°„ ê²©ë¦¬ ë³´ì¥
        # CUDA_VISIBLE_DEVICESë¡œ ì¸í•´ 0ë²ˆì´ ì‹¤ì œ GPUê°€ ë¨
        torch.cuda.set_device(0)
        # GPU ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
        torch.cuda.empty_cache()
    
    # 3. Hydra ìˆ˜ë™ ì´ˆê¸°í™”
    config_dir = Path(args.config_path).resolve()
    hydra.core.global_hydra.GlobalHydra.instance().clear()
    hydra.initialize_config_dir(config_dir=str(config_dir), version_base=None)
    
    cfg = hydra.compose(config_name=args.config_name)
    
    # 4. ë¹„ë™ê¸° ì›Œì»¤ ì‹¤í–‰
    asyncio.run(async_generation_worker(cfg, args))


if __name__ == "__main__":
    main()