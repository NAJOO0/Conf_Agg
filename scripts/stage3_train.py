"""
Stage 3: GRPO ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (Unsloth + TRL)
ì†Œí˜• ëª¨ë¸ìš© - Data Parallelism (DDP) ì‚¬ìš©
Qwen3-1.7B ê°™ì€ ì‘ì€ ëª¨ë¸ì— ìµœì í™”
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import torch
from typing import Optional

import torch.distributed as dist
from datetime import timedelta, datetime

# Unsloth imports
from unsloth import FastLanguageModel, is_bfloat16_supported
from transformers import GenerationConfig   
# TRL imports
from trl import GRPOConfig, GRPOTrainer as TRL_GRPOTrainer
from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE

# Transformers imports
from transformers import TrainingArguments
from peft import LoraConfig

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.utils.logging import setup_logging
from src.evaluation.math_verifier import MathVerifier

logger = logging.getLogger(__name__)

os.environ["UNSLOTH_VLLM_STANDBY"] = "1"
os.environ['UNSLOTH_DISABLE_FAST_LORA'] = '1'

def create_math_reward_function(math_verifier: MathVerifier):
    """
    math_verifyë¥¼ ì‚¬ìš©í•˜ëŠ” reward functionì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        math_verifier: MathVerifier ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        reward function (completions, ground_truth, **kwargs) -> List[float]
    """
    def reward_func(completions, ground_truth=None, **kwargs):
        """
        ìƒì„±ëœ ì‘ë‹µì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            completions: ìƒì„±ëœ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
            ground_truth: ì •ë‹µ (datasetì˜ 'ground_truth' ì»¬ëŸ¼ì—ì„œ ê°€ì ¸ì˜´)
            **kwargs: ì¶”ê°€ ì¸ì (datasetì˜ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤)
        
        Returns:
            reward ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ê° completionì— ëŒ€í•´ 1.0 ë˜ëŠ” 0.0)
        """
        # ground_truthê°€ kwargsì— ìˆì„ ìˆ˜ë„ ìˆìŒ
        if ground_truth is None:
            ground_truth = kwargs.get("ground_truth", None)
        
        # ì—¬ëŸ¬ ê°œì˜ ground_truthê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¬ ìˆ˜ ìˆìŒ
        if isinstance(ground_truth, list):
            if len(ground_truth) == len(completions):
                rewards = []
                for completion, gt in zip(completions, ground_truth):
                    # ì‘ë‹µì—ì„œ ìµœì¢… ë‹µì•ˆ ì¶”ì¶œ
                    predicted_answer = math_verifier.extract_final_answer_from_content(completion)
                    # ì •ë‹µ ê²€ì¦
                    is_correct = math_verifier.verify_answer(predicted_answer, gt)
                    rewards.append(1.0 if is_correct else 0.0)
                return rewards
            else:
                # ê¸¸ì´ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì²« ë²ˆì§¸ ground_truth ì‚¬ìš©
                gt = ground_truth[0] if ground_truth else ""
        else:
            gt = ground_truth or ""
        
        # ë‹¨ì¼ ground_truthë¥¼ ëª¨ë“  completionì— ëŒ€í•´ ì‚¬ìš©
        rewards = []
        for completion in completions:
            predicted_answer = math_verifier.extract_final_answer_from_content(completion)
            is_correct = math_verifier.verify_answer(predicted_answer, gt)
            rewards.append(1.0 if is_correct else 0.0)
        
        return rewards
    
    return reward_func


class OptimizedGRPOTrainer:
    """
    ì†Œí˜• ëª¨ë¸ìš© GRPO íŠ¸ë ˆì´ë„ˆ (Unsloth + TRL)
    
    íŠ¹ì§•:
    - vLLM ì§€ì› (ì„ íƒì  - inference ìµœì í™”)
    - Data Parallelism (DDP) ì‚¬ìš©ìœ¼ë¡œ 2ë°° ì†ë„
    - 1.7B~8B ëª¨ë¸ì— ìµœì í™”
    """
    
    def __init__(self, model_name, lora_config=None, grpo_config=None, training_config=None, device="cuda"):
        self.model_name = model_name
        self.lora_config = lora_config or {}
        self.grpo_config = grpo_config or {}
        self.training_config = training_config or {}
        self.device = device
        
        # GPU ê°œìˆ˜ ê°ì§€
        self.num_gpus = torch.cuda.device_count()
        logger.info(f"ğŸ® ê°ì§€ëœ GPU ê°œìˆ˜: {self.num_gpus}ê°œ")
        
        for i in range(self.num_gpus):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            logger.info(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # ===== ìˆ˜ì •: vLLM ì‚¬ìš© ì—¬ë¶€ëŠ” train()ì—ì„œ ê²°ì • =====
        # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ë§Œ ë¡œë“œ
        use_vllm = self.grpo_config.get("use_vllm", True)
        
        # ===== ëª¨ë¸ ë¡œë“œ (í•­ìƒ DDP í˜¸í™˜ ëª¨ë“œ) =====
        logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        max_seq_length = (
            self.training_config.get("max_prompt_length", 512) + 
            self.training_config.get("max_response_length", 1024)
        )

        # âœ… í•­ìƒ device_map=None (DDP í˜¸í™˜)
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            load_in_4bit=True,
            load_in_8bit=False,
            fast_inference=use_vllm, 
            unsloth_vllm_stanby=True,
            float8_kv_cache=use_vllm,
            # gpu_memory_utilization=0.4,
        )
        
        logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (DDP í˜¸í™˜ ëª¨ë“œ)")
        
        # Chat template
        if self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE
        
        # LoRA ì„¤ì •
        if self.lora_config:
            self._setup_lora()
        
        # Reward function
        timeout = self.training_config.get("verification_timeout", 30)
        self.math_verifier = MathVerifier(timeout=timeout)
        self.reward_func = create_math_reward_function(self.math_verifier)
        
    def _setup_lora(self):
        """LoRA ì–´ëŒ‘í„° ì„¤ì •"""
        logger.info("ğŸ”§ LoRA ì–´ëŒ‘í„° ì„¤ì • ì¤‘...")
        
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=self.lora_config.get("r", 16),
            target_modules=self.lora_config.get("target_modules", [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]),
            lora_alpha=self.lora_config.get("lora_alpha", 16),
            lora_dropout=self.lora_config.get("lora_dropout", 0.0),
            bias=self.lora_config.get("bias", "none"),
            use_gradient_checkpointing="unsloth",  # Unsloth ìµœì í™”
            random_state=self.training_config.get("seed", 42),
            use_rslora=self.lora_config.get("use_rslora", False),
            loftq_config=None,
        )
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê³„ì‚°
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(
            f"âœ… LoRA ì„¤ì • ì™„ë£Œ\n"
            f"   - Rank: {self.lora_config.get('r', 16)}\n"
            f"   - í›ˆë ¨ íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M ({trainable_params / total_params * 100:.2f}%)"
        )
    
    # stage3_train.py ìˆ˜ì •

    def train(self, train_dataset, validation_dataset=None, save_dir="./output"):
        """GRPO í›ˆë ¨ ì‹¤í–‰"""
        logger.info("ğŸ¯ GRPO í›ˆë ¨ ì‹œì‘...")
        
        # ===== DDP í™˜ê²½ ê°ì§€ =====
        rank = int(os.environ.get("RANK", -1))
        local_rank = int(os.environ.get("LOCAL_RANK", -1))
        world_size = int(os.environ.get("WORLD_SIZE", 1))
        is_distributed = rank >= 0

        if is_distributed:
            logger.info(
                f"âœ… DDP ëª¨ë“œ ê°ì§€\n"
                f"   - RANK: {rank}\n"
                f"   - LOCAL_RANK: {local_rank}\n"
                f"   - WORLD_SIZE: {world_size}"
            )

        # ===== vLLM ì„¤ì • ê²°ì • =====
        use_vllm = self.grpo_config.get("use_vllm", True)

        if use_vllm and is_distributed:
            # DDP + vLLM Colocate (ìµœê³  ì¡°í•©!)
            vllm_mode = "colocate"
            vllm_device = None
            logger.info(
                "ğŸš€ DDP + vLLM Colocate ëª¨ë“œ\n"
                "   - ê° GPUê°€ ë…ë¦½ì ìœ¼ë¡œ vLLM ì‹¤í–‰\n"
                "   - Gradient all-reduceë¡œ ë™ê¸°í™”"
            )
        elif use_vllm and not is_distributed:
            # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ + vLLM
            requested_mode = self.grpo_config.get("vllm_mode", "colocate")
            
            if self.num_gpus >= 2:
                # GPU 2ê°œ ì´ìƒì¼ ë•Œ colocate ë˜ëŠ” separate ì„ íƒ ê°€ëŠ¥
                if requested_mode == "colocate":
                    vllm_mode = "colocate"
                    vllm_device = None
                    logger.info(
                        f"ğŸ”§ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ vLLM Colocate ëª¨ë“œ\n"
                        f"   - GPU {self.num_gpus}ê°œ í™œìš©\n"
                        f"   - ê°™ì€ í”„ë¡œì„¸ìŠ¤ì—ì„œ training + vLLM ë™ì‹œ ì‹¤í–‰"
                    )
                else:
                    # separate ëª¨ë“œ: ë³„ë„ GPU ì‚¬ìš©
                    vllm_mode = "separate"
                    vllm_device = "cuda:1" if self.num_gpus >= 2 else None
                    logger.info(
                        f"ğŸ”§ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ vLLM Separate ëª¨ë“œ\n"
                        f"   - Training: GPU 0\n"
                        f"   - vLLM: GPU 1"
                    )
            else:
                # GPU 1ê°œì¼ ë•ŒëŠ” colocateë§Œ ê°€ëŠ¥
                vllm_mode = "colocate"
                vllm_device = None
                logger.info("ğŸ”§ ë‹¨ì¼ GPU vLLM Colocate ëª¨ë“œ")
        else:
            vllm_mode = None
            vllm_device = None
            logger.info("âš™ï¸ vLLM ë¹„í™œì„±í™”")
        
        # ===== Batch size ê²€ì¦ =====
        num_generations = self.grpo_config.get("num_generations", 8)
        batch_size = self.training_config.get("batch_size", 8)
        
        # if batch_size % num_generations != 0:
        #     adjusted_batch_size = (batch_size // num_generations) * num_generations
        #     if adjusted_batch_size == 0:
        #         adjusted_batch_size = num_generations
        #     logger.warning(
        #         f"âš ï¸ batch_size ì¡°ì •: {batch_size} -> {adjusted_batch_size}"
        #     )
        #     batch_size = adjusted_batch_size
        
        # ===== GRPO Config ì„¤ì • =====
        # warmup_steps ì²˜ë¦¬: Hydraì—ì„œ ë¬¸ìì—´ë¡œ ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³€í™˜
        warmup_steps_raw = self.training_config.get("warmup_steps", None)
        warmup_steps = None
        
        if warmup_steps_raw is not None:
            # "None" ë¬¸ìì—´ì´ë‚˜ ì‹¤ì œ None ì²´í¬
            if isinstance(warmup_steps_raw, str):
                if warmup_steps_raw.lower() not in ["none", "null", ""]:
                    try:
                        warmup_steps = int(warmup_steps_raw)
                    except (ValueError, TypeError):
                        warmup_steps = None
            elif isinstance(warmup_steps_raw, (int, float)) and warmup_steps_raw > 0:
                warmup_steps = int(warmup_steps_raw)
        
        # warmup_stepsê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ warmup_ratio ì‚¬ìš©
        warmup_ratio = None if warmup_steps is not None else self.training_config.get("warmup_ratio", 0.1)
        
        grpo_config = GRPOConfig(
            # ê¸°ë³¸ ì„¤ì •
            output_dir=save_dir,
            num_train_epochs=self.training_config.get("epochs", 1),
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=self.training_config.get("gradient_accumulation_steps", 4),
            
            # Learning rate
            learning_rate=self.training_config.get("learning_rate", 5e-6),
            lr_scheduler_type=self.training_config.get("lr_scheduler_type", "cosine"),
            **({"warmup_steps": warmup_steps} if warmup_steps is not None else {"warmup_ratio": warmup_ratio}),

            # GRPO ì„¤ì •
            num_generations=num_generations,
            max_prompt_length=self.training_config.get("max_prompt_length", 512),
            max_completion_length=self.training_config.get("max_response_length", 1024),
            temperature=self.grpo_config.get("temperature", 1.0),
            beta=self.grpo_config.get("beta", 0.01),  # 0ì´ ì•„ë‹Œ ì‘ì€ ê°’
            
            # ìµœì í™”
            optim="paged_adamw_8bit",
            gradient_checkpointing=True,
            bf16=is_bfloat16_supported(),
            # DDP ì„¤ì •
            
            # ===== ğŸ¯ vLLM ì„¤ì • (ìˆ˜ì •ë¨) =====
            use_vllm=use_vllm,
            vllm_mode=vllm_mode,  # âœ… separate ë˜ëŠ” None
            vllm_gpu_memory_utilization=self.grpo_config.get("vllm_gpu_memory_utilization", 0.85),  # âœ… ë†’ì„
            vllm_enable_sleep_mode=self.grpo_config.get("vllm_enable_sleep_mode", True),
            # ë¡œê¹… ë° ì €ì¥
            logging_steps=self.training_config.get("logging_steps", 10),
            save_steps=self.training_config.get("save_steps", 500),
            save_total_limit=self.training_config.get("save_total_limit", 3),
            
            # Evaluation
            # eval_strategy="steps" if validation_dataset else "no",
            eval_strategy="no",
            eval_steps=self.training_config.get("eval_steps", 500) if validation_dataset else None,
            per_device_eval_batch_size=num_generations,
            
            # WandB
            report_to="wandb" if self.training_config.get("use_wandb", False) else "none",
            
            # ê¸°íƒ€
            seed=self.training_config.get("seed", 42),
            dataloader_num_workers=1,
            remove_unused_columns=False,
        )
        
        # ìœ íš¨ ë°°ì¹˜ í¬ê¸°
        effective_batch_size = (
            grpo_config.per_device_train_batch_size
            * grpo_config.gradient_accumulation_steps
            * max(world_size, 1)
        )
        
        logger.info(
            f"ğŸ“¦ ìµœì¢… ì„¤ì •:\n"
            f"   - ëª¨ë“œ: {'DDP + vLLM Colocate ğŸ†' if (is_distributed and use_vllm) else 'DDP only' if is_distributed else 'Single'}\n"
            f"   - World size: {max(world_size, 1)}\n"
            f"   - GPUë‹¹ ë°°ì¹˜: {grpo_config.per_device_train_batch_size}\n"
            f"   - Gradient accumulation: {grpo_config.gradient_accumulation_steps}\n"
            f"   - ìœ íš¨ ë°°ì¹˜ í¬ê¸°: {effective_batch_size}\n"
            f"   - ì˜ˆìƒ throughput: {'ğŸš€ ìµœê³ !' if (is_distributed and use_vllm) else 'âœ… ë¹ ë¦„'}"
        )
        
        # Trainer ì´ˆê¸°í™”
        trainer = TRL_GRPOTrainer(
            model=self.model,
            reward_funcs=self.reward_func,
            args=grpo_config,
            train_dataset=train_dataset,
            processing_class=self.tokenizer,
            eval_dataset=validation_dataset,
        )
        
        # í›ˆë ¨ ì‹¤í–‰
        logger.info("ğŸƒ í›ˆë ¨ ì‹œì‘!")
        trainer.train()
        
        # ì €ì¥
        logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {save_dir}")
        trainer.save_model(save_dir)
        self.tokenizer.save_pretrained(save_dir)
        
        torch.cuda.empty_cache()
        logger.info("âœ… í›ˆë ¨ ì™„ë£Œ!")


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 3 ë©”ì¸ í•¨ìˆ˜"""

     # ğŸ”¥ í™˜ê²½ ë³€ìˆ˜ ê°•ì œ ì„¤ì • (ì¬ì´ˆê¸°í™” X)
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29500'
    os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
    os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '300'
    os.environ['NCCL_IB_DISABLE'] = '1'
    
    rank = int(os.environ.get("RANK", -1))
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_distributed = rank >= 0
    
    if rank >= 0:
        os.environ['VLLM_WORKER_NAME'] = f'worker_{rank}'
        os.environ['VLLM_INSTANCE_ID'] = str(rank)


    print(f"[RANK {rank}] torch.distributed.is_initialized(): {dist.is_initialized()}", file=sys.stderr)
    print(f"[RANK {rank}] MASTER_ADDR: {os.environ['MASTER_ADDR']}", file=sys.stderr)
    print(f"[RANK {rank}] MASTER_PORT: {os.environ['MASTER_PORT']}", file=sys.stderr)
    sys.stderr.flush()
    
    # ===== ğŸš€ DDP ìˆ˜ë™ ì´ˆê¸°í™” (TRL ì „ì—) =====
    if is_distributed and not dist.is_initialized():
        print(f"â±ï¸  [RANK {rank}] DDP ì´ˆê¸°í™” ì‹œë„ (30ì´ˆ íƒ€ì„ì•„ì›ƒ)...", file=sys.stderr)
        sys.stderr.flush()
        
        try:
            dist.init_process_group(
                backend='nccl',
                init_method='env://',
                world_size=world_size,
                rank=rank,
                timeout=timedelta(seconds=30)
            )
            torch.cuda.set_device(local_rank)
            print(f"âœ… [RANK {rank}] DDP ì´ˆê¸°í™” ì„±ê³µ! GPU {local_rank}", file=sys.stderr)
            sys.stderr.flush()
        except Exception as e:
            print(f"âŒ [RANK {rank}] DDP ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", file=sys.stderr)
            sys.stderr.flush()
            raise

    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage3_train.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    logger.info("ğŸš€ Stage 3: GRPO ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (Unsloth + TRL)")
    logger.info(f"ì„¤ì •: {cfg.training}")
    
    # GPU ê°œìˆ˜ í™•ì¸
    num_gpus = torch.cuda.device_count()
    logger.info(f"ğŸ® ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {num_gpus}ê°œ")
    
    for i in range(num_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
        logger.info(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(cfg.paths.model_dir, exist_ok=True)
    # enavle_think ë° í›ˆë ¨ ë‚ ì§œ í´ë”ë° model ì €ì¥
    enable_think = cfg.training.training.enable_think
    train_date = datetime.now().strftime("%Y%m%d")
    model_dir = os.path.join(cfg.paths.model_dir, f"enable_think_{enable_think}_{train_date}")
    os.makedirs(model_dir, exist_ok=True)
    # ì…ë ¥ íŒŒì¼ ê²½ë¡œ í™•ì¸
    train_data_path = os.path.join(cfg.paths.data_dir, "curated", "train_curated.parquet")
    validation_data_path = os.path.join(cfg.paths.data_dir, "curated", "validation_curated.parquet")
    
    if not os.path.exists(train_data_path):
        logger.error(f"í›ˆë ¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_data_path}")
        logger.error("ë¨¼ì € Stage 2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”: python scripts/stage2_curate.py")
        return
    
    # ë°ì´í„°ì…‹ ìƒì„± (TRL GRPOTrainerëŠ” Datasetì„ ë°›ìŒ)
    logger.info("ğŸ“¦ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    from src.data.training_dataset import CuratedTrainingDataset
    
    train_dataset = CuratedTrainingDataset(train_data_path)
    logger.info(f"í›ˆë ¨ ë°ì´í„°ì…‹ ìƒì„±: {len(train_dataset)} ìƒ˜í”Œ")
    
    validation_dataset = None
    if os.path.exists(validation_data_path):
        validation_dataset = CuratedTrainingDataset(validation_data_path)
        logger.info(f"ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±: {len(validation_dataset)} ìƒ˜í”Œ")
    
    # GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
    logger.info("ğŸ¦¥ Optimized GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘...")
    trainer = OptimizedGRPOTrainer(
        model_name=cfg.model.base_model,
        lora_config=cfg.training.lora if cfg.training.method == "lora" else None,
        grpo_config=cfg.training.grpo,
        training_config=cfg.training.training,
        device=cfg.experiment.device
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    logger.info("ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    trainer.train(
        train_dataset=train_dataset,
        validation_dataset=validation_dataset,
        save_dir=model_dir
    )
    
    logger.info("âœ… Stage 3 ì™„ë£Œ")
    logger.info(f"ğŸ“ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_dir}")


if __name__ == "__main__":
    main()