"""
Stage 3: GRPO ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (Unsloth + vLLM + TRL)
ìµœì í™” ë²„ì „ - 2025ë…„ 2ì›” ê¸°ì¤€ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì ìš©

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. vLLM Colocate ëª¨ë“œ í™œì„±í™” (DDP + vLLM ìµœì  ì¡°í•©)
2. FP8 KV Cache ì§€ì› (ë©”ëª¨ë¦¬ 2ë°° ì ˆì•½)
3. Unsloth ìµœì‹  ë©”ëª¨ë¦¬ ìµœì í™” í™œìš©
4. ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
5. í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§
"""
import os
# Flash Attention ì—†ì´ë„ ì‘ë™í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["UNSLOTH_VLLM_STANDBY"] = "1"  # âœ… vLLM ë©”ëª¨ë¦¬ ìµœì í™”
# Flash Attention 2ê°€ ì—†ì–´ë„ xFormersë¡œ ìë™ í´ë°±
# os.environ["VLLM_ATTENTION_BACKEND"] = "XFORMERS"  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import torch
from torch.utils.data import DataLoader
from typing import Optional, List, Dict, Any

import torch.distributed as dist
from datetime import timedelta

# Unsloth imports
from unsloth import FastLanguageModel, is_bfloat16_supported, vLLMSamplingParams
from transformers import GenerationConfig   

# TRL imports
from trl import GRPOConfig, GRPOTrainer as TRL_GRPOTrainer
from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE

# PEFT imports
from peft import LoraConfig

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.utils.logging import setup_logging
from src.evaluation.math_verifier import MathVerifier

logger = logging.getLogger(__name__)



def create_math_reward_function(math_verifier: MathVerifier):
    """
    ìˆ˜í•™ ë¬¸ì œ ê²€ì¦ìš© reward function ìƒì„±
    
    Args:
        math_verifier: MathVerifier ì¸ìŠ¤í„´ìŠ¤
    
    Returns:
        reward function (completions, ground_truth, **kwargs) -> List[float]
    """
    def reward_func(completions, ground_truth=None, **kwargs):
        """
        ìƒì„±ëœ ì‘ë‹µì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            completions: ìƒì„±ëœ ì‘ë‹µ ë¦¬ìŠ¤íŠ¸ (ê° ìš”ì†ŒëŠ” str ë˜ëŠ” list of dict)
            ground_truth: ì •ë‹µ (datasetì˜ 'ground_truth' ì»¬ëŸ¼ì—ì„œ ê°€ì ¸ì˜´)
            **kwargs: ì¶”ê°€ ì¸ì (datasetì˜ ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤)
        
        Returns:
            reward ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ (ê° completionì— ëŒ€í•´ 1.0 ë˜ëŠ” 0.0)
        """
        # ground_truthê°€ kwargsì— ìˆì„ ìˆ˜ë„ ìˆìŒ
        if ground_truth is None:
            ground_truth = kwargs.get("ground_truth", None)
        
        # completions í˜•ì‹ ì •ê·œí™” (list of dict -> str)
        normalized_completions = []
        for completion in completions:
            if isinstance(completion, list) and len(completion) > 0:
                # [{"role": "assistant", "content": "..."}] í˜•ì‹
                if isinstance(completion[0], dict) and "content" in completion[0]:
                    normalized_completions.append(completion[0]["content"])
                else:
                    normalized_completions.append(str(completion))
            elif isinstance(completion, str):
                normalized_completions.append(completion)
            else:
                normalized_completions.append(str(completion))
        
        # ì—¬ëŸ¬ ê°œì˜ ground_truthê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¬ ìˆ˜ ìˆìŒ
        if isinstance(ground_truth, list):
            if len(ground_truth) == len(normalized_completions):
                # ê° completionë§ˆë‹¤ ëŒ€ì‘í•˜ëŠ” ground_truth ì‚¬ìš©
                rewards = []
                for completion, gt in zip(normalized_completions, ground_truth):
                    predicted_answer = math_verifier.extract_final_answer_from_content(completion)
                    is_correct = math_verifier.verify_answer(predicted_answer, gt)
                    rewards.append(1.0 if is_correct else 0.0)
                return rewards
            else:
                # ê¸¸ì´ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì²« ë²ˆì§¸ ground_truth ì‚¬ìš©
                gt = ground_truth[0] if ground_truth else ""
        else:
            gt = ground_truth or ""
        
        # ë‹¨ì¼ ground_truthë¥¼ ëª¨ë“  completionì— ëŒ€í•´ ì‚¬ìš©
        rewards = []
        for completion in normalized_completions:
            predicted_answer = math_verifier.extract_final_answer_from_content(completion)
            is_correct = math_verifier.verify_answer(predicted_answer, gt)
            rewards.append(1.0 if is_correct else 0.0)
        
        return rewards
    
    return reward_func


class OptimizedGRPOTrainer:
    """
    Unsloth + vLLM + GRPO ìµœì í™” íŠ¸ë ˆì´ë„ˆ
    
    íŠ¹ì§•:
    1. vLLM Colocate ëª¨ë“œ: DDPì™€ ì™„ë²½ í˜¸í™˜
    2. FP8 KV Cache: ë©”ëª¨ë¦¬ 2ë°° ì ˆì•½ (RTX 3090/A100 ì´ìƒ)
    3. Unsloth ë©”ëª¨ë¦¬ ìµœì í™”: 90% VRAM ì ˆê°
    4. ìë™ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    5. ë©€í‹° GPU DDP ì§€ì›
    """
    
    def __init__(
        self, 
        model_name: str,
        lora_config: Optional[Dict[str, Any]] = None,
        grpo_config: Optional[Dict[str, Any]] = None,
        training_config: Optional[Dict[str, Any]] = None,
        device: str = "cuda"
    ):
        self.model_name = model_name
        self.lora_config = lora_config or {}
        self.grpo_config = grpo_config or {}
        self.training_config = training_config or {}
        self.device = device
        
        # GPU í™˜ê²½ ê°ì§€
        self.num_gpus = torch.cuda.device_count()
        self.is_distributed = int(os.environ.get("RANK", -1)) >= 0
        self.rank = int(os.environ.get("RANK", -1))
        self.local_rank = int(os.environ.get("LOCAL_RANK", -1))
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))
        
        # GPU ì •ë³´ ë¡œê¹… (rank 0ì—ì„œë§Œ)
        if not self.is_distributed or self.rank == 0:
            logger.info(f"ğŸ® GPU í™˜ê²½ ì •ë³´")
            logger.info(f"   - GPU ê°œìˆ˜: {self.num_gpus}")
            logger.info(f"   - ë¶„ì‚° ëª¨ë“œ: {'âœ… DDP' if self.is_distributed else 'âŒ Single'}")
            if self.is_distributed:
                logger.info(f"   - Rank: {self.rank}/{self.world_size}")
            
            for i in range(self.num_gpus):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                gpu_capability = torch.cuda.get_device_capability(i)
                logger.info(
                    f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB, "
                    f"Compute {gpu_capability[0]}.{gpu_capability[1]})"
                )
        
        # FP8 KV Cache ì§€ì› ì—¬ë¶€ í™•ì¸ (Compute Capability >= 8.0)
        self.supports_fp8 = False
        if torch.cuda.is_available():
            capability = torch.cuda.get_device_capability(0)
            self.supports_fp8 = capability[0] >= 8  # Ampere(A100) ì´ìƒ
            logger.info(f"FP8 KV Cache ì§€ì› ì—¬ë¶€: {self.supports_fp8}")
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model()
        
        # Reward function ì„¤ì •
        self._setup_reward_function()
        
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ (DDP í˜¸í™˜ ëª¨ë“œ)"""
        if not self.is_distributed or self.rank == 0:
            logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
        max_seq_length = (
            self.training_config.get("max_prompt_length", 512) + 
            self.training_config.get("max_response_length", 1024)
        )
        
        # âœ… enable_thinkê°€ ì¼œì ¸ìˆìœ¼ë©´ ì‘ë‹µ ê¸¸ì´ ì¦ê°€
        enable_think = self.grpo_config.get("enable_think", False)
        if enable_think:
            # Thinkingì€ ë³´í†µ ë‹µë³€ë³´ë‹¤ 2-3ë°° ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŒ
            max_seq_length = int(max_seq_length * 1.5)
            if not self.is_distributed or self.rank == 0:
                logger.info(f"ğŸ“ enable_think=True: max_seq_length ì¦ê°€ â†’ {max_seq_length}")
        
        load_in_4bit = False if max_seq_length > 16384 else True
        load_in_8bit = True if max_seq_length > 16384 else False
        use_vllm = self.grpo_config.get("use_vllm", True)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model, self.tokenizer = FastLanguageModel.from_pretrained(
            model_name=self.model_name,
            max_seq_length=max_seq_length,
            dtype=None,
            load_in_4bit=load_in_4bit,
            load_in_8bit=load_in_8bit,
            device_map=None,
            fast_inference=use_vllm,
            float8_kv_cache=self.supports_fp8 and use_vllm,
        )
        
        if not self.is_distributed or self.rank == 0:
            logger.info(
                f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n"
                f"   - Max sequence: {max_seq_length}\n"
                f"   - Quantization: {'4-bit' if load_in_4bit else '8-bit'}\n"
                f"   - vLLM: {'âœ…' if use_vllm else 'âŒ'}\n"
                f"   - enable_think: {'âœ…' if enable_think else 'âŒ'}"
            )
        
        # Chat template ì„¤ì •
        if self.tokenizer.chat_template is None:
            self.tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE
        
        # LoRA ì„¤ì •
        if self.lora_config:
            self._setup_lora()

    def _setup_lora(self):
        """LoRA ì–´ëŒ‘í„° ì„¤ì •"""
        if not self.is_distributed or self.rank == 0:
            logger.info("ğŸ”§ LoRA ì–´ëŒ‘í„° ì„¤ì • ì¤‘...")
        
        lora_r = self.lora_config.get("r", 16)
        lora_alpha = self.lora_config.get("lora_alpha", lora_r)  # ê¸°ë³¸ê°’: rê³¼ ë™ì¼
        
        # target_modulesë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Hydraì˜ ListConfigë¥¼ ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ë¡œ)
        target_modules_raw = self.lora_config.get("target_modules", [
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ])
        # ListConfigë‚˜ ë‹¤ë¥¸ íƒ€ì…ì„ ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if hasattr(target_modules_raw, '__iter__') and not isinstance(target_modules_raw, str):
            target_modules = list(target_modules_raw)
        else:
            target_modules = target_modules_raw if isinstance(target_modules_raw, list) else [target_modules_raw]
        
        self.model = FastLanguageModel.get_peft_model(
            self.model,
            r=lora_r,
            target_modules=target_modules,
            lora_alpha=lora_alpha,
            lora_dropout=self.lora_config.get("lora_dropout", 0.0),
            bias=self.lora_config.get("bias", "none"),
            use_gradient_checkpointing="unsloth",  # Unsloth ìµœì í™”
            random_state=self.training_config.get("seed", 42),
            use_rslora=self.lora_config.get("use_rslora", False),
            loftq_config=None,
        )
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ê³„ì‚°
        if not self.is_distributed or self.rank == 0:
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(
                f"âœ… LoRA ì„¤ì • ì™„ë£Œ\n"
                f"   - Rank (r): {lora_r}\n"
                f"   - Alpha: {lora_alpha}\n"
                f"   - Dropout: {self.lora_config.get('lora_dropout', 0.0)}\n"
                f"   - í›ˆë ¨ íŒŒë¼ë¯¸í„°: {trainable_params / 1e6:.2f}M "
                f"({trainable_params / total_params * 100:.2f}%)"
            )
    
    def _setup_reward_function(self):
        """Reward function ì„¤ì •"""
        timeout = self.training_config.get("verification_timeout", 30)
        self.math_verifier = MathVerifier(timeout=timeout)
        self.reward_func = create_math_reward_function(self.math_verifier)
        
        if not self.is_distributed or self.rank == 0:
            logger.info(f"âœ… Reward function ì„¤ì • ì™„ë£Œ (timeout: {timeout}s)")
    
    def _validate_and_adjust_batch_size(
        self, 
        batch_size: int, 
        num_generations: int
    ) -> int:
        """
        ë°°ì¹˜ í¬ê¸° ê²€ì¦ ë° ìë™ ì¡°ì •
        
        GRPOì—ì„œ batch_sizeëŠ” num_generationsì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨
        """
        if batch_size % num_generations != 0:
            adjusted_batch_size = (batch_size // num_generations) * num_generations
            if adjusted_batch_size == 0:
                adjusted_batch_size = num_generations
            
            if not self.is_distributed or self.rank == 0:
                logger.warning(
                    f"âš ï¸ batch_sizeê°€ num_generationsì˜ ë°°ìˆ˜ê°€ ì•„ë‹™ë‹ˆë‹¤\n"
                    f"   - ì›ë˜: {batch_size}\n"
                    f"   - ì¡°ì •: {adjusted_batch_size}\n"
                    f"   - num_generations: {num_generations}"
                )
            return adjusted_batch_size
        
        return batch_size
    
    def _create_vllm_sampling_params(self) -> Optional[vLLMSamplingParams]:
        """vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ìƒì„±"""
        use_vllm = self.grpo_config.get("use_vllm", True)
        if not use_vllm:
            return None
        
        # ì‚¬ìš©ì ì •ì˜ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° (ìˆìœ¼ë©´)
        sampling_config = self.grpo_config.get("vllm_sampling", {})
        
        if sampling_config:
            params = vLLMSamplingParams(
                temperature=sampling_config.get("temperature", 0.7),
                top_p=sampling_config.get("top_p", 0.8),
                top_k=sampling_config.get("top_k", 20),
                min_p=sampling_config.get("min_p", 0.0),
                seed=self.training_config.get("seed", 42),
            )
            
            if not self.is_distributed or self.rank == 0:
                logger.info(f"ğŸ² vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •: {sampling_config}")
            
            return params
        
        return None
    
    def train(
        self, 
        train_dataset, 
        validation_dataset=None, 
        save_dir="./output"
    ):
        """GRPO í›ˆë ¨ ì‹¤í–‰"""
        if not self.is_distributed or self.rank == 0:
            logger.info("ğŸ¯ GRPO í›ˆë ¨ ì¤€ë¹„ ì¤‘...")
        
        # ===== ë°°ì¹˜ í¬ê¸° ì„¤ì • ë° ê²€ì¦ =====
        num_generations = self.grpo_config.get("num_generations", 8)
        batch_size = self.training_config.get("batch_size", 8)
        # batch_size = self._validate_and_adjust_batch_size(batch_size, num_generations)
        
        # ===== Warmup ì„¤ì • =====
        warmup_steps_raw = self.training_config.get("warmup_steps", None)
        warmup_steps = None
        
        if warmup_steps_raw is not None:
            if isinstance(warmup_steps_raw, str):
                if warmup_steps_raw.lower() not in ["none", "null", ""]:
                    try:
                        warmup_steps = int(warmup_steps_raw)
                    except (ValueError, TypeError):
                        warmup_steps = None
            elif isinstance(warmup_steps_raw, (int, float)) and warmup_steps_raw > 0:
                warmup_steps = int(warmup_steps_raw)
        
        warmup_ratio = None if warmup_steps is not None else self.training_config.get("warmup_ratio", 0.1)
        
        # ===== vLLM ëª¨ë“œ ê²°ì • =====
        use_vllm = self.grpo_config.get("use_vllm", True)
        
        if use_vllm:
            if self.is_distributed:
                # ===== ğŸ† DDP + vLLM Colocate (ìµœì  ì¡°í•©!) =====
                vllm_mode = "colocate"
                if not self.is_distributed or self.rank == 0:
                    logger.info(
                        "ğŸš€ vLLM Colocate ëª¨ë“œ (DDP)\n"
                        "   - ê° GPUì—ì„œ ë…ë¦½ì ìœ¼ë¡œ vLLM ì‹¤í–‰\n"
                        "   - Gradient all-reduceë¡œ ë™ê¸°í™”\n"
                        "   - ìµœê³ ì˜ throughput!"
                    )
            else:
                # ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤: colocate ì‚¬ìš©
                vllm_mode = "colocate"
                if not self.is_distributed or self.rank == 0:
                    logger.info("ğŸ”§ vLLM Colocate ëª¨ë“œ (Single GPU)")
        else:
            vllm_mode = None
            if not self.is_distributed or self.rank == 0:
                logger.info("âš™ï¸ vLLM ë¹„í™œì„±í™”")
        
        # ===== GRPO Config ìƒì„± =====
        eval_batch_size = None
        eval_enabled = False

        if validation_dataset is not None:
            val_len = len(validation_dataset)
            num_gens = num_generations
            
            if val_len < num_gens:
                if not self.is_distributed or self.rank == 0:
                    logger.warning(
                        f"âš ï¸ ê²€ì¦ ìƒ˜í”Œ ìˆ˜({val_len})ê°€ num_generations({num_gens})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. "
                        f"í‰ê°€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤."
                    )
                validation_dataset = None
                eval_enabled = False
            else:
                # âœ… í‰ê°€ ë°°ì¹˜ í¬ê¸°: ìµœì†Œ 1, ìµœëŒ€ num_generations, ë°ì´í„°ì…‹ í¬ê¸° ê³ ë ¤
                # ë¹ˆ ë°°ì¹˜ ë°©ì§€ë¥¼ ìœ„í•´ ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
                eval_batch_size = max(1, min(num_gens, val_len // 10))  # ë°ì´í„°ì…‹ í¬ê¸°ì˜ 10% ì´í•˜
                eval_enabled = True
                if not self.is_distributed or self.rank == 0:
                    logger.info(f"âœ… ê²€ì¦ ë°°ì¹˜ í¬ê¸°: {eval_batch_size} (ë°ì´í„°ì…‹ í¬ê¸°: {val_len})")
        grpo_config = GRPOConfig(
            # ê¸°ë³¸ ì„¤ì •
            output_dir=save_dir,
            num_train_epochs=self.training_config.get("epochs", 1),
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=self.training_config.get("gradient_accumulation_steps", 4),
            
            # Learning rate
            learning_rate=self.training_config.get("learning_rate", 5e-6),
            lr_scheduler_type=self.training_config.get("lr_scheduler_type", "cosine"),
            **({"warmup_steps": warmup_steps} if warmup_steps is not None else {"warmup_ratio": warmup_ratio}),
            
            # GRPO ì•Œê³ ë¦¬ì¦˜ ì„¤ì •
            num_generations=num_generations,
            max_prompt_length=self.training_config.get("max_prompt_length", 512),
            max_completion_length=self.training_config.get("max_response_length", 1024),
            temperature=self.grpo_config.get("temperature", 1.0),
            beta=self.grpo_config.get("beta", 0.00),
            # mask_truncated_completions: Falseë¡œ ì„¤ì •í•˜ì—¬ truncated completionë„ ì‚¬ìš©
            # Trueë¡œ ì„¤ì •í•˜ë©´ ëª¨ë“  completionì´ truncatedë˜ë©´ ë¹ˆ ë°°ì¹˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
            mask_truncated_completions=False,
            
            # ìµœì í™” ì„¤ì •
            bf16=is_bfloat16_supported(),  # Auto-detect
            fp16=not is_bfloat16_supported(),
            optim="adamw_8bit",  # ë©”ëª¨ë¦¬ ì ˆì•½
            gradient_checkpointing=True,
            max_grad_norm=self.training_config.get("max_grad_norm", 1.0),
            
            # DDP ì„¤ì •
            ddp_find_unused_parameters=False,
            dataloader_pin_memory=False,
            
            # ===== vLLM ì„¤ì • =====
            # use_vllm: GRPO í›ˆë ¨ ì¤‘ generation ë‹¨ê³„ì—ì„œ vLLM ì‚¬ìš© (í•„ìˆ˜!)
            # vllm_mode: "colocate" = trainingê³¼ ê°™ì€ GPUì—ì„œ vLLM ì‹¤í–‰
            use_vllm=use_vllm,
            vllm_mode=vllm_mode,
            vllm_gpu_memory_utilization=self.grpo_config.get(
                "vllm_gpu_memory_utilization", 
                0.85  # DDPì—ì„œ ë†’ê²Œ ì„¤ì • ê°€ëŠ¥
            ),
            vllm_enable_sleep_mode=self.grpo_config.get("vllm_enable_sleep_mode", True),
            
            # ë¡œê¹… ë° ì €ì¥
            logging_steps=self.training_config.get("logging_steps", 10),
            save_steps=self.training_config.get("save_steps", 500),
            save_total_limit=self.training_config.get("save_total_limit", 3),
            load_best_model_at_end=False,
            
            # Evaluation
            # ë¹ˆ ë°°ì¹˜ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ í‰ê°€ë¥¼ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŒ
            # eval_strategy="no",  # í‰ê°€ ë¹„í™œì„±í™” (ë¹ˆ ë°°ì¹˜ ë¬¸ì œ í•´ê²° ì „ê¹Œì§€)
            eval_strategy="steps" if eval_enabled else "no",
            # ë¹ˆ ë°°ì¹˜ ë¬¸ì œë¡œ í‰ê°€ ë¹ˆë„ ì¤„ì„ (configì—ì„œ eval_stepsê°€ 1ì´ë©´ 100ìœ¼ë¡œ ë³€ê²½)
            eval_steps = self.training_config.get("eval_steps", 500) if eval_enabled else None,
            # í‰ê°€ ë°°ì¹˜ í¬ê¸°: ë¹ˆ ë°°ì¹˜ ë°©ì§€ë¥¼ ìœ„í•´ ìµœì†Œ 1, ìµœëŒ€ num_generationsê³¼ ë°ì´í„°ì…‹ í¬ê¸° ì¤‘ ì‘ì€ ê°’
            per_device_eval_batch_size=max(1, min(eval_batch_size, len(validation_dataset) if validation_dataset else 1)),
            eval_accumulation_steps=1 if eval_enabled else None,
            # ë¹ˆ ë°°ì¹˜ í•„í„°ë§
            dataloader_drop_last=False,  # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ìœ ì§€í•˜ë˜, ë¹ˆ ë°°ì¹˜ëŠ” í•„í„°ë§
            
            # WandB
            report_to="wandb" if self.training_config.get("use_wandb", False) else "none",
            
            # ê¸°íƒ€
            seed=self.training_config.get("seed", 42),
            dataloader_num_workers=1,
            remove_unused_columns=False,
            
            # ì¶”ê°€ ì•ˆì •ì„± ì„¤ì •
            logging_nan_inf_filter=True,
            skip_memory_metrics=True,
        )
        
        # ===== ìœ íš¨ ë°°ì¹˜ í¬ê¸° ê³„ì‚° =====
        effective_batch_size = (
            grpo_config.per_device_train_batch_size
            * grpo_config.gradient_accumulation_steps
            * max(self.world_size, 1)
        )
        
        if not self.is_distributed or self.rank == 0:
            logger.info(
                f"\n{'='*60}\n"
                f"ğŸ“¦ ìµœì¢… í›ˆë ¨ ì„¤ì •\n"
                f"{'='*60}\n"
                f"ğŸ® Hardware:\n"
                f"   - ëª¨ë“œ: {'DDP' if self.is_distributed else 'Single GPU'}\n"
                f"   - World size: {max(self.world_size, 1)}\n"
                f"   - GPU ë©”ëª¨ë¦¬: {self.grpo_config.get('vllm_gpu_memory_utilization', 0.85):.0%}\n"
                f"\n"
                f"ğŸš€ vLLM:\n"
                f"   - ì‚¬ìš©: {'âœ…' if use_vllm else 'âŒ'}\n"
                f"   - ëª¨ë“œ: {vllm_mode or 'N/A'}\n"
                f"   - FP8 KV Cache: {'âœ…' if self.supports_fp8 else 'âŒ'}\n"
                f"\n"
                f"ğŸ“Š Batch Size:\n"
                f"   - GPUë‹¹ ë°°ì¹˜: {grpo_config.per_device_train_batch_size}\n"
                f"   - Gradient accumulation: {grpo_config.gradient_accumulation_steps}\n"
                f"   - Num generations: {num_generations}\n"
                f"   - ìœ íš¨ ë°°ì¹˜: {effective_batch_size}\n"
                f"\n"
                f"ğŸ“ Training:\n"
                f"   - Epochs: {grpo_config.num_train_epochs}\n"
                f"   - Learning rate: {grpo_config.learning_rate}\n"
                f"   - Warmup: {warmup_steps or f'{warmup_ratio:.1%} ratio'}\n"
                f"   - Beta: {grpo_config.beta}\n"
                f"\n"
                f"ğŸ’¾ Checkpoints:\n"
                f"   - Save every: {grpo_config.save_steps} steps\n"
                f"   - Max keep: {grpo_config.save_total_limit}\n"
                f"{'='*60}\n"
            )
        
        # ===== vLLM ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° (ì„ íƒì ) =====
        vllm_sampling_params = self._create_vllm_sampling_params()
        
        # ===== Trainer ì´ˆê¸°í™” =====
        trainer = TRL_GRPOTrainer(
            model=self.model,
            reward_funcs=self.reward_func,
            args=grpo_config,
            train_dataset=train_dataset,
            eval_dataset=validation_dataset,
            processing_class=self.tokenizer,
            # vllm_sampling_params=vllm_sampling_params,  # í•„ìš” ì‹œ ì‚¬ìš©
        )
        
        # ===== _generate_and_score_completions í•¨ìˆ˜ ë˜í•‘ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€) =====
        try:
            original_generate_and_score = trainer._generate_and_score_completions
            
            def wrapped_generate_and_score_completions(inputs):
                """ë””ë²„ê¹… ë¡œê·¸ê°€ ì¶”ê°€ëœ _generate_and_score_completions"""
                # ì…ë ¥ ë°°ì¹˜ í¬ê¸° í™•ì¸
                input_batch_size = len(inputs) if inputs else 0
                print(f"[DEBUG _generate_and_score_completions] ì…ë ¥ ë°°ì¹˜ í¬ê¸°: {input_batch_size}", 
                      file=sys.stderr, flush=True)
                
                if input_batch_size == 0:
                    print(f"[WARNING _generate_and_score_completions] ë¹ˆ ì…ë ¥ ë°°ì¹˜ ê°ì§€!", 
                          file=sys.stderr, flush=True)
                    # ë¹ˆ ë°°ì¹˜ì— ëŒ€í•œ ê¸°ë³¸ ë°˜í™˜ê°’ ìƒì„±
                    device = trainer.accelerator.device
                    return {
                        'prompt_completion_ids': torch.empty((0, 0), dtype=torch.long, device=device),
                        'attention_mask': torch.empty((0, 0), dtype=torch.long, device=device),
                        'completion_mask': torch.empty((0, 0), dtype=torch.long, device=device),
                        'completion_ids': torch.empty((0, 0), dtype=torch.long, device=device),
                        'completion_ids_list': [],
                        'old_per_token_logps': None,
                        'ref_per_token_logps': None,
                        'sampling_per_token_logps': None,
                        'advantages': torch.empty((0,), dtype=torch.float32, device=device),
                    }
                
                # ì›ë³¸ í•¨ìˆ˜ í˜¸ì¶œ
                try:
                    result = original_generate_and_score(inputs)
                    
                    # ê²°ê³¼ ì „ì²´ êµ¬ì¡° í™•ì¸
                    print(f"[DEBUG _generate_and_score_completions] ê²°ê³¼ í‚¤: {list(result.keys()) if isinstance(result, dict) else 'N/A'}", 
                          file=sys.stderr, flush=True)
                    
                    # prompt_idsì™€ completion_ids í™•ì¸ (ì‹¤ì œ ë°˜í™˜ê°’ êµ¬ì¡°)
                    if 'prompt_ids' in result:
                        prompt_ids_bsz = result['prompt_ids'].shape[0] if hasattr(result['prompt_ids'], 'shape') else 0
                        prompt_ids_len = result['prompt_ids'].shape[1] if hasattr(result['prompt_ids'], 'shape') and len(result['prompt_ids'].shape) > 1 else 0
                        print(f"[DEBUG _generate_and_score_completions] prompt_ids.shape={result['prompt_ids'].shape if hasattr(result['prompt_ids'], 'shape') else 'N/A'}, bsz={prompt_ids_bsz}", 
                              file=sys.stderr, flush=True)
                    
                    if 'completion_ids' in result:
                        completion_ids_bsz = result['completion_ids'].shape[0] if hasattr(result['completion_ids'], 'shape') else 0
                        completion_ids_len = result['completion_ids'].shape[1] if hasattr(result['completion_ids'], 'shape') and len(result['completion_ids'].shape) > 1 else 0
                        print(f"[DEBUG _generate_and_score_completions] completion_ids.shape={result['completion_ids'].shape if hasattr(result['completion_ids'], 'shape') else 'N/A'}, bsz={completion_ids_bsz}", 
                              file=sys.stderr, flush=True)
                        
                        if completion_ids_bsz == 0:
                            print(f"[WARNING _generate_and_score_completions] completion_idsê°€ ë¹ˆ ë°°ì¹˜ì…ë‹ˆë‹¤! ì…ë ¥: {input_batch_size}ê°œ", 
                                  file=sys.stderr, flush=True)
                    
                    if 'completion_mask' in result:
                        comp_mask_bsz = result['completion_mask'].shape[0] if hasattr(result['completion_mask'], 'shape') else 0
                        print(f"[DEBUG _generate_and_score_completions] completion_mask.shape={result['completion_mask'].shape if hasattr(result['completion_mask'], 'shape') else 'N/A'}, bsz={comp_mask_bsz}", 
                              file=sys.stderr, flush=True)
                    
                    # ê²°ê³¼ ë°°ì¹˜ í¬ê¸° í™•ì¸ (prompt_completion_idsëŠ” ë°˜í™˜ê°’ì— ì—†ì„ ìˆ˜ ìˆìŒ)
                    if 'prompt_completion_ids' in result:
                        result_bsz = result['prompt_completion_ids'].shape[0] if hasattr(result['prompt_completion_ids'], 'shape') else 0
                        result_qlen = result['prompt_completion_ids'].shape[1] if hasattr(result['prompt_completion_ids'], 'shape') and len(result['prompt_completion_ids'].shape) > 1 else 0
                        print(f"[DEBUG _generate_and_score_completions] ê²°ê³¼ ë°°ì¹˜ í¬ê¸°: {result_bsz}, qlen: {result_qlen}", 
                              file=sys.stderr, flush=True)
                        
                        if result_bsz == 0:
                            print(f"[WARNING _generate_and_score_completions] ========== ë¹ˆ ê²°ê³¼ ë°°ì¹˜ ìƒì„±! ==========", 
                                  file=sys.stderr, flush=True)
                            print(f"[WARNING _generate_and_score_completions] ì…ë ¥ ë°°ì¹˜ í¬ê¸°: {input_batch_size}ê°œ", 
                                  file=sys.stderr, flush=True)
                            print(f"[WARNING _generate_and_score_completions] prompt_completion_ids.shape={result['prompt_completion_ids'].shape if hasattr(result['prompt_completion_ids'], 'shape') else 'N/A'}", 
                                  file=sys.stderr, flush=True)
                            
                            # completion_mask í™•ì¸
                            if 'completion_mask' in result and result['completion_mask'] is not None:
                                comp_mask = result['completion_mask']
                                if hasattr(comp_mask, 'shape'):
                                    print(f"[WARNING _generate_and_score_completions] completion_mask.shape={comp_mask.shape}", 
                                          file=sys.stderr, flush=True)
                                if hasattr(comp_mask, 'sum'):
                                    mask_sum = comp_mask.sum().item()
                                    print(f"[WARNING _generate_and_score_completions] completion_mask.sum()={mask_sum}", 
                                          file=sys.stderr, flush=True)
                            
                            # completion_ids í™•ì¸
                            if 'completion_ids' in result and result['completion_ids'] is not None:
                                comp_ids = result['completion_ids']
                                if hasattr(comp_ids, 'shape'):
                                    print(f"[WARNING _generate_and_score_completions] completion_ids.shape={comp_ids.shape}", 
                                          file=sys.stderr, flush=True)
                            
                            # attention_mask í™•ì¸
                            if 'attention_mask' in result and result['attention_mask'] is not None:
                                attn_mask = result['attention_mask']
                                if hasattr(attn_mask, 'shape'):
                                    print(f"[WARNING _generate_and_score_completions] attention_mask.shape={attn_mask.shape}", 
                                          file=sys.stderr, flush=True)
                            
                            print(f"[WARNING _generate_and_score_completions] =========================================", 
                                  file=sys.stderr, flush=True)
                        else:
                            # promptì™€ completion ê¸¸ì´ í™•ì¸
                            prompt_ids = result['prompt_completion_ids']
                            if 'completion_mask' in result and result['completion_mask'] is not None:
                                comp_mask = result['completion_mask']
                                if hasattr(comp_mask, 'shape') and len(comp_mask.shape) >= 2:
                                    comp_len = comp_mask.shape[1]
                                    prompt_len = prompt_ids.shape[1] - comp_len if len(prompt_ids.shape) > 1 else 0
                                    print(f"[DEBUG _generate_and_score_completions] prompt_len={prompt_len}, completion_len={comp_len}", 
                                          file=sys.stderr, flush=True)
                    else:
                        print(f"[ERROR _generate_and_score_completions] 'prompt_completion_ids' í‚¤ê°€ ê²°ê³¼ì— ì—†ìŠµë‹ˆë‹¤!", 
                              file=sys.stderr, flush=True)
                        print(f"[ERROR _generate_and_score_completions] ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(result.keys()) if isinstance(result, dict) else 'N/A'}", 
                              file=sys.stderr, flush=True)
                    
                    return result
                except Exception as e:
                    print(f"[ERROR _generate_and_score_completions] ì—ëŸ¬ ë°œìƒ: {type(e).__name__}: {e}", 
                          file=sys.stderr, flush=True)
                    import traceback
                    print(f"[ERROR _generate_and_score_completions] íŠ¸ë ˆì´ìŠ¤ë°±:\n{traceback.format_exc()}", 
                          file=sys.stderr, flush=True)
                    raise
            
            # í•¨ìˆ˜ êµì²´
            trainer._generate_and_score_completions = wrapped_generate_and_score_completions
            if not self.is_distributed or self.rank == 0:
                logger.info("âœ… _generate_and_score_completions í•¨ìˆ˜ ë˜í•‘ ì™„ë£Œ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)")
        except Exception as e:
            if not self.is_distributed or self.rank == 0:
                logger.warning(f"âš ï¸ _generate_and_score_completions ë˜í•‘ ì‹¤íŒ¨: {e}")
        
        # ===== compute_loss í•¨ìˆ˜ ë˜í•‘ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€) =====
        # ì£¼ì˜: compute_lossëŠ” Trainerì˜ ë©”ì„œë“œì´ë¯€ë¡œ ë˜í•‘ ì‹œ ì¬ê·€ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
        # ëŒ€ì‹  grpo_accumulated_lossì—ì„œë§Œ ë¡œê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ ë¬¸ì œ ì¶”ì 
        # compute_loss ë˜í•‘ì€ ì œê±° (ì¬ê·€ ì—ëŸ¬ ë°©ì§€)
        
        # ===== grpo_accumulated_loss í•¨ìˆ˜ ëŸ°íƒ€ì„ íŒ¨ì¹˜ (ë¹ˆ ë°°ì¹˜ ë°©ì–´) =====
        try:
            import sys
            import numpy as np
            import importlib
            
            # ìºì‹œ íŒŒì¼ì—ì„œ ì§ì ‘ grpo_accumulated_loss í•¨ìˆ˜ ì°¾ê¸° (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
            cache_file = Path(__file__).parent.parent / "unsloth_compiled_cache" / "UnslothGRPOTrainer.py"
            import time
            max_wait = 10
            waited = 0
            while not cache_file.exists() and waited < max_wait:
                time.sleep(0.5)
                waited += 0.5
            
            grpo_func = None
            if cache_file.exists():
                import importlib.util
                spec = importlib.util.spec_from_file_location("unsloth_compiled_cache_UnslothGRPOTrainer", cache_file)
                if spec and spec.loader:
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    if hasattr(module, 'grpo_accumulated_loss'):
                        candidate = module.grpo_accumulated_loss
                        # callableì¸ì§€ í™•ì¸
                        if callable(candidate):
                            grpo_func = candidate
                            if not self.is_distributed or self.rank == 0:
                                logger.info(f"âœ… grpo_accumulated_loss í•¨ìˆ˜ ë°œê²¬: ìºì‹œ íŒŒì¼ (callable í™•ì¸ ì™„ë£Œ)")
                        else:
                            if not self.is_distributed or self.rank == 0:
                                logger.warning(f"âš ï¸ ìºì‹œ íŒŒì¼ì˜ grpo_accumulated_lossëŠ” callableì´ ì•„ë‹™ë‹ˆë‹¤: {type(candidate)}")
            
            # sys.modulesì—ì„œë„ ì°¾ê¸° (ë°±ì—… ë°©ë²•)
            if grpo_func is None:
                for module_name, module in sys.modules.items():
                    if hasattr(module, 'grpo_accumulated_loss'):
                        candidate = getattr(module, 'grpo_accumulated_loss')
                        # callableì´ê³  unsloth ê´€ë ¨ ëª¨ë“ˆì¸ì§€ í™•ì¸
                        if callable(candidate) and ('unsloth' in module_name.lower() or 'grpo' in module_name.lower()):
                            grpo_func = candidate
                            if not self.is_distributed or self.rank == 0:
                                logger.info(f"âœ… grpo_accumulated_loss í•¨ìˆ˜ ë°œê²¬: {module_name}")
                            break
            
            if grpo_func is not None:
                # ì›ë³¸ í•¨ìˆ˜ ì €ì¥
                original_grpo_accumulated_loss = grpo_func
                
                # íŒ¨ì¹˜ëœ í•¨ìˆ˜ ì •ì˜
                def patched_grpo_accumulated_loss(trainer, input_ids, attention_mask, logits_to_keep, 
                                                  completion_mask, advantages, old_hidden_states, 
                                                  ref_hidden_states, n_chunks=-1, **kwargs):
                    """ë¹ˆ ë°°ì¹˜ ë°©ì–´ê°€ ì¶”ê°€ëœ grpo_accumulated_loss"""
                    # ì…ë ¥ í™•ì¸ (ê°€ì¥ ë¨¼ì €)
                    print(f"[DEBUG grpo_accumulated_loss] ========== í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘ ==========", 
                          file=sys.stderr, flush=True)
                    if input_ids is not None:
                        print(f"[DEBUG grpo_accumulated_loss] ì…ë ¥ input_ids.shape={input_ids.shape if hasattr(input_ids, 'shape') else 'N/A'}", 
                              file=sys.stderr, flush=True)
                    if completion_mask is not None:
                        print(f"[DEBUG grpo_accumulated_loss] ì…ë ¥ completion_mask.shape={completion_mask.shape if hasattr(completion_mask, 'shape') else 'N/A'}", 
                              file=sys.stderr, flush=True)
                    if advantages is not None:
                        print(f"[DEBUG grpo_accumulated_loss] ì…ë ¥ advantages.shape={advantages.shape if hasattr(advantages, 'shape') else 'N/A'}", 
                              file=sys.stderr, flush=True)
                    
                    # input_ids shape ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                    try:
                        if input_ids is None or not hasattr(input_ids, 'shape') or len(input_ids.shape) < 2:
                            raise ValueError(f"Invalid input_ids: {input_ids}")
                        bsz, qlen = input_ids.shape
                    except (AttributeError, ValueError, TypeError) as e:
                        print(f"[DEBUG grpo_accumulated_loss] input_ids shape ì˜¤ë¥˜: {e}, input_ids={input_ids}", 
                              file=sys.stderr, flush=True)
                        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
                        device = getattr(trainer.model, 'device', None) if hasattr(trainer, 'model') else None
                        if device is None:
                            try:
                                device = input_ids.device if hasattr(input_ids, 'device') else None
                            except:
                                device = None
                        zero = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                        completion_length = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                        empty_tensor = torch.tensor([], device=device, dtype=torch.float32).detach() if device is not None else torch.tensor([], dtype=torch.float32).detach()
                        return zero, completion_length, zero, zero, empty_tensor
                    
                    # ë””ë²„ê¹…: stderrë¡œ ê°•ì œ ì¶œë ¥
                    # max_prompt_lengthì™€ max_completion_length í™•ì¸
                    max_prompt_len = getattr(trainer.args, 'max_prompt_length', None) if hasattr(trainer, 'args') else None
                    max_completion_len = getattr(trainer.args, 'max_completion_length', None) if hasattr(trainer, 'args') else None
                    max_total = (max_prompt_len + max_completion_len) if (max_prompt_len and max_completion_len) else None
                    
                    # prompt ê¸¸ì´ ê³„ì‚° (completion_maskì˜ ê¸¸ì´ë¡œ completion ê¸¸ì´ë¥¼ ì•Œ ìˆ˜ ìˆìŒ)
                    completion_len = completion_mask.size(1) if completion_mask is not None and hasattr(completion_mask, 'size') else None
                    prompt_len = qlen - completion_len if completion_len is not None else None
                    
                    # completion_mask ìƒì„¸ ì •ë³´
                    comp_mask_info = ""
                    if completion_mask is not None and hasattr(completion_mask, 'sum'):
                        comp_mask_sum = completion_mask.sum().item()
                        comp_mask_shape = completion_mask.shape if hasattr(completion_mask, 'shape') else "unknown"
                        comp_mask_info = f", completion_mask.shape={comp_mask_shape}, completion_mask.sum()={comp_mask_sum}"
                    
                    # ì…ë ¥ ìƒì„¸ ì •ë³´ (ë¹ˆ ë°°ì¹˜ ì¶”ì ìš©)
                    print(f"[DEBUG grpo_accumulated_loss] ========== í•¨ìˆ˜ ì§„ì… ==========", 
                          file=sys.stderr, flush=True)
                    print(f"[DEBUG grpo_accumulated_loss] bsz={bsz}, qlen={qlen} (prompt+completion), n_chunks={n_chunks}{comp_mask_info}", 
                          file=sys.stderr, flush=True)
                    if prompt_len is not None:
                        print(f"[DEBUG grpo_accumulated_loss] prompt_len={prompt_len}, completion_len={completion_len}", 
                              file=sys.stderr, flush=True)
                    
                    # ëª¨ë“  ì…ë ¥ í…ì„œ shape í™•ì¸
                    if input_ids is not None and hasattr(input_ids, 'shape'):
                        input_shape_before = input_ids.shape
                        print(f"[DEBUG grpo_accumulated_loss] input_ids.shape (before left_pack)={input_shape_before}", 
                              file=sys.stderr, flush=True)
                    
                    if attention_mask is not None and hasattr(attention_mask, 'shape'):
                        print(f"[DEBUG grpo_accumulated_loss] attention_mask.shape={attention_mask.shape}", 
                              file=sys.stderr, flush=True)
                    
                    if advantages is not None and hasattr(advantages, 'shape'):
                        print(f"[DEBUG grpo_accumulated_loss] advantages.shape={advantages.shape}", 
                              file=sys.stderr, flush=True)
                    
                    print(f"[DEBUG grpo_accumulated_loss] =============================", 
                          file=sys.stderr, flush=True)
                    if max_prompt_len and prompt_len is not None and prompt_len > max_prompt_len:
                        print(f"[WARNING grpo_accumulated_loss] prompt_len({prompt_len}) > max_prompt_length({max_prompt_len})!", 
                              file=sys.stderr, flush=True)
                    if max_total and qlen > max_total:
                        print(f"[WARNING grpo_accumulated_loss] qlen({qlen}=prompt+completion) > max_total({max_total}=max_prompt+max_completion)!", 
                              file=sys.stderr, flush=True)
                    
                    # ë°©ì–´: ë¹ˆ ë°°ì¹˜ê°€ ë“¤ì–´ì˜¤ë©´ 0 loss ë°˜í™˜ (ëª¨ë“  ë°˜í™˜ê°’ì„ í…ì„œë¡œ ë§ì¶¤)
                    if bsz == 0:
                        device = getattr(trainer.model, 'device', None) if hasattr(trainer, 'model') else None
                        if device is None:
                            try:
                                device = input_ids.device if input_ids is not None and hasattr(input_ids, 'device') else None
                            except:
                                device = None
                        zero = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                        completion_length = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                        # flat_is_ratioëŠ” ë¹ˆ í…ì„œë¡œ ë°˜í™˜ (Noneì´ë©´ compute_lossì—ì„œ .numel() í˜¸ì¶œ ì‹œ ì—ëŸ¬)
                        empty_tensor = torch.tensor([], device=device, dtype=torch.float32).detach() if device is not None else torch.tensor([], dtype=torch.float32).detach()
                        
                        # ë¹ˆ ë°°ì¹˜ ì›ì¸ ìƒì„¸ ë¶„ì„
                        print(f"[WARNING grpo_accumulated_loss] ========== ë¹ˆ ë°°ì¹˜ ê°ì§€ ==========", 
                              file=sys.stderr, flush=True)
                        print(f"[WARNING grpo_accumulated_loss] bsz=0, qlen={qlen}", 
                              file=sys.stderr, flush=True)
                        print(f"[WARNING grpo_accumulated_loss] prompt_len={prompt_len if prompt_len else 'unknown'}, completion_len={completion_len if completion_len else 'unknown'}", 
                              file=sys.stderr, flush=True)
                        print(f"[WARNING grpo_accumulated_loss] max_prompt_length={max_prompt_len}, max_completion_length={max_completion_len}", 
                              file=sys.stderr, flush=True)
                        
                        # input_ids ìƒì„¸ ì •ë³´
                        if input_ids is not None:
                            print(f"[WARNING grpo_accumulated_loss] input_ids.shape={input_ids.shape if hasattr(input_ids, 'shape') else 'N/A'}", 
                                  file=sys.stderr, flush=True)
                        
                        # completion_mask ìƒì„¸ ë¶„ì„
                        if completion_mask is not None and hasattr(completion_mask, 'sum'):
                            completion_mask_sum = completion_mask.sum().item()
                            completion_mask_shape = completion_mask.shape if hasattr(completion_mask, 'shape') else 'unknown'
                            print(f"[WARNING grpo_accumulated_loss] completion_mask.shape={completion_mask_shape}, completion_mask.sum()={completion_mask_sum}", 
                                  file=sys.stderr, flush=True)
                            
                            if completion_mask_sum == 0:
                                print(f"[WARNING grpo_accumulated_loss] ì›ì¸: ëª¨ë“  completionì´ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤!", 
                                      file=sys.stderr, flush=True)
                                print(f"[WARNING grpo_accumulated_loss] í•´ê²°ì±…: max_completion_lengthë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ promptë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.", 
                                      file=sys.stderr, flush=True)
                            else:
                                print(f"[WARNING grpo_accumulated_loss] ì›ì¸: bsz=0ì¸ë° completion_mask.sum()={completion_mask_sum} (ì´ìƒí•¨!)", 
                                      file=sys.stderr, flush=True)
                        else:
                            print(f"[WARNING grpo_accumulated_loss] ì›ì¸: completion_maskê°€ Noneì´ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ", 
                                  file=sys.stderr, flush=True)
                        
                        # attention_mask í™•ì¸
                        if attention_mask is not None:
                            attn_mask_shape = attention_mask.shape if hasattr(attention_mask, 'shape') else 'unknown'
                            attn_mask_sum = attention_mask.sum().item() if hasattr(attention_mask, 'sum') else 'N/A'
                            print(f"[WARNING grpo_accumulated_loss] attention_mask.shape={attn_mask_shape}, attention_mask.sum()={attn_mask_sum}", 
                                  file=sys.stderr, flush=True)
                        
                        print(f"[WARNING grpo_accumulated_loss] =========================================", 
                              file=sys.stderr, flush=True)
                        
                        return zero, completion_length, zero, zero, empty_tensor
                    
                    # left_pack_padding í›„ ìƒíƒœ í™•ì¸ (ì›ë³¸ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ í˜¸ì¶œë¨)
                    # ì›ë³¸ í•¨ìˆ˜ í˜¸ì¶œ ì „ì— try-exceptë¡œ ê°ì‹¸ì„œ ì—ëŸ¬ ì²˜ë¦¬
                    try:
                        result = original_grpo_accumulated_loss(
                            trainer, input_ids, attention_mask, logits_to_keep, completion_mask,
                            advantages, old_hidden_states, ref_hidden_states, n_chunks, **kwargs
                        )
                        
                        # ê²°ê³¼ ê²€ì¦
                        if result is not None and len(result) > 0:
                            # ì²« ë²ˆì§¸ ë°˜í™˜ê°’ì´ lossì¸ì§€ í™•ì¸
                            if isinstance(result[0], torch.Tensor):
                                loss_val = result[0].item() if hasattr(result[0], 'item') else None
                                if loss_val is not None and (torch.isnan(result[0]) or torch.isinf(result[0])):
                                    print(f"[WARNING grpo_accumulated_loss] ë°˜í™˜ëœ lossê°€ NaN ë˜ëŠ” Inf: {loss_val}", 
                                          file=sys.stderr, flush=True)
                        
                        return result
                    except (IndexError, AttributeError, RuntimeError, ValueError) as e:
                        # ë‹¤ì–‘í•œ ì—ëŸ¬ íƒ€ì… ì²˜ë¦¬ (IndexError, AttributeError, RuntimeError, ValueError ë“±)
                        error_type = type(e).__name__
                        
                        # IndexErrorì˜ ê²½ìš° factors ê³„ì‚° ì‹œë„
                        factors = []
                        if isinstance(e, IndexError):
                            try:
                                factors = [i for i in range(1, bsz + 1) if bsz % i == 0] if bsz > 0 else []
                            except:
                                factors = []
                        
                        print(f"[DEBUG grpo_accumulated_loss] {error_type} ë°œìƒ! bsz={bsz}, factors={factors}, len(factors)={len(factors)}, n_chunks={n_chunks}", 
                              file=sys.stderr, flush=True)
                        print(f"[DEBUG grpo_accumulated_loss] ì—ëŸ¬ ë©”ì‹œì§€: {e}", file=sys.stderr, flush=True)
                        
                        # factorsê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš° 0 loss ë°˜í™˜ (ëª¨ë“  ë°˜í™˜ê°’ì„ í…ì„œë¡œ ë§ì¶¤)
                        if len(factors) == 0 or bsz <= 0:
                            device = getattr(trainer.model, 'device', None) if hasattr(trainer, 'model') else None
                            if device is None:
                                try:
                                    device = input_ids.device if hasattr(input_ids, 'device') else None
                                except:
                                    device = None
                            zero = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                            completion_length = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                            # flat_is_ratioëŠ” ë¹ˆ í…ì„œë¡œ ë°˜í™˜ (Noneì´ë©´ compute_lossì—ì„œ .numel() í˜¸ì¶œ ì‹œ ì—ëŸ¬)
                            empty_tensor = torch.tensor([], device=device, dtype=torch.float32).detach() if device is not None else torch.tensor([], dtype=torch.float32).detach()
                            print(f"[DEBUG grpo_accumulated_loss] factors ë¬¸ì œë¡œ 0 loss ë°˜í™˜", file=sys.stderr, flush=True)
                            return zero, completion_length, zero, zero, empty_tensor
                        
                        # ë‹¤ì‹œ ì‹œë„ (ì´ë¡ ìƒ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•˜ì§€ë§Œ)
                        print(f"[DEBUG grpo_accumulated_loss] {error_type} ì¬ë°œìƒ", file=sys.stderr, flush=True)
                        raise
                
                # ëª¨ë“  ëª¨ë“ˆì—ì„œ í•¨ìˆ˜ êµì²´ (callableì¸ ê²½ìš°ë§Œ)
                patched_count = 0
                for module_name, module in sys.modules.items():
                    if hasattr(module, 'grpo_accumulated_loss'):
                        existing = getattr(module, 'grpo_accumulated_loss')
                        # callableì´ê³  unsloth ê´€ë ¨ ëª¨ë“ˆì¸ ê²½ìš°ë§Œ íŒ¨ì¹˜
                        if callable(existing) and ('unsloth' in module_name.lower() or 'grpo' in module_name.lower()):
                            setattr(module, 'grpo_accumulated_loss', patched_grpo_accumulated_loss)
                            patched_count += 1
                            if not self.is_distributed or self.rank == 0:
                                logger.info(f"âœ… {module_name}ì˜ grpo_accumulated_loss íŒ¨ì¹˜ ì™„ë£Œ")
                
                # ìºì‹œ íŒŒì¼ ëª¨ë“ˆì—ë„ ì§ì ‘ íŒ¨ì¹˜
                if cache_file.exists():
                    try:
                        import importlib.util
                        spec = importlib.util.spec_from_file_location("unsloth_compiled_cache_UnslothGRPOTrainer", cache_file)
                        if spec and spec.loader:
                            module = importlib.util.module_from_spec(spec)
                            spec.loader.exec_module(module)
                            if hasattr(module, 'grpo_accumulated_loss'):
                                module.grpo_accumulated_loss = patched_grpo_accumulated_loss
                                patched_count += 1
                                if not self.is_distributed or self.rank == 0:
                                    logger.info(f"âœ… ìºì‹œ íŒŒì¼ ëª¨ë“ˆì˜ grpo_accumulated_loss íŒ¨ì¹˜ ì™„ë£Œ")
                    except Exception as e:
                        if not self.is_distributed or self.rank == 0:
                            logger.debug(f"ìºì‹œ íŒŒì¼ ëª¨ë“ˆ íŒ¨ì¹˜ ì‹œë„ ì¤‘ ì—ëŸ¬ (ë¬´ì‹œ): {e}")
                
                if not self.is_distributed or self.rank == 0:
                    logger.info(f"âœ… ì´ {patched_count}ê°œ ëª¨ë“ˆì—ì„œ grpo_accumulated_loss íŒ¨ì¹˜ ì™„ë£Œ")
                
                # trainer ë‚´ë¶€ì—ì„œë„ ì§ì ‘ ì°¸ì¡°í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ compute_loss ì˜¤ë²„ë¼ì´ë“œ
                original_compute_loss = trainer.compute_loss
                def patched_compute_loss(model, inputs, return_outputs=False, num_items_in_batch=None):
                    """compute_loss ë˜í¼ - ë¹ˆ ë°°ì¹˜ ì²´í¬ (ë‹¤ì–‘í•œ ì—ëŸ¬ ë°©ì–´)"""
                    try:
                        # ì›ë³¸ í•¨ìˆ˜ í˜¸ì¶œ
                        return original_compute_loss(model, inputs, return_outputs, num_items_in_batch)
                    except (IndexError, AttributeError, RuntimeError) as e:
                        # ë‹¤ì–‘í•œ ì—ëŸ¬ íƒ€ì… ì²˜ë¦¬ (IndexError, AttributeError, RuntimeError ë“±)
                        error_type = type(e).__name__
                        prompt_ids = inputs.get("prompt_ids", None)
                        completion_ids = inputs.get("completion_ids", None)
                        
                        print(f"[DEBUG compute_loss] {error_type} ë°œìƒ: {e}", file=sys.stderr, flush=True)
                        if prompt_ids is not None:
                            try:
                                print(f"[DEBUG compute_loss] prompt_ids.shape={prompt_ids.shape}", file=sys.stderr, flush=True)
                            except:
                                print(f"[DEBUG compute_loss] prompt_ids={prompt_ids}", file=sys.stderr, flush=True)
                        if completion_ids is not None:
                            try:
                                print(f"[DEBUG compute_loss] completion_ids.shape={completion_ids.shape}", file=sys.stderr, flush=True)
                            except:
                                print(f"[DEBUG compute_loss] completion_ids={completion_ids}", file=sys.stderr, flush=True)
                        
                        # ì‹¤ì œë¡œ ë¹ˆ ë°°ì¹˜ì¸ ê²½ìš°ì—ë§Œ 0 loss ë°˜í™˜
                        if prompt_ids is not None and completion_ids is not None:
                            try:
                                prompt_bsz = prompt_ids.shape[0] if hasattr(prompt_ids, 'shape') and len(prompt_ids.shape) > 0 else -1
                                completion_bsz = completion_ids.shape[0] if hasattr(completion_ids, 'shape') and len(completion_ids.shape) > 0 else -1
                                
                                if prompt_bsz == 0 or completion_bsz == 0:
                                    device = getattr(model, 'device', None) if hasattr(model, 'device') else None
                                    if device is None:
                                        try:
                                            if prompt_bsz > 0 and hasattr(prompt_ids, 'device'):
                                                device = prompt_ids.device
                                            elif completion_bsz > 0 and hasattr(completion_ids, 'device'):
                                                device = completion_ids.device
                                        except:
                                            device = None
                                    zero = torch.tensor(0.0, device=device) if device is not None else torch.tensor(0.0)
                                    print(f"[DEBUG compute_loss] ë¹ˆ ë°°ì¹˜ í™•ì¸! 0 loss ë°˜í™˜ (prompt_bsz={prompt_bsz}, completion_bsz={completion_bsz})", 
                                          file=sys.stderr, flush=True)
                                    return zero
                            except Exception as shape_err:
                                print(f"[DEBUG compute_loss] shape ì²´í¬ ì¤‘ ì—ëŸ¬: {shape_err}", file=sys.stderr, flush=True)
                        
                        # ë¹ˆ ë°°ì¹˜ê°€ ì•„ë‹Œë° ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš°ëŠ” ì¬ë°œìƒ
                        print(f"[DEBUG compute_loss] ë¹ˆ ë°°ì¹˜ê°€ ì•„ë‹Œë° {error_type} ë°œìƒ, ì¬ë°œìƒ", file=sys.stderr, flush=True)
                        raise
                
                trainer.compute_loss = patched_compute_loss
                
                if not self.is_distributed or self.rank == 0:
                    logger.info("âœ… grpo_accumulated_loss í•¨ìˆ˜ íŒ¨ì¹˜ ì™„ë£Œ (ë¹ˆ ë°°ì¹˜ ë°©ì–´ + ë””ë²„ê¹…)")
            else:
                if not self.is_distributed or self.rank == 0:
                    logger.warning("âš ï¸ grpo_accumulated_loss í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        except Exception as e:
            if not self.is_distributed or self.rank == 0:
                logger.warning(f"âš ï¸ grpo_accumulated_loss íŒ¨ì¹˜ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
                import traceback
                logger.debug(traceback.format_exc())
        
        # ===== í›ˆë ¨ ì‹¤í–‰ =====
        if not self.is_distributed or self.rank == 0:
            logger.info("ğŸƒ í›ˆë ¨ ì‹œì‘!\n")
        
        try:
            trainer.train()
            
            # ===== ëª¨ë¸ ì €ì¥ (rank 0ì—ì„œë§Œ) =====
            if not self.is_distributed or self.rank == 0:
                logger.info(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {save_dir}")
                trainer.save_model(save_dir)
                self.tokenizer.save_pretrained(save_dir)
                logger.info("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ í›ˆë ¨ ì¤‘ ì—ëŸ¬ ë°œìƒ: {error_msg}", exc_info=True)
            
            # CUDA illegal memory access ì—ëŸ¬ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´
            if "illegal memory access" in error_msg.lower() or "CUDA error" in error_msg:
                logger.error(
                    "\n" + "="*60 + "\n"
                    "ğŸ” CUDA Illegal Memory Access ì—ëŸ¬ í•´ê²° ë°©ë²•:\n"
                    "1. vLLM GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì„ ë” ë‚®ì¶”ì„¸ìš” (í˜„ì¬: 0.2)\n"
                    "2. ë°°ì¹˜ í¬ê¸°ë‚˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¤„ì´ì„¸ìš”\n"
                    "3. CUDA_LAUNCH_BLOCKING=1 í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ ë””ë²„ê¹…í•˜ì„¸ìš”:\n"
                    "   export CUDA_LAUNCH_BLOCKING=1\n"
                    "4. vLLMì„ server ëª¨ë“œë¡œ ë³€ê²½í•˜ì—¬ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”\n"
                    "="*60
                )
            
            raise
        finally:
            # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬ (ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì‹œë„)
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass  # CUDA ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš° empty_cacheë„ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
        
        if not self.is_distributed or self.rank == 0:
            logger.info("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ!")


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 3 ë©”ì¸ í•¨ìˆ˜"""
    
    # ===== DDP í™˜ê²½ ë³€ìˆ˜ ì„¤ì • =====
    rank = int(os.environ.get("RANK", -1))
    local_rank = int(os.environ.get("LOCAL_RANK", -1))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    is_distributed = rank >= 0
    
    if is_distributed:
        # DDP í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜
        os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR', '127.0.0.1')
        os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '29500')
        os.environ['NCCL_SOCKET_IFNAME'] = 'lo'
        os.environ['TORCH_DISTRIBUTED_TIMEOUT'] = '300'
        os.environ['NCCL_IB_DISABLE'] = '1'
        
        # vLLM í™˜ê²½ ë³€ìˆ˜ (ê° rankë³„ë¡œ ê³ ìœ í•˜ê²Œ)
        os.environ['VLLM_WORKER_NAME'] = f'worker_{rank}'
        os.environ['VLLM_INSTANCE_ID'] = str(rank)
        
        # ë””ë²„ê·¸ ì¶œë ¥
        print(
            f"[RANK {rank}] í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ\n"
            f"  - MASTER_ADDR: {os.environ['MASTER_ADDR']}\n"
            f"  - MASTER_PORT: {os.environ['MASTER_PORT']}\n"
            f"  - LOCAL_RANK: {local_rank}\n"
            f"  - WORLD_SIZE: {world_size}",
            file=sys.stderr
        )
        sys.stderr.flush()
    
    # ===== DDP ì´ˆê¸°í™” (TRL ì „ì— ìˆ˜ë™ìœ¼ë¡œ) =====
    if is_distributed and not dist.is_initialized():
        print(f"[RANK {rank}] DDP ì´ˆê¸°í™” ì‹œë„...", file=sys.stderr)
        sys.stderr.flush()
        
        try:
            dist.init_process_group(
                backend='nccl',
                init_method='env://',
                world_size=world_size,
                rank=rank,
                timeout=timedelta(seconds=60)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
            torch.cuda.set_device(local_rank)
            print(f"âœ… [RANK {rank}] DDP ì´ˆê¸°í™” ì„±ê³µ! (GPU {local_rank})", file=sys.stderr)
            sys.stderr.flush()
        except Exception as e:
            print(f"âŒ [RANK {rank}] DDP ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", file=sys.stderr)
            sys.stderr.flush()
            raise
    
    # ===== ë¡œê¹… ì„¤ì • (rank 0ì—ì„œë§Œ) =====
    if not is_distributed or rank == 0:
        log_file = os.path.join(cfg.paths.log_dir, "stage3_train.log")
        setup_logging(
            log_level=cfg.experiment.get("log_level", "INFO"),
            log_file=log_file,
            wandb_enabled=cfg.experiment.wandb.enabled,
            wandb_project=cfg.experiment.wandb.project,
            wandb_tags=cfg.experiment.wandb.tags
        )
        
        logger.info("ğŸš€ Stage 3: GRPO ëª¨ë¸ í›ˆë ¨ ì‹œì‘ (Unsloth + vLLM + TRL)")
        logger.info(f"ì„¤ì •: {cfg.training}")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cfg.paths.model_dir, exist_ok=True)
    
    # ===== ë°ì´í„° ê²½ë¡œ í™•ì¸ =====
    # train_data_path = os.path.join(cfg.paths.data_dir, "curated", "train_filtered.parquet")
    # validation_data_path = os.path.join(cfg.paths.data_dir, "curated", "validation_filtered.parquet")
    train_data_path = os.path.join(cfg.paths.data_dir, "curated", "train_curated.parquet")
    validation_data_path = os.path.join(cfg.paths.data_dir, "curated", "validation_curated.parquet")

    if not os.path.exists(train_data_path):
        if not is_distributed or rank == 0:
            logger.error(f"âŒ í›ˆë ¨ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_data_path}")
            logger.error("ë¨¼ì € Stage 2ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”: python scripts/stage2_curate.py")
        return
    
    # ===== ë°ì´í„°ì…‹ ìƒì„± =====
    if not is_distributed or rank == 0:
        logger.info("ğŸ“¦ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
    
    from src.data.training_dataset import CuratedTrainingDataset
    
    train_dataset = CuratedTrainingDataset(train_data_path)
    
    if not is_distributed or rank == 0:
        logger.info(f"âœ… í›ˆë ¨ ë°ì´í„°ì…‹: {len(train_dataset)} ìƒ˜í”Œ")
    
    validation_dataset = None
    # logger.info("ê²€ì¦ ë°ì´í„°ì…‹ ë¹„í™œì„±í™”")
    if os.path.exists(validation_data_path):
        validation_dataset = CuratedTrainingDataset(validation_data_path)
        # ë¹ˆ ê²€ì¦ ì„¸íŠ¸ëŠ” í‰ê°€ ë£¨í”„ì—ì„œ ë¹ˆ ë°°ì¹˜(bsz==0)ë¥¼ ìœ ë°œí•˜ì—¬ í¬ë˜ì‹œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¹„í™œì„±í™”
        if len(validation_dataset) == 0:
            if not is_distributed or rank == 0:
                logger.warning("âš ï¸ ê²€ì¦ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í‰ê°€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
            validation_dataset = None
        else:
            if not is_distributed or rank == 0:
                logger.info(f"âœ… ê²€ì¦ ë°ì´í„°ì…‹: {len(validation_dataset)} ìƒ˜í”Œ")
    
    # ===== GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” =====
    if not is_distributed or rank == 0:
        logger.info("ğŸ¦¥ Unsloth GRPO íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì¤‘...")
    
    trainer = OptimizedGRPOTrainer(
        model_name=cfg.model.base_model,
        lora_config=cfg.training.lora if cfg.training.method == "lora" else None,
        grpo_config=cfg.training.grpo,
        training_config=cfg.training.training,
        device=cfg.experiment.device
    )
    
    # ===== í›ˆë ¨ ì‹¤í–‰ =====
    if not is_distributed or rank == 0:
        logger.info("ğŸ‹ï¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    
    trainer.train(
        train_dataset=train_dataset,
        validation_dataset=validation_dataset,
        save_dir=cfg.paths.model_dir
    )
    
    if not is_distributed or rank == 0:
        logger.info("âœ… Stage 3 ì™„ë£Œ")
        logger.info(f"ğŸ“ í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {cfg.paths.model_dir}")
    
    # ===== DDP ì •ë¦¬ =====
    if is_distributed and dist.is_initialized():
        dist.destroy_process_group()


if __name__ == "__main__":
    main()