"""
Stage 4-1: Baselineê³¼ AggLLM Solution ìƒì„±
ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ë°°ì¹˜ë¡œ solution ìƒì„± í›„ ì €ì¥
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from datasets import load_dataset
from vllm import LLM, SamplingParams
import tempfile

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.data.confidence import ConfidenceCalculator
from src.evaluation.math_verifier import MathVerifier
from src.utils.logging import setup_logging
from src.evaluation.comprehensive_benchmark import (
    extract_reasoning_content,
    extract_content,
    simple_extract_topk
)

logger = logging.getLogger(__name__)


def load_dataset_data(dataset_name: str) -> list:
    """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    logger.info(f"ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {dataset_name}")
    
    try:
        # aimeëŠ” test, hmmtëŠ” trainìœ¼ë¡œ splitì„ ë‹¤ë¥´ê²Œ ë¡œë“œ
        if "aime" in dataset_name.lower():
            dataset = load_dataset(dataset_name, split="test")
        elif "hmmt" in dataset_name.lower():
            dataset = load_dataset(dataset_name, split="train")
        else:
            dataset = load_dataset(dataset_name, split="test")
        
        data = []
        for item in dataset:
            problem_text = item.get("problem", item.get("question", ""))
            ground_truth = item.get("answer", item.get("solution", ""))
            
            # aime24ì˜ answerê°€ \boxed{xxx} í˜•íƒœë¼ë©´ ì¤‘ê´„í˜¸ ì•ˆ ê°’ë§Œ ì €ì¥
            if "aime24" in dataset_name.lower() and isinstance(ground_truth, str):
                import re
                match = re.search(r"\\boxed\{([^{}]+)\}", ground_truth)
                if match:
                    ground_truth = match.group(1).strip()
            
            data.append({
                "problem_id": len(data),
                "problem_text": problem_text,
                "ground_truth": ground_truth
            })
        
        logger.info(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ ë¬¸ì œ")
        return data
        
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


def generate_solutions_batch(
    llm: LLM,
    tokenizer: AutoTokenizer,
    problems: list,
    num_solutions: int,
    base_instruction: str,
    enable_thinking: bool,
    temperature: float,
    max_tokens: int,
    top_p: float,
    top_k: int,
    min_p: float,
    logprobs: int,
    confidence_calculator: ConfidenceCalculator,
    math_verifier: MathVerifier
) -> list:
    """
    ì „ì²´ ë¬¸ì œì— ëŒ€í•´ ë°°ì¹˜ë¡œ solution ìƒì„±
    
    Returns:
        ê° ë¬¸ì œë³„ë¡œ 16ê°œ solutionì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ë°°ì¹˜ ìƒì„± ì‹œì‘: {len(problems)}ê°œ ë¬¸ì œ, ë¬¸ì œë‹¹ {num_solutions}ê°œ solution")
    
    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ë¬¸ì œë‹¹ num_solutionsê°œì”©)
    all_prompts = []
    problem_indices = []  # ê° í”„ë¡¬í”„íŠ¸ê°€ ì–´ëŠ ë¬¸ì œì— ì†í•˜ëŠ”ì§€
    
    for problem in problems:
        prompt = f"{problem['problem_text']}\n\n{base_instruction}"
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=enable_thinking,
        )
        
        # ê° ë¬¸ì œë‹¹ num_solutionsê°œì”© ì¶”ê°€
        for _ in range(num_solutions):
            all_prompts.append(formatted_prompt)
            problem_indices.append(problem['problem_id'])
    
    logger.info(f"ì´ {len(all_prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
    
    # SamplingParams ì„¤ì •
    sampling_params = SamplingParams(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        top_k=top_k,
        min_p=min_p,
        logprobs=logprobs,
    )
    
    # vLLMìœ¼ë¡œ ë°°ì¹˜ ìƒì„±
    logger.info("vLLM ë°°ì¹˜ ìƒì„± ì‹œì‘...")
    outputs = llm.generate(all_prompts, sampling_params)
    logger.info("vLLM ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
    
    # ê²°ê³¼ë¥¼ ë¬¸ì œë³„ë¡œ ê·¸ë£¹í™”
    problem_solutions = {pid: [] for pid in range(len(problems))}
    
    for idx, output in enumerate(outputs):
        problem_id = problem_indices[idx]
        generated_text = output.outputs[0].text
        
        # logprobs ì¶”ì¶œ
        logprobs_list = []
        if hasattr(output.outputs[0], 'logprobs') and output.outputs[0].logprobs:
            logprobs_list = simple_extract_topk(output.outputs[0].logprobs, logprobs)
        
        # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
        if logprobs_list:
            confidence_scores = confidence_calculator.calculate_all_confidence_scores(logprobs_list)
        else:
            confidence_scores = {
                "mean_group_confidence": 0.0,
                "bottom_10_percent_confidence": 0.0,
                "tail_confidence": 0.0,
                "lowest_group_confidence": 0.0
            }
        
        # enable_thinkingì— ë”°ë¼ íŒŒì‹±
        if enable_thinking:
            reasoning_content = extract_reasoning_content(generated_text)
            content = extract_content(generated_text)
            if not content:
                content = generated_text
        else:
            reasoning_content = ""
            content = generated_text
        
        # final_answer ì¶”ì¶œ
        final_answer = math_verifier.extract_final_answer_from_content(content)
        
        solution = {
            "generated_text": generated_text,
            "reasoning_content": reasoning_content,
            "content": content,
            "final_answer": final_answer,
            "confidence_scores": confidence_scores
        }
        
        problem_solutions[problem_id].append(solution)
    
    # ë¬¸ì œë³„ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    results = []
    for problem in problems:
        problem_id = problem['problem_id']
        results.append({
            "problem_id": problem_id,
            "problem_text": problem['problem_text'],
            "ground_truth": problem['ground_truth'],
            "solutions": problem_solutions[problem_id]
        })
    
    return results


def generate_baseline(
    model_name: str,
    problems: list,
    output_path: str,
    num_solutions: int,
    base_instruction: str,
    enable_thinking: bool,
    temperature: float,
    max_tokens: int,
    top_p: float,
    top_k: int,
    min_p: float,
    logprobs: int,
    gpu_memory_utilization: float,
    max_model_len: int,
    confidence_calculator: ConfidenceCalculator,
    math_verifier: MathVerifier
):
    """Baseline ëª¨ë¸ë¡œ solution ìƒì„±"""
    logger.info("=" * 60)
    logger.info("Baseline ëª¨ë¸ Solution ìƒì„± ì‹œì‘")
    logger.info("=" * 60)
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info(f"Baseline í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # vLLM ëª¨ë¸ ë¡œë“œ
    logger.info(f"Baseline vLLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
    llm = LLM(
        model=model_name,
        tensor_parallel_size=1,
        gpu_memory_utilization=gpu_memory_utilization,
        max_model_len=max_model_len,
        dtype="bfloat16",
        trust_remote_code=True,
        kv_cache_dtype="fp8"
    )
    logger.info("Baseline ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    try:
        # ë°°ì¹˜ ìƒì„±
        results = generate_solutions_batch(
            llm=llm,
            tokenizer=tokenizer,
            problems=problems,
            num_solutions=num_solutions,
            base_instruction=base_instruction,
            enable_thinking=enable_thinking,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            min_p=min_p,
            logprobs=logprobs,
            confidence_calculator=confidence_calculator,
            math_verifier=math_verifier
        )
        
        # ì €ì¥
        output_data = {
            "dataset_name": problems[0].get("dataset_name", "unknown"),
            "total_problems": len(problems),
            "generated_solutions": results
        }
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Baseline ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        
    finally:
        # ëª¨ë¸ unload
        logger.info("Baseline ëª¨ë¸ unload ì¤‘...")
        del llm
        torch.cuda.empty_cache()
        logger.info("Baseline ëª¨ë¸ unload ì™„ë£Œ")


def generate_aggllm(
    model_name: str,
    aggllm_model_path: str,
    problems: list,
    output_path: str,
    num_solutions: int,
    base_instruction: str,
    enable_thinking: bool,
    temperature: float,
    max_tokens: int,
    top_p: float,
    top_k: int,
    min_p: float,
    logprobs: int,
    gpu_memory_utilization: float,
    max_model_len: int,
    merged_model_cache_dir: str,
    confidence_calculator: ConfidenceCalculator,
    math_verifier: MathVerifier
):
    """AggLLM ëª¨ë¸ë¡œ solution ìƒì„±"""
    logger.info("=" * 60)
    logger.info("AggLLM ëª¨ë¸ Solution ìƒì„± ì‹œì‘")
    logger.info("=" * 60)
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    logger.info(f"AggLLM í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # LoRA ë³‘í•© ë° ì €ì¥
    logger.info("LoRA ê°€ì¤‘ì¹˜ë¥¼ base ëª¨ë¸ì— ë³‘í•© ì¤‘...")
    base_model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    peft_model = PeftModel.from_pretrained(base_model, aggllm_model_path)
    merged_model = peft_model.merge_and_unload()
    
    # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ ê²°ì •
    if merged_model_cache_dir:
        merged_model_path = merged_model_cache_dir
    else:
        merged_model_path = tempfile.mkdtemp(prefix="aggllm_merged_")
    
    os.makedirs(merged_model_path, exist_ok=True)
    
    # ë³‘í•©ëœ ëª¨ë¸ì´ ì´ë¯¸ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    config_path = os.path.join(merged_model_path, "config.json")
    if not os.path.exists(config_path):
        logger.info(f"ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {merged_model_path}")
        merged_model.save_pretrained(merged_model_path, safe_serialization=True)
        tokenizer.save_pretrained(merged_model_path)
        logger.info("ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    else:
        logger.info(f"ê¸°ì¡´ ë³‘í•©ëœ ëª¨ë¸ ì‚¬ìš©: {merged_model_path}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del base_model, peft_model, merged_model
    torch.cuda.empty_cache()
    
    # vLLMìœ¼ë¡œ ë¡œë“œ
    logger.info(f"vLLMìœ¼ë¡œ AggLLM ëª¨ë¸ ë¡œë“œ ì¤‘... (GPU memory utilization: {gpu_memory_utilization})")
    llm = LLM(
        model=merged_model_path,
        tensor_parallel_size=1,
        gpu_memory_utilization=gpu_memory_utilization,
        max_model_len=max_model_len,
        dtype="bfloat16",  # FP8 KV cache ì‚¬ìš© ì‹œ BF16 í•„ìš”
        trust_remote_code=True,
        kv_cache_dtype="fp8"
    )
    logger.info("AggLLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    try:
        # ë°°ì¹˜ ìƒì„±
        results = generate_solutions_batch(
            llm=llm,
            tokenizer=tokenizer,
            problems=problems,
            num_solutions=num_solutions,
            base_instruction=base_instruction,
            enable_thinking=enable_thinking,
            temperature=temperature,
            max_tokens=max_tokens,
            top_p=top_p,
            top_k=top_k,
            min_p=min_p,
            logprobs=logprobs,
            confidence_calculator=confidence_calculator,
            math_verifier=math_verifier
        )
        
        # ì €ì¥
        output_data = {
            "dataset_name": problems[0].get("dataset_name", "unknown"),
            "total_problems": len(problems),
            "generated_solutions": results
        }
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"AggLLM ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        
    finally:
        # ëª¨ë¸ unload
        logger.info("AggLLM ëª¨ë¸ unload ì¤‘...")
        del llm
        torch.cuda.empty_cache()
        logger.info("AggLLM ëª¨ë¸ unload ì™„ë£Œ")


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 4-1: Solution ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
    
    # GPU ì„¤ì • (CUDA_VISIBLE_DEVICESê°€ ì„¤ì •ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    if torch.cuda.is_available():
        # CUDA_VISIBLE_DEVICESê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ 0ë²ˆì´ ì‹¤ì œ GPU
        # ëª…ì‹œì ìœ¼ë¡œ GPUë¥¼ ì„¤ì •í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ê°„ ê²©ë¦¬ ë³´ì¥
        torch.cuda.set_device(0)
        torch.cuda.empty_cache()
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage4_1_generate.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    if torch.cuda.is_available():
        logger.info(f"GPU ì„¤ì •: device=0, GPU={torch.cuda.get_device_name(0)}")
    
    logger.info("ğŸš€ Stage 4-1: Solution ìƒì„± ì‹œì‘")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(cfg.paths.output_dir, exist_ok=True)
    results_dir = os.path.join(cfg.paths.output_dir, "comprehensive_results")
    results_dir = os.path.join(results_dir, cfg.model.base_model.replace('/', '_'))
    os.makedirs(results_dir, exist_ok=True)
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    checkpoint_num = cfg.evaluation.benchmarks.evaluation.checkpoint_num
    if checkpoint_num is not None:
        aggllm_model_path = os.path.join(cfg.paths.model_dir, f"checkpoint-{checkpoint_num}")
    else:
        aggllm_model_path = os.path.join(cfg.paths.model_dir, "checkpoint-final")
    
    if not os.path.exists(aggllm_model_path):
        logger.warning(f"AggLLM ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {aggllm_model_path}")
        logger.warning("Baseline ëª¨ë¸ë§Œ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•©ë‹ˆë‹¤.")
        aggllm_model_path = None
    
    # í‰ê°€ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
    eval_config = cfg.evaluation.benchmarks.evaluation
    enable_thinking = eval_config.get("enable_thinking", False)
    confidence_group_size = eval_config.get("confidence_group_size", 512)
    
    confidence_calculator = ConfidenceCalculator(group_size=confidence_group_size)
    math_verifier = MathVerifier(timeout=eval_config.timeout)
    
    base_instruction = "Please reason step by step, and put your final answer within \\boxed{}."
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì„¤ì •
    benchmark_datasets = [
        {"name": "AIME24", "path": "math-ai/aime24"},
        {"name": "AIME25", "path": "math-ai/aime25"},
        {"name": "HMMT24", "path": "MathArena/hmmt_feb_2024"},
        {"name": "HMMT25", "path": "MathArena/hmmt_feb_2025"},
    ]
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ìƒì„±
    for benchmark in benchmark_datasets:
        dataset_name = benchmark["name"]
        dataset_path = benchmark["path"]
        
        logger.info("=" * 60)
        logger.info(f"ë°ì´í„°ì…‹: {dataset_name}")
        logger.info("=" * 60)
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        problems = load_dataset_data(dataset_path)
        if not problems:
            logger.warning(f"{dataset_name} ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
            continue
        
        # dataset_name ì¶”ê°€
        for p in problems:
            p["dataset_name"] = dataset_name
        
        dataset_safe_name = dataset_path.replace('/', '_')
        
        # Stage 1: Baseline ìƒì„±
        baseline_output_path = os.path.join(
            results_dir, 
            f"{dataset_safe_name}_baseline_generated.json"
        )
        
        try:
            generate_baseline(
                model_name=cfg.model.base_model,
                problems=problems,
                output_path=baseline_output_path,
                num_solutions=32,
                base_instruction=base_instruction,
                enable_thinking=enable_thinking,
                temperature=eval_config.temperature,
                max_tokens=eval_config.max_tokens,
                top_p=eval_config.get("top_p", 0.95),
                top_k=eval_config.get("top_k", 20),
                min_p=eval_config.get("min_p", 0.0),
                logprobs=eval_config.get("logprobs", 5),
                gpu_memory_utilization=eval_config.get("gpu_memory_utilization", 0.9),
                max_model_len=eval_config.get("max_model_len", eval_config.max_tokens + 8192),
                confidence_calculator=confidence_calculator,
                math_verifier=math_verifier
            )
        except Exception as e:
            logger.error(f"{dataset_name} Baseline ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue
        
        # Stage 2: AggLLM ìƒì„±
        if aggllm_model_path:
            aggllm_output_path = os.path.join(
                results_dir,
                f"{dataset_safe_name}_aggllm_generated.json"
            )
            
            try:
                generate_aggllm(
                    model_name=cfg.model.base_model,
                    aggllm_model_path=aggllm_model_path,
                    problems=problems,
                    output_path=aggllm_output_path,
                    num_solutions=16,
                    base_instruction=base_instruction,
                    enable_thinking=enable_thinking,
                    temperature=eval_config.temperature,
                    max_tokens=eval_config.max_tokens,
                    top_p=eval_config.get("top_p", 0.8),
                    top_k=eval_config.get("top_k", 20),
                    min_p=eval_config.get("min_p", 0.0),
                    logprobs=eval_config.get("logprobs", 5),
                    gpu_memory_utilization=eval_config.get("aggllm_gpu_memory_utilization", 0.4),
                    max_model_len=eval_config.get("max_model_len", eval_config.max_tokens + 8192),
                    merged_model_cache_dir=cfg.paths.get("merged_model_cache_dir", None),
                    confidence_calculator=confidence_calculator,
                    math_verifier=math_verifier
                )
            except Exception as e:
                logger.error(f"{dataset_name} AggLLM ìƒì„± ì‹¤íŒ¨: {e}")
                import traceback
                logger.error(traceback.format_exc())
                continue
    
    logger.info("=" * 60)
    logger.info("âœ… Stage 4-1: Solution ìƒì„± ì™„ë£Œ")
    logger.info(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_dir}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()

