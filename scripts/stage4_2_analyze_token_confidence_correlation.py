"""
Stage 4-2 ì¶”ê°€ ë¶„ì„: Generated Content í† í° ìˆ˜, ì •í™•ë„, Confidence ìƒê´€ê´€ê³„ ë¶„ì„
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import json
import numpy as np
from scipy.stats import pearsonr, spearmanr
from collections import defaultdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.evaluation.math_verifier import MathVerifier
from src.utils.logging import setup_logging

logger = logging.getLogger(__name__)

# Tokenizer ë¡œë“œ í•¨ìˆ˜
try:
    from transformers import AutoTokenizer
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    logger.warning("transformersë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def load_tokenizer(model_name: str):
    """
    Tokenizer ë¡œë“œ
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
        
    Returns:
        tokenizer
    """
    if not HAS_TRANSFORMERS:
        raise ImportError("transformersê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    logger.info(f"Tokenizer ë¡œë“œ ì¤‘: {model_name}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        logger.info("âœ… Tokenizer ë¡œë“œ ì™„ë£Œ")
        return tokenizer
    except Exception as e:
        logger.error(f"Tokenizer ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise


def count_tokens_with_tokenizer(tokenizer, text: str) -> int:
    """
    Tokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°
    
    Args:
        tokenizer: tokenizer ì¸ìŠ¤í„´ìŠ¤
        text: í…ìŠ¤íŠ¸ ë¬¸ìì—´
        
    Returns:
        í† í° ìˆ˜
    """
    if not text or not isinstance(text, str):
        return 0
    
    try:
        # tokenizer.encode ì‚¬ìš©
        if hasattr(tokenizer, 'encode'):
            tokens = tokenizer.encode(text, add_special_tokens=False)
            return len(tokens)
        elif hasattr(tokenizer, 'tokenize'):
            tokens = tokenizer.tokenize(text)
            return len(tokens)
        else:
            # Fallback: ë‹¨ì–´ ìˆ˜ ì¶”ì •
            return len(text.split())
    except Exception as e:
        logger.warning(f"í† í° ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        # Fallback: ë‹¨ì–´ ìˆ˜ ì¶”ì •
        return len(str(text).split())


def extract_content_from_generated_text(generated_text: str) -> str:
    """
    </think> í† í° ì´í›„ ê°’ë“¤ ì¶”ì¶œ
    
    Args:
        generated_text: generated_text
        
    Returns:
        </think> ì´í›„ ë‚´ìš© (ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸)
    """
    if not generated_text or not isinstance(generated_text, str):
        return ""
    
    marker = "</think>"
    marker_pos = generated_text.find(marker)
    
    if marker_pos == -1:
        # ë§ˆì»¤ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜ (enable_thinking=Falseì¸ ê²½ìš°)
        return generated_text.strip()
    
    # ë§ˆì»¤ ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    content = generated_text[marker_pos + len(marker):].strip()
    return content


def analyze_token_confidence_correlation(
    baseline_path: str,
    aggllm_path: str,
    tokenizer,
    math_verifier: MathVerifier,
    output_dir: str,
    max_tokens: int = 16384
) -> dict:
    """
    Generated contentì˜ í† í° ìˆ˜, ì •í™•ë„, ê° confidenceì˜ ìƒê´€ê´€ê³„ ë¶„ì„
    
    Args:
        baseline_path: Baseline ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        aggllm_path: AggLLM ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        tokenizer: tokenizer ì¸ìŠ¤í„´ìŠ¤
        math_verifier: MathVerifier ì¸ìŠ¤í„´ìŠ¤
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {}
    
    # Baseline ë¶„ì„
    if os.path.exists(baseline_path):
        logger.info(f"Baseline ê²°ê³¼ ë¶„ì„: {baseline_path}")
        baseline_results = analyze_single_file(
            baseline_path, tokenizer, math_verifier, "baseline", max_tokens
        )
        results["baseline"] = baseline_results
    else:
        logger.warning(f"Baseline ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {baseline_path}")
    
    # AggLLM ë¶„ì„
    if os.path.exists(aggllm_path):
        logger.info(f"AggLLM ê²°ê³¼ ë¶„ì„: {aggllm_path}")
        aggllm_results = analyze_single_file(
            aggllm_path, tokenizer, math_verifier, "aggllm", max_tokens
        )
        results["aggllm"] = aggllm_results
    else:
        logger.warning(f"AggLLM ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {aggllm_path}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = os.path.join(output_dir, "token_confidence_correlation_analysis.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
    
    return results


def analyze_single_file(
    file_path: str,
    tokenizer,
    math_verifier: MathVerifier,
    method_name: str,
    max_tokens: int = 16384
) -> dict:
    """
    ë‹¨ì¼ íŒŒì¼ì— ëŒ€í•œ ë¶„ì„ ìˆ˜í–‰
    
    Args:
        file_path: JSON íŒŒì¼ ê²½ë¡œ
        tokenizer: tokenizer ì¸ìŠ¤í„´ìŠ¤
        math_verifier: MathVerifier ì¸ìŠ¤í„´ìŠ¤
        method_name: ë°©ë²• ì´ë¦„ (baseline ë˜ëŠ” aggllm)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 32768)
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"=== {method_name.upper()} ë¶„ì„ ===")
    logger.info(f"{'='*60}")
    logger.info(f"Max tokens ì„¤ì •: {max_tokens}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°ì´í„° ìˆ˜ì§‘ (instanceë³„ë¡œ ê·¸ë£¹í™”)
    all_data = []
    instance_data = defaultdict(lambda: {
        "problem_id": None,
        "ground_truth": None,
        "solutions": []
    })
    
    for problem_data in data["generated_solutions"]:
        problem_id = problem_data.get("problem_id", len(instance_data))
        ground_truth = problem_data["ground_truth"]
        
        instance_data[problem_id]["problem_id"] = problem_id
        instance_data[problem_id]["ground_truth"] = ground_truth
        
        for solution in problem_data["solutions"]:
            # Generated text ì „ì²´ í† í° ìˆ˜ ê³„ì‚° (max_tokens ë„ë‹¬ ì—¬ë¶€ í™•ì¸ìš©)
            generated_text = solution.get("generated_text", "")
            generated_text_token_count = count_tokens_with_tokenizer(tokenizer, generated_text)
            
            # Content ì¶”ì¶œ
            content = extract_content_from_generated_text(generated_text)
            
            # Content í† í° ìˆ˜ ê³„ì‚°
            content_token_count = count_tokens_with_tokenizer(tokenizer, content)
            
            # Max tokens ë„ë‹¬ ì—¬ë¶€ í™•ì¸ (99% ì´ìƒì´ë©´ ë„ë‹¬í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼)
            reached_max_tokens = generated_text_token_count >= (max_tokens * 0.99)
            
            # ì •ë‹µ ì—¬ë¶€ í™•ì¸
            final_answer = solution.get("final_answer", "")
            is_correct = False
            if final_answer:
                try:
                    is_correct = math_verifier.verify_answer(final_answer, ground_truth)
                except Exception as e:
                    logger.warning(f"ì •ë‹µ ê²€ì¦ ì‹¤íŒ¨: {e}")
                    is_correct = False
            
            # Confidence scores ì¶”ì¶œ
            confidence_scores = solution.get("confidence_scores", {})
            
            # ë°ì´í„° ì €ì¥
            solution_data = {
                "content_token_count": content_token_count,
                "generated_text_token_count": generated_text_token_count,
                "reached_max_tokens": reached_max_tokens,
                "is_correct": is_correct,
                "confidence_scores": confidence_scores
            }
            all_data.append(solution_data)
            instance_data[problem_id]["solutions"].append(solution_data)
    
    logger.info(f"ì´ {len(all_data)}ê°œ solution ë¶„ì„")
    
    # Instanceë³„ max_tokens ë„ë‹¬ í†µê³„
    instance_stats = []
    for problem_id, instance_info in instance_data.items():
        solutions = instance_info["solutions"]
        max_tokens_count = sum(1 for s in solutions if s["reached_max_tokens"])
        correct_count = sum(1 for s in solutions if s["is_correct"])
        total_count = len(solutions)
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        
        # í‰ê·  í† í° ìˆ˜ ê³„ì‚° (content_token_count ê¸°ì¤€)
        token_counts_list = [s["content_token_count"] for s in solutions]
        avg_token_count = np.mean(token_counts_list) if token_counts_list else 0.0
        
        instance_stats.append({
            "problem_id": problem_id,
            "max_tokens_reached_count": max_tokens_count,
            "total_solutions": total_count,
            "correct_count": correct_count,
            "accuracy": accuracy,
            "avg_token_count": float(avg_token_count)
        })
    
    logger.info(f"\n=== {method_name.upper()} Instanceë³„ Max Tokens ë„ë‹¬ í†µê³„ ===")
    for stat in instance_stats:
        logger.info(f"  Problem {stat['problem_id']}: {stat['max_tokens_reached_count']}/{stat['total_solutions']}ê°œ ë„ë‹¬, ì •í™•ë„: {stat['accuracy']:.3f}, í‰ê·  í† í° ìˆ˜: {stat['avg_token_count']:.1f}")
    
    # Max tokens ë„ë‹¬í•œ instance ëª©ë¡
    max_tokens_reached_instances = set(
        stat["problem_id"] for stat in instance_stats 
        if stat["max_tokens_reached_count"] > 0
    )
    
    logger.info(f"\nMax tokens ë„ë‹¬í•œ instance ìˆ˜: {len(max_tokens_reached_instances)}/{len(instance_stats)}")
    
    # Max tokens ë„ë‹¬í•œ solution ê°œìˆ˜ í•©ê³„
    total_max_tokens_reached_solutions = sum(
        stat["max_tokens_reached_count"] for stat in instance_stats
    )
    
    logger.info(f"Max tokens ë„ë‹¬í•œ solution ê°œìˆ˜ (ì‹¤ì œ ë„ë‹¬): {total_max_tokens_reached_solutions}ê°œ")
    
    # Max tokens ë„ë‹¬í•˜ì§€ ì•Šì€ inferenceë§Œ í•„í„°ë§ (instance ì „ì²´ê°€ ì•„ë‹Œ ê°œë³„ inferenceë§Œ)
    filtered_data = []
    excluded_data = []
    for d in all_data:
        if d["reached_max_tokens"]:
            excluded_data.append(d)
        else:
            filtered_data.append(d)
    
    logger.info(f"Max tokens ë„ë‹¬í•œ solution ìˆ˜ (ì œì™¸ë¨): {len(excluded_data)}ê°œ")
    logger.info(f"Max tokens ë„ë‹¬í•˜ì§€ ì•Šì€ solution ìˆ˜ (ë¶„ì„ì— í¬í•¨): {len(filtered_data)}/{len(all_data)}")
    
    # ë°°ì—´ë¡œ ë³€í™˜ (ì „ì²´ ë°ì´í„°)
    token_counts_all = np.array([d["content_token_count"] for d in all_data])
    is_corrects_all = np.array([d["is_correct"] for d in all_data], dtype=float)
    
    # ë°°ì—´ë¡œ ë³€í™˜ (í•„í„°ë§ëœ ë°ì´í„° - max_tokens ë„ë‹¬í•œ instance ì œì™¸)
    token_counts = np.array([d["content_token_count"] for d in filtered_data]) if filtered_data else np.array([])
    is_corrects = np.array([d["is_correct"] for d in filtered_data], dtype=float) if filtered_data else np.array([])
    
    # Confidence scores ì¶”ì¶œ (ì „ì²´ ë°ì´í„°)
    confidence_metrics = [
        "bottom_10_percent_confidence",
        "tail_confidence",
        "mean_group_confidence",
        "lowest_group_confidence",
        "top_10_percent_confidence",
        "highest_group_confidence"
    ]
    
    confidence_dict_all = {}
    for metric in confidence_metrics:
        values = []
        for d in all_data:
            conf = d["confidence_scores"].get(metric)
            if conf is not None:
                values.append(float(conf))
            else:
                values.append(np.nan)
        confidence_dict_all[metric] = np.array(values)
    
    # Confidence scores ì¶”ì¶œ (í•„í„°ë§ëœ ë°ì´í„°)
    confidence_dict = {}
    for metric in confidence_metrics:
        values = []
        for d in filtered_data:
            conf = d["confidence_scores"].get(metric)
            if conf is not None:
                values.append(float(conf))
            else:
                values.append(np.nan)
        confidence_dict[metric] = np.array(values) if filtered_data else np.array([])
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analysis_results = {
        "total_solutions": len(all_data),
        "max_tokens": max_tokens,
        "max_tokens_reached_instances": {
            "count": len(max_tokens_reached_instances),
            "total_instances": len(instance_stats),
            "instance_ids": sorted(list(max_tokens_reached_instances))
        },
        "instance_statistics": instance_stats,
        "filtered_solutions": {
            "count": len(filtered_data),
            "excluded_count": len(excluded_data),
            "excluded_max_tokens_reached_count": total_max_tokens_reached_solutions
        },
        "token_statistics": {
            "all_solutions": {
                "mean": float(np.mean(token_counts_all)) if len(token_counts_all) > 0 else 0.0,
                "median": float(np.median(token_counts_all)) if len(token_counts_all) > 0 else 0.0,
                "std": float(np.std(token_counts_all)) if len(token_counts_all) > 0 else 0.0,
                "min": int(np.min(token_counts_all)) if len(token_counts_all) > 0 else 0,
                "max": int(np.max(token_counts_all)) if len(token_counts_all) > 0 else 0,
            },
            "filtered_solutions": {
                "mean": float(np.mean(token_counts)) if len(token_counts) > 0 else 0.0,
                "median": float(np.median(token_counts)) if len(token_counts) > 0 else 0.0,
                "std": float(np.std(token_counts)) if len(token_counts) > 0 else 0.0,
                "min": int(np.min(token_counts)) if len(token_counts) > 0 else 0,
                "max": int(np.max(token_counts)) if len(token_counts) > 0 else 0,
                "percentiles": {
                    "25": float(np.percentile(token_counts, 25)) if len(token_counts) > 0 else 0.0,
                    "50": float(np.percentile(token_counts, 50)) if len(token_counts) > 0 else 0.0,
                    "75": float(np.percentile(token_counts, 75)) if len(token_counts) > 0 else 0.0,
                    "90": float(np.percentile(token_counts, 90)) if len(token_counts) > 0 else 0.0,
                    "95": float(np.percentile(token_counts, 95)) if len(token_counts) > 0 else 0.0,
                    "99": float(np.percentile(token_counts, 99)) if len(token_counts) > 0 else 0.0
                }
            }
        },
        "accuracy": {
            "all_solutions": {
                "overall": float(np.mean(is_corrects_all)) if len(is_corrects_all) > 0 else 0.0,
                "correct_count": int(np.sum(is_corrects_all)),
                "total_count": len(is_corrects_all)
            },
            "filtered_solutions": {
                "overall": float(np.mean(is_corrects)) if len(is_corrects) > 0 else 0.0,
                "correct_count": int(np.sum(is_corrects)),
                "total_count": len(is_corrects)
            }
        },
        "correlations": {}
    }
    
    # í† í° ìˆ˜ì™€ ì •í™•ë„ ìƒê´€ê´€ê³„
    valid_mask = ~np.isnan(token_counts) & ~np.isnan(is_corrects)
    if valid_mask.sum() > 0:
        valid_tokens = token_counts[valid_mask]
        valid_corrects = is_corrects[valid_mask]
        
        if len(valid_tokens) > 1 and len(np.unique(valid_tokens)) > 1:
            pearson_corr, pearson_p = pearsonr(valid_tokens, valid_corrects)
            spearman_corr, spearman_p = spearmanr(valid_tokens, valid_corrects)
            
            analysis_results["correlations"]["token_count_vs_accuracy"] = {
                "pearson": {
                    "correlation": float(pearson_corr),
                    "p_value": float(pearson_p)
                },
                "spearman": {
                    "correlation": float(spearman_corr),
                    "p_value": float(spearman_p)
                }
            }
    
    # ê° Confidenceì™€ ì •í™•ë„ ìƒê´€ê´€ê³„
    for metric in confidence_metrics:
        conf_values = confidence_dict[metric]
        valid_mask = ~np.isnan(conf_values) & ~np.isnan(is_corrects)
        
        if valid_mask.sum() > 0:
            valid_conf = conf_values[valid_mask]
            valid_corrects = is_corrects[valid_mask]
            
            if len(valid_conf) > 1 and len(np.unique(valid_conf)) > 1:
                pearson_corr, pearson_p = pearsonr(valid_conf, valid_corrects)
                spearman_corr, spearman_p = spearmanr(valid_conf, valid_corrects)
                
                analysis_results["correlations"][f"{metric}_vs_accuracy"] = {
                    "pearson": {
                        "correlation": float(pearson_corr),
                        "p_value": float(pearson_p)
                    },
                    "spearman": {
                        "correlation": float(spearman_corr),
                        "p_value": float(spearman_p)
                    }
                }
    
    # ê° Confidenceì™€ í† í° ìˆ˜ ìƒê´€ê´€ê³„
    for metric in confidence_metrics:
        conf_values = confidence_dict[metric]
        valid_mask = ~np.isnan(conf_values) & ~np.isnan(token_counts)
        
        if valid_mask.sum() > 0:
            valid_conf = conf_values[valid_mask]
            valid_tokens = token_counts[valid_mask]
            
            if len(valid_conf) > 1 and len(np.unique(valid_conf)) > 1:
                pearson_corr, pearson_p = pearsonr(valid_conf, valid_tokens)
                spearman_corr, spearman_p = spearmanr(valid_conf, valid_tokens)
                
                analysis_results["correlations"][f"{metric}_vs_token_count"] = {
                    "pearson": {
                        "correlation": float(pearson_corr),
                        "p_value": float(pearson_p)
                    },
                    "spearman": {
                        "correlation": float(spearman_corr),
                        "p_value": float(spearman_p)
                    }
                }
    
    # êµ¬ê°„ë³„ ì •ë‹µë¥  ë¶„ì„ (í† í° ìˆ˜ ê¸°ì¤€) - í•„í„°ë§ëœ ë°ì´í„°ë§Œ ì‚¬ìš©
    interval_analysis = []
    if len(token_counts) > 0:
        logger.info(f"\n=== {method_name.upper()} êµ¬ê°„ë³„ ì •ë‹µë¥  ë¶„ì„ (í† í° ìˆ˜ ê¸°ì¤€, max_tokens ë„ë‹¬ instance ì œì™¸) ===")
        percentiles = [0, 25, 50, 75, 90, 100]
        percentile_values = [np.percentile(token_counts, p) for p in percentiles]
        
        for i in range(len(percentiles) - 1):
            pct_start = percentiles[i]
            pct_end = percentiles[i+1]
            val_start = percentile_values[i]
            val_end = percentile_values[i+1] if i+1 < len(percentile_values) else np.inf
            
            if i < len(percentiles) - 2:
                mask = (token_counts >= val_start) & (token_counts < val_end)
            else:
                mask = (token_counts >= val_start) & (token_counts <= val_end)
            
            if mask.sum() > 0:
                interval_accuracy = is_corrects[mask].mean()
                interval_count = mask.sum()
                interval_analysis.append({
                    "percentile_range": f"{pct_start}-{pct_end}%",
                    "token_range": f"{val_start:.0f}-{val_end:.0f}",
                    "accuracy": float(interval_accuracy),
                    "count": int(interval_count)
                })
                logger.info(f"  {pct_start}%-{pct_end}% êµ¬ê°„ ({val_start:.0f} ~ {val_end:.0f} í† í°): ì •ë‹µë¥  {interval_accuracy:.3f} ({interval_count}ê°œ)")
    
    analysis_results["interval_analysis"] = interval_analysis
    
    # ë¡œê·¸ ì¶œë ¥
    logger.info(f"\n=== {method_name.upper()} ì „ì²´ í†µê³„ ===")
    logger.info(f"ì´ solution ìˆ˜: {len(all_data)}")
    logger.info(f"Max tokens ë„ë‹¬ instance ì œì™¸ í›„ solution ìˆ˜: {len(filtered_data)}")
    if len(token_counts) > 0:
        logger.info(f"í‰ê·  í† í° ìˆ˜ (í•„í„°ë§ í›„): {np.mean(token_counts):.2f}")
        logger.info(f"ì¤‘ì•™ê°’ í† í° ìˆ˜ (í•„í„°ë§ í›„): {np.median(token_counts):.2f}")
        logger.info(f"ì „ì²´ ì •ë‹µë¥  (í•„í„°ë§ í›„): {np.mean(is_corrects):.3f}")
    
    logger.info(f"\n=== {method_name.upper()} ìƒê´€ê´€ê³„ ìš”ì•½ ===")
    for key, value in analysis_results["correlations"].items():
        if "pearson" in value:
            logger.info(f"  {key}:")
            logger.info(f"    í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜: {value['pearson']['correlation']:.4f} (p-value: {value['pearson']['p_value']:.4e})")
            logger.info(f"    ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜: {value['spearman']['correlation']:.4f} (p-value: {value['spearman']['p_value']:.4e})")
    
    return analysis_results


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 4-2 ì¶”ê°€ ë¶„ì„: í† í° ìˆ˜, ì •í™•ë„, Confidence ìƒê´€ê´€ê³„ ë¶„ì„"""
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage4_2_analyze_token_confidence_correlation.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    logger.info("ğŸš€ Stage 4-2 ì¶”ê°€ ë¶„ì„: í† í° ìˆ˜, ì •í™•ë„, Confidence ìƒê´€ê´€ê³„ ë¶„ì„ ì‹œì‘")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    results_dir = os.path.join(cfg.paths.output_dir, "comprehensive_results")
    results_dir = os.path.join(results_dir, "Qwen_Qwen3-4B-Instruct-2507")
    
    # Tokenizer ì´ˆê¸°í™”
    model_name = cfg.model.base_model
    logger.info(f"Tokenizer ë¡œë“œ: {model_name}")
    tokenizer = load_tokenizer(model_name)
    
    # Math Verifier ì´ˆê¸°í™”
    math_verifier = MathVerifier(
        timeout=cfg.evaluation.benchmarks.evaluation.timeout
    )
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì„¤ì •
    benchmark_datasets = [
        {"name": "AIME24", "path": "math-ai/aime24"},
        {"name": "AIME25", "path": "math-ai/aime25"},
        {"name": "HMMT24", "path": "MathArena/hmmt_feb_2024"},
        {"name": "HMMT25", "path": "MathArena/hmmt_feb_2025"},
    ]
    
    all_results = {}
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¶„ì„
    for benchmark in benchmark_datasets:
        dataset_name = benchmark["name"]
        dataset_path = benchmark["path"]
        dataset_safe_name = dataset_path.replace('/', '_')
        
        logger.info("=" * 60)
        logger.info(f"ë°ì´í„°ì…‹ ë¶„ì„: {dataset_name}")
        logger.info("=" * 60)
        
        # Baseline ê²½ë¡œ
        baseline_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_baseline_generated.json"
        )
        
        # AggLLM ê²½ë¡œ
        aggllm_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_aggllm_generated.json"
        )
        
        # Max tokens ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        max_tokens = cfg.evaluation.benchmarks.evaluation.max_tokens
        
        # ë¶„ì„ ìˆ˜í–‰
        dataset_results = analyze_token_confidence_correlation(
            baseline_path,
            aggllm_path,
            tokenizer,
            math_verifier,
            results_dir,
            max_tokens
        )
        
        all_results[dataset_name] = dataset_results
    
    # ì „ì²´ ìš”ì•½ ì €ì¥
    summary_path = os.path.join(results_dir, "token_confidence_correlation_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ì „ì²´ ìš”ì•½ ì €ì¥: {summary_path}")
    logger.info("\nâœ… Stage 4-2 ì¶”ê°€ ë¶„ì„ ì™„ë£Œ")


if __name__ == "__main__":
    main()

