"""
Stage 4-2: ì €ì¥ëœ Solution ê²°ê³¼ë¡œ ë©”íŠ¸ë¦­ í‰ê°€
Pass@k, Majority Voting, Confidence Weighted Voting ê³„ì‚°
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import json
from collections import defaultdict
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.evaluation.math_verifier import MathVerifier
from src.utils.logging import setup_logging

logger = logging.getLogger(__name__)


def calculate_pass_at_k(solutions: list, ground_truth: str, k_values: list, math_verifier: MathVerifier) -> dict:
    """
    Pass@k ë©”íŠ¸ë¦­ ê³„ì‚°
    
    Pass@kì˜ í‘œì¤€ ì •ì˜: kê°œì˜ solution ì¤‘ í•˜ë‚˜ë¼ë„ ì •ë‹µì´ë©´ 1.0, ì•„ë‹ˆë©´ 0.0
    - Pass@1~16: ê° kì— ëŒ€í•´ ì²˜ìŒ kê°œ solution ì¤‘ í•˜ë‚˜ë¼ë„ ì •ë‹µì¸ì§€ í™•ì¸
    """
    results = {}
    
    for k in k_values:
        # í‘œì¤€ Pass@k ì •ì˜: kê°œ ì¤‘ í•˜ë‚˜ë¼ë„ ì •ë‹µì´ë©´ í†µê³¼
        is_correct = False
        for sol in solutions[:k]:
            if math_verifier.verify_answer(sol["final_answer"], ground_truth):
                is_correct = True
                break
        results[k] = 1.0 if is_correct else 0.0
    
    return results


def majority_voting(solutions: list, samples_per_set: int, math_verifier: MathVerifier, ground_truth: str) -> dict:
    """
    Majority Voting ìˆ˜í–‰
    
    16ê°œ solutionì„ samples_per_setê°œì”© ë‚˜ëˆ ì„œ ê° setì— ëŒ€í•´ majority voting ìˆ˜í–‰
    """
    total_solutions = len(solutions)
    num_sets = total_solutions // samples_per_set
    correct_count = 0
    
    for i in range(num_sets):
        start_idx = i * samples_per_set
        end_idx = start_idx + samples_per_set
        set_solutions = solutions[start_idx:end_idx]
        
        # ê° sampleì˜ final_answer ì¶”ì¶œ
        answers = [sol["final_answer"] for sol in set_solutions if sol.get("final_answer")]
        
        if not answers:
            continue
        
        # ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹µì•ˆ ì„ íƒ
        answer_counts = defaultdict(int)
        for answer in answers:
            answer_counts[answer] += 1
        
        if answer_counts:
            majority_answer = max(answer_counts.items(), key=lambda x: x[1])[0]
            if math_verifier.verify_answer(majority_answer, ground_truth):
                correct_count += 1
    
    return {
        "correct_count": correct_count,
        "total_sets": num_sets,
        "accuracy": correct_count / num_sets if num_sets > 0 else 0.0
    }


def confidence_weighted_voting(
    solutions: list, 
    samples_per_set: int, 
    confidence_metric: str,
    math_verifier: MathVerifier,
    ground_truth: str
) -> dict:
    """
    Confidence Weighted Voting ìˆ˜í–‰
    
    16ê°œ solutionì„ samples_per_setê°œì”© ë‚˜ëˆ ì„œ ê° setì— ëŒ€í•´ confidence weighted voting ìˆ˜í–‰
    """
    total_solutions = len(solutions)
    num_sets = total_solutions // samples_per_set
    correct_count = 0
    
    for i in range(num_sets):
        start_idx = i * samples_per_set
        end_idx = start_idx + samples_per_set
        set_solutions = solutions[start_idx:end_idx]
        
        # ê° sampleì˜ final_answerì™€ confidence ì¶”ì¶œ
        weighted_votes = defaultdict(float)
        for sol in set_solutions:
            answer = sol.get("final_answer")
            if not answer:
                continue
            conf = sol.get("confidence_scores", {}).get(confidence_metric, 0.0)
            weighted_votes[answer] += conf
        
        if weighted_votes:
            best_answer = max(weighted_votes.items(), key=lambda x: x[1])[0]
            if math_verifier.verify_answer(best_answer, ground_truth):
                correct_count += 1
    
    return {
        "correct_count": correct_count,
        "total_sets": num_sets,
        "accuracy": correct_count / num_sets if num_sets > 0 else 0.0
    }


def evaluate_solutions(
    solutions: list,
    ground_truth: str,
    math_verifier: MathVerifier
) -> dict:
    """Solution ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°"""
    results = {}
    
    # Pass@k ê³„ì‚°
    pass_at_k = calculate_pass_at_k(solutions, ground_truth, [1] + list(range(2, 16)) + [16], math_verifier)
    results["pass_at_k"] = pass_at_k
    
    # Majority Voting
    majority_results = {}
    for samples_per_set in list(range(2, 17)):
        voting_result = majority_voting(solutions, samples_per_set, math_verifier, ground_truth)
        majority_results[f"{samples_per_set}_samples"] = voting_result
    results["majority_voting"] = majority_results
    
    # Confidence Weighted Voting
    confidence_metrics = [
        "bottom_10_percent_confidence",
        "tail_confidence",
        "mean_group_confidence",
        "lowest_group_confidence"
    ]
    
    confidence_results = {}
    for metric in confidence_metrics:
        metric_results = {}
        for samples_per_set in list(range(2, 17)):
            voting_result = confidence_weighted_voting(
                solutions,
                samples_per_set,
                metric,
                math_verifier,
                ground_truth
            )
            metric_results[f"{samples_per_set}_samples"] = voting_result
        confidence_results[metric] = metric_results
    results["confidence_weighted_voting"] = confidence_results
    
    return results


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 4-2: ë©”íŠ¸ë¦­ í‰ê°€ ë©”ì¸ í•¨ìˆ˜"""
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage4_2_evaluate_metrics.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    logger.info("ğŸš€ Stage 4-2: ë©”íŠ¸ë¦­ í‰ê°€ ì‹œì‘")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    results_dir = os.path.join(cfg.paths.output_dir, "comprehensive_results")
    
    # results_dir = os.path.join(results_dir, "think" if cfg.evaluation.benchmarks.evaluation.get("enable_thinking", False) else "no_think")
    results_dir = os.path.join(results_dir, "think") 
    
    # Math Verifier ì´ˆê¸°í™”
    math_verifier = MathVerifier(
        timeout=cfg.evaluation.benchmarks.evaluation.timeout
    )
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì„¤ì •
    benchmark_datasets = [
        {"name": "AIME24", "path": "math-ai/aime24"},
        {"name": "AIME25", "path": "math-ai/aime25"},
        {"name": "HMMT24", "path": "MathArena/hmmt_feb_2024"},
        {"name": "HMMT25", "path": "MathArena/hmmt_feb_2025"},
    ]
    
    all_results = {}
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ í‰ê°€
    for benchmark in benchmark_datasets:
        dataset_name = benchmark["name"]
        dataset_path = benchmark["path"]
        dataset_safe_name = dataset_path.replace('/', '_')
        
        logger.info("=" * 60)
        logger.info(f"ë°ì´í„°ì…‹ í‰ê°€: {dataset_name}")
        logger.info("=" * 60)
        
        results = {
            "dataset_name": dataset_path,
            "total_problems": 0
        }
        
        # Baseline í‰ê°€
        baseline_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_baseline_generated.json"
        )
        
        if os.path.exists(baseline_path):
            logger.info(f"Baseline ê²°ê³¼ ë¡œë“œ: {baseline_path}")
            with open(baseline_path, 'r', encoding='utf-8') as f:
                baseline_data = json.load(f)
            
            baseline_metrics = defaultdict(list)
            
            for problem_data in baseline_data["generated_solutions"]:
                solutions = problem_data["solutions"]
                ground_truth = problem_data["ground_truth"]
                
                problem_results = evaluate_solutions(solutions, ground_truth, math_verifier)
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                for k, v in problem_results.get("pass_at_k", {}).items():
                    baseline_metrics[f"pass_at_{k}"].append(v)
                
                for key, value in problem_results.get("majority_voting", {}).items():
                    baseline_metrics[f"majority_voting_{key}"].append(value["accuracy"])
                
                for metric, metric_results in problem_results.get("confidence_weighted_voting", {}).items():
                    for key, value in metric_results.items():
                        baseline_metrics[f"confidence_weighted_{metric}_{key}"].append(value["accuracy"])
            
            # í‰ê·  ê³„ì‚°
            baseline_final = {}
            for key, values in baseline_metrics.items():
                baseline_final[key] = np.mean(values) if values else 0.0
            
            results["baseline"] = baseline_final
            results["total_problems"] = len(baseline_data["generated_solutions"])
            logger.info(f"Baseline í‰ê°€ ì™„ë£Œ: {len(baseline_data['generated_solutions'])}ê°œ ë¬¸ì œ")
        else:
            logger.warning(f"Baseline ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {baseline_path}")
        
        # AggLLM í‰ê°€
        aggllm_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_aggllm_generated.json"
        )
        
        if os.path.exists(aggllm_path):
            logger.info(f"AggLLM ê²°ê³¼ ë¡œë“œ: {aggllm_path}")
            with open(aggllm_path, 'r', encoding='utf-8') as f:
                aggllm_data = json.load(f)
            
            aggllm_metrics = defaultdict(list)
            
            for problem_data in aggllm_data["generated_solutions"]:
                solutions = problem_data["solutions"]
                ground_truth = problem_data["ground_truth"]
                
                problem_results = evaluate_solutions(solutions, ground_truth, math_verifier)
                
                # ë©”íŠ¸ë¦­ ëˆ„ì 
                for k, v in problem_results.get("pass_at_k", {}).items():
                    aggllm_metrics[f"pass_at_{k}"].append(v)
                
                for key, value in problem_results.get("majority_voting", {}).items():
                    aggllm_metrics[f"majority_voting_{key}"].append(value["accuracy"])
                
                for metric, metric_results in problem_results.get("confidence_weighted_voting", {}).items():
                    for key, value in metric_results.items():
                        aggllm_metrics[f"confidence_weighted_{metric}_{key}"].append(value["accuracy"])
            
            # í‰ê·  ê³„ì‚°
            aggllm_final = {}
            for key, values in aggllm_metrics.items():
                aggllm_final[key] = np.mean(values) if values else 0.0
            
            results["aggllm"] = aggllm_final
            logger.info(f"AggLLM í‰ê°€ ì™„ë£Œ: {len(aggllm_data['generated_solutions'])}ê°œ ë¬¸ì œ")
        else:
            logger.warning(f"AggLLM ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {aggllm_path}")
        
        # ê²°ê³¼ ì €ì¥
        metrics_path = os.path.join(results_dir, f"{dataset_safe_name}_metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ì¥: {metrics_path}")
        all_results[dataset_name] = results
    
    # ì „ì²´ ìš”ì•½
    logger.info("=" * 60)
    logger.info("ì „ì²´ ë©”íŠ¸ë¦­ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 60)
    
    for dataset_name, results in all_results.items():
        logger.info(f"\n{dataset_name}:")
        if "baseline" in results:
            logger.info("  Baseline:")
            for k in [1] + list(range(2, 17)):
                logger.info(f"    Pass@{k}: {results['baseline'].get(f'pass_at_{k}', 0.0):.3f}")
        if "aggllm" in results:
            logger.info("  AggLLM:")
            for k in [1] + list(range(2, 17)):
                logger.info(f"    Pass@{k}: {results['aggllm'].get(f'pass_at_{k}', 0.0):.3f}")
    
    logger.info("\nâœ… Stage 4-2: ë©”íŠ¸ë¦­ í‰ê°€ ì™„ë£Œ")


if __name__ == "__main__":
    main()


