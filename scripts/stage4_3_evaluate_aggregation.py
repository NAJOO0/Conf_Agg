"""
Stage 4-3: Aggregation í‰ê°€
ì €ì¥ëœ Solution ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Prompt Aggregationê³¼ AggLLM Aggregation ìˆ˜í–‰
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from vllm import LLM, SamplingParams
import tempfile
from typing import Tuple, List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.evaluation.math_verifier import MathVerifier
from src.utils.logging import setup_logging

logger = logging.getLogger(__name__)


def filter_solutions_by_token_length(
    solutions: list,
    tokenizer: AutoTokenizer,
    max_tokens: int = 2000
) -> Tuple[list, list]:
    """
    contentê°€ max_tokens ì´ìƒì¸ solution í•„í„°ë§
    
    Args:
        solutions: solution ë¦¬ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì €
        max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ 2000)
    
    Returns:
        (í•„í„°ë§ëœ solution ë¦¬ìŠ¤íŠ¸, í† í° ìˆ˜ ë¦¬ìŠ¤íŠ¸)
    """
    filtered = []
    token_counts = []
    for sol in solutions:
        content = sol.get("content", "")
        if not content:
            continue
        
        # í† í° ìˆ˜ ê³„ì‚°
        tokens = tokenizer.encode(content, add_special_tokens=False)
        token_count = len(tokens)
        token_counts.append(token_count)
        
        if token_count < max_tokens:
            filtered.append(sol)
        else:
            logger.debug(f"Solution í•„í„°ë§ë¨: {token_count} í† í° (ìµœëŒ€: {max_tokens})")
    
    return filtered, token_counts


def format_solutions_for_aggregation(solutions: list, include_confidence: bool = True) -> str:
    """Aggregation í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•œ solution í…ìŠ¤íŠ¸ ìƒì„±"""
    lines = []
    for idx, sol in enumerate(solutions, start=1):
        solution_content = sol.get("content", "")
        lines.append(
            f"solution{idx}:\n"
            f"{solution_content}\n"
            f"final_answer: {sol['final_answer']}\n"
        )
        if include_confidence:
            conf_value = sol["confidence_scores"].get("tail_confidence", None)
            conf_str = f"{conf_value:.4f}" if conf_value is not None else "N/A"
            lines[-1] += f"confidence: {conf_str}\n"
    return "\n".join(lines)


def extract_content(text: str) -> str:
    """
    </think> í† í° ì´í›„ ê°’ë“¤ ì¶”ì¶œ
    
    Args:
        text: generated_text
        
    Returns:
        </think> ì´í›„ ë‚´ìš©
    """
    if not text:
        return ""
    
    text_str = str(text)
    marker = "</think>"
    
    marker_pos = text_str.find(marker)
    if marker_pos == -1:
        return text_str.strip()
    
    # ë§ˆì»¤ ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    content = text_str[marker_pos + len(marker):].strip()
    return content


def prepare_aggregation_prompts_batch(
    tokenizer: AutoTokenizer,
    aggregation_requests: List[Dict[str, Any]],
    aggregation_prompt_template: str,
    enable_thinking: bool,
    include_confidence: bool = True
) -> Tuple[List[str], List[str]]:
    """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ aggregation í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    
    Returns:
        (formatted_prompts, prompt_texts) - formatted_promptsëŠ” í† í¬ë‚˜ì´ì € ì ìš©ëœ ê²ƒ, prompt_textsëŠ” ì›ë³¸ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
    """
    formatted_prompts = []
    prompt_texts = []
    for req in aggregation_requests:
        problem_text = req["problem_text"]
        solutions = req["solutions"]
        
        # Solution í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        solutions_text = format_solutions_for_aggregation(solutions, include_confidence=include_confidence)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = aggregation_prompt_template.format(
            problem=problem_text,
            solutions=solutions_text
        )
        prompt_texts.append(prompt)
        
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=enable_thinking,
        )
        formatted_prompts.append(formatted_prompt)
    
    return formatted_prompts, prompt_texts


def count_correct_solutions(solutions: list, ground_truth: str, math_verifier: MathVerifier) -> int:
    """ì£¼ì–´ì§„ solutions ì¤‘ ì •ë‹µì¸ ê²ƒì˜ ê°œìˆ˜ ê³„ì‚°"""
    correct_count = 0
    for sol in solutions:
        final_answer = sol.get("final_answer", "")
        if final_answer and math_verifier.verify_answer(final_answer, ground_truth):
            correct_count += 1
    return correct_count


def group_solutions_for_aggregation(
    filtered_solutions: list,
    target_group_size: int = 4,
    max_groups: int = None
) -> List[list]:
    """
    í•„í„°ë§ëœ solutionsë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°
    
    Args:
        filtered_solutions: í•„í„°ë§ëœ solution ë¦¬ìŠ¤íŠ¸
        target_group_size: ê° ê·¸ë£¹ì˜ í¬ê¸°
        max_groups: ìµœëŒ€ ê·¸ë£¹ ìˆ˜ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
    
    Returns:
        ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸ (ê° ê·¸ë£¹ì€ solution ë¦¬ìŠ¤íŠ¸)
    """
    groups = []
    for i in range(0, len(filtered_solutions), target_group_size):
        if max_groups and len(groups) >= max_groups:
            break
        group = filtered_solutions[i:i+target_group_size]
        # target_group_sizeë³´ë‹¤ ì ìœ¼ë©´ ì œì™¸
        if len(group) == target_group_size:
            groups.append(group)
    return groups


def group_results_by_problem_id(results: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ê²°ê³¼ë¥¼ problem_idë³„ë¡œ ê·¸ë£¹í™”
    
    Args:
        results: aggregation ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        problem_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    grouped = {}
    for result in results:
        problem_id = result["problem_id"]
        if problem_id not in grouped:
            grouped[problem_id] = {
                "problem_id": problem_id,
                "problem_text": result["problem_text"],
                "ground_truth": result["ground_truth"],
                "prompts": []
            }
        
        # prompt ì •ë³´ ì¶”ê°€
        prompt_info = {
            "prompt_text": result.get("prompt_text", ""),
            "num_solutions": result.get("num_solutions", 0),
            "correct_solutions_count": result.get("correct_solutions_count", 0),
            "generated_text": result.get("generated_text", ""),
            "parsed_content": result.get("parsed_content", ""),
            "final_answer": result.get("final_answer", ""),
            "is_correct": result.get("is_correct", None)
        }
        grouped[problem_id]["prompts"].append(prompt_info)
    
    return grouped


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 4-3: Aggregation í‰ê°€ ë©”ì¸ í•¨ìˆ˜"""
    
    # GPU ì„¤ì • (CUDA_VISIBLE_DEVICESê°€ ì„¤ì •ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„)
    if torch.cuda.is_available():
        # CUDA_VISIBLE_DEVICESê°€ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ 0ë²ˆì´ ì‹¤ì œ GPU
        # ëª…ì‹œì ìœ¼ë¡œ GPUë¥¼ ì„¤ì •í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ê°„ ê²©ë¦¬ ë³´ì¥
        torch.cuda.set_device(0)
        torch.cuda.empty_cache()
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage4_3_evaluate_aggregation.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    if torch.cuda.is_available():
        logger.info(f"GPU ì„¤ì •: device=0, GPU={torch.cuda.get_device_name(0)}")
    
    logger.info("ğŸš€ Stage 4-3: Aggregation í‰ê°€ ì‹œì‘")
    
    # ì˜µì…˜ í™•ì¸
    eval_config = cfg.evaluation.benchmarks.evaluation
    do_analysis = eval_config.get("aggregation_do_analysis", True)
    do_generation = eval_config.get("aggregation_do_generation", True)
    do_evaluation = eval_config.get("aggregation_do_evaluation", True)
    
    if not do_analysis and not do_generation and not do_evaluation:
        logger.error("analysis, generation, evaluation ì¤‘ í•˜ë‚˜ëŠ” Trueì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return
    
    logger.info(f"ì˜µì…˜: analysis={do_analysis}, generation={do_generation}, evaluation={do_evaluation}")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    results_dir = os.path.join(cfg.paths.output_dir, "comprehensive_results")
    results_dir = os.path.join(results_dir, "think" if eval_config.get("enable_thinking", False) else "no_think")
    # results_dir = os.path.join(results_dir, "no_think")
    logger.info(f"results_dir: {results_dir}")
    # Math Verifier ì´ˆê¸°í™”
    math_verifier = MathVerifier(
        timeout=eval_config.timeout
    )
    
    # Aggregation í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    aggregation_prompt_template = (
        "Given the following problem:\n{problem}\n"
        "and these solution attempts:\n{solutions}\n"
        "It is possible that any, all, or none of these solutions are correct or complete. Carefully review the\n"
        "provided solutions, using them as starting pointsâ€”correcting mistakes, filling in gaps, and/or combining\n"
        "useful ideasâ€”to produce a final, comprehensive, and correct solution to the problem."
    )
    
    base_instruction = "Please reason step by step, and put your final answer within \\boxed{}."
    
    # ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ì„¤ì •
    benchmark_datasets = [
        {"name": "AIME24", "path": "math-ai/aime24"},
        {"name": "AIME25", "path": "math-ai/aime25"},
        {"name": "HMMT24", "path": "MathArena/hmmt_feb_2024"},
        {"name": "HMMT25", "path": "MathArena/hmmt_feb_2025"},
    ]
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    checkpoint_num = eval_config.checkpoint_num
    if checkpoint_num is not None:
        aggllm_model_path = os.path.join(cfg.paths.model_dir, f"checkpoint-{checkpoint_num}")
    else:
        aggllm_model_path = os.path.join(cfg.paths.model_dir, "checkpoint-final")
    
    if not os.path.exists(aggllm_model_path):
        logger.warning(f"AggLLM ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {aggllm_model_path}")
        aggllm_model_path = None
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ í‰ê°€
    for benchmark in benchmark_datasets:
        dataset_name = benchmark["name"]
        dataset_path = benchmark["path"]
        dataset_safe_name = dataset_path.replace('/', '_')
        
        logger.info("=" * 60)
        logger.info(f"ë°ì´í„°ì…‹: {dataset_name}")
        logger.info("=" * 60)
        
        # ê·¸ë£¹ í¬ê¸°ë³„ë¡œ ì‹¤í—˜ (4, 2, 1)
        group_sizes = [4, 2, 1]
        all_group_results = {}  # {group_size: aggregation_results}
        
        # Baseline ê²°ê³¼ ë¡œë“œ
        baseline_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_baseline_generated.json"
        )
        
        baseline_data = None
        if os.path.exists(baseline_path):
            with open(baseline_path, 'r', encoding='utf-8') as f:
                baseline_data = json.load(f)
            logger.info(f"Baseline ê²°ê³¼ ë¡œë“œ: {len(baseline_data['generated_solutions'])}ê°œ ë¬¸ì œ")
        else:
            logger.warning(f"Baseline ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {baseline_path}")
        
        # AggLLM ê²°ê³¼ ë¡œë“œ
        aggllm_path = os.path.join(
            results_dir,
            f"{dataset_safe_name}_aggllm_generated.json"
        )
        
        aggllm_data = None
        if os.path.exists(aggllm_path):
            with open(aggllm_path, 'r', encoding='utf-8') as f:
                aggllm_data = json.load(f)
            logger.info(f"AggLLM ê²°ê³¼ ë¡œë“œ: {len(aggllm_data['generated_solutions'])}ê°œ ë¬¸ì œ")
        else:
            logger.warning(f"AggLLM ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {aggllm_path}")
        
        if not baseline_data and not aggllm_data:
            logger.warning(f"{dataset_name}ì— ëŒ€í•œ ê²°ê³¼ íŒŒì¼ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        # Analysis ë‹¨ê³„: í† í° ìˆ˜ ë¶„í¬ ë¶„ì„
        if do_analysis:
            logger.info("=" * 60)
            logger.info("Analysis ë‹¨ê³„ ì‹œì‘: í† í° ìˆ˜ ë¶„í¬ ë¶„ì„")
            logger.info("=" * 60)
            
            analysis_results = {
                "dataset_name": dataset_path,
                "baseline_token_distribution": {},
                "aggllm_token_distribution": {}
            }
            
            # Baseline í† í° ìˆ˜ ë¶„ì„
            if baseline_data:
                logger.info("Baseline í† í° ìˆ˜ ë¶„ì„ ì¤‘...")
                baseline_tokenizer = AutoTokenizer.from_pretrained(
                    cfg.model.base_model,
                    trust_remote_code=True
                )
                if baseline_tokenizer.pad_token is None:
                    baseline_tokenizer.pad_token = baseline_tokenizer.eos_token
                
                instance_token_counts = []
                for problem_data in baseline_data["generated_solutions"]:
                    solutions = problem_data["solutions"]
                    problem_id = problem_data["problem_id"]
                    
                    problem_token_counts = []
                    for sol in solutions:
                        content = sol.get("content", "")
                        if content:
                            tokens = baseline_tokenizer.encode(content, add_special_tokens=False)
                            token_count = len(tokens)
                            problem_token_counts.append(token_count)
                    
                    # 32000ì„ ë„˜ëŠ” í† í° ì œê±°
                    filtered_token_counts = [t for t in problem_token_counts if t <= 32000]
                    total_instance_tokens = sum(filtered_token_counts)
                    instance_token_counts.append({
                        "problem_id": problem_id,
                        "token_counts": filtered_token_counts,
                        "total_solutions": len(solutions),
                        "filtered_solutions": len(filtered_token_counts),
                        "total_instance_tokens": total_instance_tokens,
                        "avg_tokens": sum(filtered_token_counts) / len(filtered_token_counts) if filtered_token_counts else 0,
                        "max_tokens": max(filtered_token_counts) if filtered_token_counts else 0,
                        "min_tokens": min(filtered_token_counts) if filtered_token_counts else 0
                    })
                
                # 32000 í† í°ì„ ë„˜ëŠ” instanceëŠ” í‰ê·  ê³„ì‚°ì—ì„œ ì œì™¸
                filtered_instances = [inst for inst in instance_token_counts if inst.get("total_instance_tokens", 0) <= 32000]
                # ê° solutionì˜ í† í° ìˆ˜ê°€ 32000ì„ ë„˜ëŠ” ê²½ìš°ë„ ì œì™¸
                all_tokens = [t for inst in filtered_instances for t in inst["token_counts"] if t <= 32000]
                
                # ê²€ì¦: all_tokensì— 32000ì„ ë„˜ëŠ” ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                if all_tokens:
                    max_token_value = max(all_tokens)
                    if max_token_value > 32000:
                        logger.warning(f"ê²½ê³ : all_tokensì— 32000ì„ ë„˜ëŠ” ê°’ì´ ìˆìŠµë‹ˆë‹¤: {max_token_value}")
                        # 32000ì„ ë„˜ëŠ” ê°’ ì œê±°
                        all_tokens = [t for t in all_tokens if t <= 32000]
                
                analysis_results["baseline_token_distribution"] = {
                    "instance_level": instance_token_counts,
                    "dataset_level": {
                        "total_instances": len(instance_token_counts),
                        "filtered_instances": len(filtered_instances),
                        "excluded_instances": len(instance_token_counts) - len(filtered_instances),
                        "total_solutions": len(all_tokens),
                        "avg_tokens": sum(all_tokens) / len(all_tokens) if all_tokens else 0,
                        "max_tokens": max(all_tokens) if all_tokens else 0,
                        "min_tokens": min(all_tokens) if all_tokens else 0,
                        "median_tokens": sorted(all_tokens)[len(all_tokens)//2] if all_tokens else 0
                    }
                }
                
                # ìµœì¢… ê²€ì¦
                if all_tokens and analysis_results["baseline_token_distribution"]["dataset_level"]["max_tokens"] > 32000:
                    logger.error(f"ì˜¤ë¥˜: dataset_level max_tokensê°€ 32000ì„ ë„˜ìŠµë‹ˆë‹¤: {analysis_results['baseline_token_distribution']['dataset_level']['max_tokens']}")
                    analysis_results["baseline_token_distribution"]["dataset_level"]["max_tokens"] = max([t for t in all_tokens if t <= 32000]) if all_tokens else 0
                logger.info(f"Baseline ë¶„ì„ ì™„ë£Œ: {len(instance_token_counts)}ê°œ ì¸ìŠ¤í„´ìŠ¤, í‰ê·  {analysis_results['baseline_token_distribution']['dataset_level']['avg_tokens']:.1f} í† í°")
            
            # AggLLM í† í° ìˆ˜ ë¶„ì„
            if aggllm_data:
                logger.info("AggLLM í† í° ìˆ˜ ë¶„ì„ ì¤‘...")
                aggllm_tokenizer = AutoTokenizer.from_pretrained(
                    cfg.model.base_model,
                    trust_remote_code=True
                )
                if aggllm_tokenizer.pad_token is None:
                    aggllm_tokenizer.pad_token = aggllm_tokenizer.eos_token
                
                instance_token_counts = []
                for problem_data in aggllm_data["generated_solutions"]:
                    solutions = problem_data["solutions"]
                    problem_id = problem_data["problem_id"]
                    
                    problem_token_counts = []
                    for sol in solutions:
                        content = sol.get("content", "")
                        if content:
                            tokens = aggllm_tokenizer.encode(content, add_special_tokens=False)
                            token_count = len(tokens)
                            problem_token_counts.append(token_count)
                    
                    # 32000ì„ ë„˜ëŠ” í† í° ì œê±°
                    filtered_token_counts = [t for t in problem_token_counts if t <= 32000]
                    total_instance_tokens = sum(filtered_token_counts)
                    instance_token_counts.append({
                        "problem_id": problem_id,
                        "token_counts": filtered_token_counts,
                        "total_solutions": len(solutions),
                        "filtered_solutions": len(filtered_token_counts),
                        "total_instance_tokens": total_instance_tokens,
                        "avg_tokens": sum(filtered_token_counts) / len(filtered_token_counts) if filtered_token_counts else 0,
                        "max_tokens": max(filtered_token_counts) if filtered_token_counts else 0,
                        "min_tokens": min(filtered_token_counts) if filtered_token_counts else 0
                    })
                
                # 32000 í† í°ì„ ë„˜ëŠ” instanceëŠ” í‰ê·  ê³„ì‚°ì—ì„œ ì œì™¸
                filtered_instances = [inst for inst in instance_token_counts if inst.get("total_instance_tokens", 0) <= 32000]
                # ê° solutionì˜ í† í° ìˆ˜ê°€ 32000ì„ ë„˜ëŠ” ê²½ìš°ë„ ì œì™¸
                all_tokens = [t for inst in filtered_instances for t in inst["token_counts"] if t <= 32000]
                
                # ê²€ì¦: all_tokensì— 32000ì„ ë„˜ëŠ” ê°’ì´ ìˆëŠ”ì§€ í™•ì¸
                if all_tokens:
                    max_token_value = max(all_tokens)
                    if max_token_value > 32000:
                        logger.warning(f"ê²½ê³ : all_tokensì— 32000ì„ ë„˜ëŠ” ê°’ì´ ìˆìŠµë‹ˆë‹¤: {max_token_value}")
                        # 32000ì„ ë„˜ëŠ” ê°’ ì œê±°
                        all_tokens = [t for t in all_tokens if t <= 32000]
                
                analysis_results["aggllm_token_distribution"] = {
                    "instance_level": instance_token_counts,
                    "dataset_level": {
                        "total_instances": len(instance_token_counts),
                        "filtered_instances": len(filtered_instances),
                        "excluded_instances": len(instance_token_counts) - len(filtered_instances),
                        "total_solutions": len(all_tokens),
                        "avg_tokens": sum(all_tokens) / len(all_tokens) if all_tokens else 0,
                        "max_tokens": max(all_tokens) if all_tokens else 0,
                        "min_tokens": min(all_tokens) if all_tokens else 0,
                        "median_tokens": sorted(all_tokens)[len(all_tokens)//2] if all_tokens else 0
                    }
                }
                
                # ìµœì¢… ê²€ì¦
                if all_tokens and analysis_results["aggllm_token_distribution"]["dataset_level"]["max_tokens"] > 32000:
                    logger.error(f"ì˜¤ë¥˜: dataset_level max_tokensê°€ 32000ì„ ë„˜ìŠµë‹ˆë‹¤: {analysis_results['aggllm_token_distribution']['dataset_level']['max_tokens']}")
                    analysis_results["aggllm_token_distribution"]["dataset_level"]["max_tokens"] = max([t for t in all_tokens if t <= 32000]) if all_tokens else 0
                logger.info(f"AggLLM ë¶„ì„ ì™„ë£Œ: {len(instance_token_counts)}ê°œ ì¸ìŠ¤í„´ìŠ¤, í‰ê·  {analysis_results['aggllm_token_distribution']['dataset_level']['avg_tokens']:.1f} í† í°")
            
            # ë¶„ì„ ê²°ê³¼ ì €ì¥
            analysis_path = os.path.join(
                results_dir,
                f"{dataset_safe_name}_token_analysis.json"
            )
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2)
            logger.info(f"í† í° ë¶„ì„ ê²°ê³¼ ì €ì¥: {analysis_path}")
        
        # Generation ë‹¨ê³„ - ê° ê·¸ë£¹ í¬ê¸°ë³„ë¡œ ì‹¤í—˜
        if do_generation:
            enable_thinking = eval_config.get("enable_thinking", False)
            
            # Baseline ëª¨ë¸ ë¡œë“œ ë° Baseline ê´€ë ¨ inference ì‹¤í–‰
            if baseline_data:
                logger.info("=" * 60)
                logger.info("Baseline ëª¨ë¸ ë¡œë“œ ì¤‘...")
                logger.info("=" * 60)
                baseline_tokenizer = AutoTokenizer.from_pretrained(
                    cfg.model.base_model,
                    trust_remote_code=True
                )
                if baseline_tokenizer.pad_token is None:
                    baseline_tokenizer.pad_token = baseline_tokenizer.eos_token
                
                baseline_llm = LLM(
                    model=cfg.model.base_model,
                    tensor_parallel_size=1,
                    gpu_memory_utilization=eval_config.get("gpu_memory_utilization", 0.9),
                    max_model_len=eval_config.get("max_model_len", eval_config.max_tokens + 16384),
                    dtype="bfloat16",
                    trust_remote_code=True,
                )
                logger.info("Baseline ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
                # ê° ê·¸ë£¹ í¬ê¸°ë³„ë¡œ Baseline ê´€ë ¨ inference ì‹¤í–‰
                for group_size in group_sizes:
                    max_groups = 4  # ê° ê·¸ë£¹ í¬ê¸°ë³„ë¡œ ìµœëŒ€ 4ê°œ ê·¸ë£¹ë§Œ ì‚¬ìš©
                    
                    logger.info("=" * 60)
                    logger.info(f"Baseline ê´€ë ¨ inference - ê·¸ë£¹ í¬ê¸° {group_size}")
                    logger.info("=" * 60)
                    
                    # ì´ ê·¸ë£¹ í¬ê¸°ì— ëŒ€í•œ ê²°ê³¼ ì´ˆê¸°í™” (Baseline ê´€ë ¨ë§Œ)
                    aggregation_results = {
                        "dataset_name": dataset_path,
                        "baseline_to_baseline_aggregation": [],
                        "baseline_to_baseline_aggregation_without_confidence": [],
                        "baseline_to_aggllm_aggregation": [],
                        "baseline_to_aggllm_aggregation_without_confidence": [],
                        "aggllm_to_aggllm_aggregation": []
                    }
                    
                    # Baseline â†’ Baseline Aggregation
                    logger.info(f"Baseline â†’ Baseline Aggregation ìƒì„± ì¤‘ (ê·¸ë£¹ í¬ê¸°: {group_size})...")
                    
                    # ë¨¼ì € ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ í•„í„°ë§ ë° ê·¸ë£¹í™”
                    aggregation_requests = []
                    for problem_data in baseline_data["generated_solutions"]:
                        problem_text = problem_data["problem_text"]
                        solutions = problem_data["solutions"]
                        ground_truth = problem_data["ground_truth"]
                        problem_id = problem_data["problem_id"]
                        
                        # ì „ì²´ 16ê°œë¥¼ ë¨¼ì € í•„í„°ë§
                        filtered_solutions, _ = filter_solutions_by_token_length(
                            solutions,
                            baseline_tokenizer,
                            max_tokens=eval_config.filter_max_tokens
                        )
                        
                        # ê·¸ë£¹í™” (group_sizeê°œì”©, ìµœëŒ€ max_groupsê°œ)
                        solution_groups = group_solutions_for_aggregation(
                            filtered_solutions, 
                            target_group_size=group_size,
                            max_groups=max_groups
                        )
                        
                        # ê° ê·¸ë£¹ì— ëŒ€í•´ ìš”ì²­ ìƒì„±
                        for group in solution_groups:
                            correct_count = count_correct_solutions(group, ground_truth, math_verifier)
                            group_sizes_str = ",".join(str(len(g)) for g in solution_groups)
                            
                            aggregation_requests.append({
                                "problem_id": problem_id,
                                "problem_text": problem_text,
                                "ground_truth": ground_truth,
                                "solutions": group,
                                "correct_count": correct_count,
                                "group_sizes": group_sizes_str
                            })
                    
                    if aggregation_requests:
                            logger.info(f"ì´ {len(aggregation_requests)}ê°œ aggregation ìš”ì²­ ì¤€ë¹„ ì™„ë£Œ")
                            
                            # ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (confidence í¬í•¨)
                            formatted_prompts, prompt_texts = prepare_aggregation_prompts_batch(
                                baseline_tokenizer,
                                aggregation_requests,
                                aggregation_prompt_template,
                                enable_thinking,
                                include_confidence=True
                            )
                            
                            # SamplingParams ì„¤ì •
                            sampling_params = SamplingParams(
                                temperature=eval_config.temperature,
                                max_tokens=eval_config.max_tokens,
                                top_p=eval_config.get("top_p", 0.8),
                                top_k=eval_config.get("top_k", 20),
                                min_p=eval_config.get("min_p", 0.0),
                            )
                            
                            # ë°°ì¹˜ë¡œ ìƒì„±
                            logger.info("ë°°ì¹˜ ìƒì„± ì‹œì‘...")
                            outputs = baseline_llm.generate(formatted_prompts, sampling_params)
                            logger.info("ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for idx, output in enumerate(outputs):
                                req = aggregation_requests[idx]
                                agg_text = output.outputs[0].text
                                parsed_content = extract_content(agg_text)
                                agg_answer = math_verifier.extract_final_answer_from_content(parsed_content)
                                
                                aggregation_results["baseline_to_baseline_aggregation"].append({
                                    "problem_id": req["problem_id"],
                                    "problem_text": req["problem_text"],
                                    "ground_truth": req["ground_truth"],
                                    "prompt_text": prompt_texts[idx],
                                    "num_solutions": len(req["solutions"]),
                                    "correct_solutions_count": req["correct_count"],
                                    "generated_text": agg_text,
                                    "parsed_content": parsed_content,
                                    "final_answer": agg_answer,
                                    "is_correct": math_verifier.verify_answer(agg_answer, req["ground_truth"]) if do_evaluation else None,
                                    "solution_group_sizes": req["group_sizes"]
                                })
                            
                            # Baseline â†’ Baseline Aggregation (without confidence)
                            logger.info("Baseline â†’ Baseline Aggregation (without confidence) ìƒì„± ì¤‘...")
                            
                            # ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (confidence ì œì™¸)
                            formatted_prompts, prompt_texts = prepare_aggregation_prompts_batch(
                                baseline_tokenizer,
                                aggregation_requests,
                                aggregation_prompt_template,
                                enable_thinking,
                                include_confidence=False
                            )
                            
                            # ë°°ì¹˜ë¡œ ìƒì„±
                            logger.info("ë°°ì¹˜ ìƒì„± ì‹œì‘...")
                            outputs = baseline_llm.generate(formatted_prompts, sampling_params)
                            logger.info("ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for idx, output in enumerate(outputs):
                                req = aggregation_requests[idx]
                                agg_text = output.outputs[0].text
                                parsed_content = extract_content(agg_text)
                                agg_answer = math_verifier.extract_final_answer_from_content(parsed_content)
                                
                                aggregation_results["baseline_to_baseline_aggregation_without_confidence"].append({
                                    "problem_id": req["problem_id"],
                                    "problem_text": req["problem_text"],
                                    "ground_truth": req["ground_truth"],
                                    "prompt_text": prompt_texts[idx],
                                    "num_solutions": len(req["solutions"]),
                                    "correct_solutions_count": req["correct_count"],
                                    "generated_text": agg_text,
                                    "parsed_content": parsed_content,
                                    "final_answer": agg_answer,
                                    "is_correct": math_verifier.verify_answer(agg_answer, req["ground_truth"]) if do_evaluation else None,
                                    "solution_group_sizes": req["group_sizes"]
                                })
                    
                    # Baseline ê´€ë ¨ ê²°ê³¼ ì„ì‹œ ì €ì¥
                    group_results_dir = os.path.join(results_dir, f"group_size_{group_size}")
                    os.makedirs(group_results_dir, exist_ok=True)
                    aggregation_path = os.path.join(
                        group_results_dir,
                        f"{dataset_safe_name}_aggregation_results.json"
                    )
                    
                    # Baseline ê²°ê³¼ë§Œ ë¨¼ì € ì €ì¥
                    formatted_results = {
                        "dataset_name": dataset_path,
                        "group_size": group_size,
                        "baseline_to_baseline_aggregation": {},
                        "baseline_to_baseline_aggregation_without_confidence": {},
                        "baseline_to_aggllm_aggregation": {},
                        "baseline_to_aggllm_aggregation_without_confidence": {},
                        "aggllm_to_aggllm_aggregation": {}
                    }
                    
                    # Baseline ê²°ê³¼ ê·¸ë£¹í™” ë° ì €ì¥
                    for key in ["baseline_to_baseline_aggregation", "baseline_to_baseline_aggregation_without_confidence"]:
                        results = aggregation_results.get(key, [])
                        if results:
                            grouped = group_results_by_problem_id(results)
                            formatted_results[key] = grouped
                    
                    with open(aggregation_path, 'w', encoding='utf-8') as f:
                        json.dump(formatted_results, f, ensure_ascii=False, indent=2)
                    logger.info(f"ê·¸ë£¹ í¬ê¸° {group_size} Baseline ê²°ê³¼ ì„ì‹œ ì €ì¥ ì™„ë£Œ")
                
                # Baseline ëª¨ë¸ unload
                logger.info("Baseline ëª¨ë¸ unload ì¤‘...")
                del baseline_llm
                torch.cuda.empty_cache()
                logger.info("Baseline ëª¨ë¸ unload ì™„ë£Œ")
            
            # AggLLM ëª¨ë¸ ë¡œë“œ ë° AggLLM ê´€ë ¨ inference ì‹¤í–‰
            if aggllm_model_path and (aggllm_data or baseline_data):
                logger.info("=" * 60)
                logger.info("AggLLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                logger.info("=" * 60)
                aggllm_tokenizer = AutoTokenizer.from_pretrained(
                    cfg.model.base_model,
                    trust_remote_code=True
                )
                if aggllm_tokenizer.pad_token is None:
                    aggllm_tokenizer.pad_token = aggllm_tokenizer.eos_token
                
                # LoRA ë³‘í•©
                logger.info("LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ì¤‘...")
                base_model = AutoModelForCausalLM.from_pretrained(
                    cfg.model.base_model,
                    torch_dtype="auto",
                    device_map="auto",
                    trust_remote_code=True
                )
                peft_model = PeftModel.from_pretrained(base_model, aggllm_model_path)
                merged_model = peft_model.merge_and_unload()
                
                merged_model_path = cfg.paths.get("merged_model_cache_dir", None)
                if not merged_model_path:
                    merged_model_path = tempfile.mkdtemp(prefix="aggllm_merged_")
                
                os.makedirs(merged_model_path, exist_ok=True)
                config_path = os.path.join(merged_model_path, "config.json")
                
                if not os.path.exists(config_path):
                    merged_model.save_pretrained(merged_model_path, safe_serialization=True)
                    aggllm_tokenizer.save_pretrained(merged_model_path)
                
                del base_model, peft_model, merged_model
                torch.cuda.empty_cache()
                
                aggllm_llm = LLM(
                    model=merged_model_path,
                    tensor_parallel_size=1,
                    gpu_memory_utilization=eval_config.get("aggllm_gpu_memory_utilization", 0.9),
                    max_model_len=eval_config.get("max_model_len", eval_config.max_tokens + 8192),
                    dtype="bfloat16",
                    trust_remote_code=True,
                )
                logger.info("AggLLM ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
                # Baseline tokenizer ë‹¤ì‹œ ë¡œë“œ (Baseline â†’ AggLLM Aggregationì— í•„ìš”)
                if baseline_data:
                    baseline_tokenizer = AutoTokenizer.from_pretrained(
                        cfg.model.base_model,
                        trust_remote_code=True
                    )
                    if baseline_tokenizer.pad_token is None:
                        baseline_tokenizer.pad_token = baseline_tokenizer.eos_token
                
                # ê° ê·¸ë£¹ í¬ê¸°ë³„ë¡œ AggLLM ê´€ë ¨ inference ì‹¤í–‰
                for group_size in group_sizes:
                    max_groups = 4
                    
                    logger.info("=" * 60)
                    logger.info(f"AggLLM ê´€ë ¨ inference - ê·¸ë£¹ í¬ê¸° {group_size}")
                    logger.info("=" * 60)
                    
                    # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
                    group_results_dir = os.path.join(results_dir, f"group_size_{group_size}")
                    aggregation_path = os.path.join(
                        group_results_dir,
                        f"{dataset_safe_name}_aggregation_results.json"
                    )
                    
                    # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ (Baseline ê²°ê³¼ê°€ ìˆìœ¼ë©´)
                    if os.path.exists(aggregation_path):
                        with open(aggregation_path, 'r', encoding='utf-8') as f:
                            formatted_results = json.load(f)
                    else:
                        formatted_results = {
                            "dataset_name": dataset_path,
                            "group_size": group_size,
                            "baseline_to_baseline_aggregation": {},
                            "baseline_to_baseline_aggregation_without_confidence": {},
                            "baseline_to_aggllm_aggregation": {},
                            "baseline_to_aggllm_aggregation_without_confidence": {},
                            "aggllm_to_aggllm_aggregation": {}
                        }
                    
                    # ê²°ê³¼ ì´ˆê¸°í™” (AggLLM ê´€ë ¨ë§Œ)
                    aggregation_results = {
                        "baseline_to_aggllm_aggregation": [],
                        "baseline_to_aggllm_aggregation_without_confidence": [],
                        "aggllm_to_aggllm_aggregation": []
                    }
                    
                    # Baseline â†’ AggLLM Aggregation
                    if baseline_data:
                        logger.info(f"Baseline â†’ AggLLM Aggregation ìƒì„± ì¤‘ (ê·¸ë£¹ í¬ê¸°: {group_size})...")
                        
                        # ë¨¼ì € ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ í•„í„°ë§ ë° ê·¸ë£¹í™”
                        aggregation_requests = []
                        for problem_data in baseline_data["generated_solutions"]:
                            problem_text = problem_data["problem_text"]
                            solutions = problem_data["solutions"]
                            ground_truth = problem_data["ground_truth"]
                            problem_id = problem_data["problem_id"]
                            
                            # ì „ì²´ 16ê°œë¥¼ ë¨¼ì € í•„í„°ë§ (baseline tokenizer ì‚¬ìš©)
                            filtered_solutions, _ = filter_solutions_by_token_length(
                                solutions,
                                baseline_tokenizer,
                                max_tokens=eval_config.filter_max_tokens
                            )
                            
                            # ê·¸ë£¹í™” (group_sizeê°œì”©, ìµœëŒ€ max_groupsê°œ)
                            solution_groups = group_solutions_for_aggregation(
                                filtered_solutions, 
                                target_group_size=group_size,
                                max_groups=max_groups
                            )
                            
                            # ê° ê·¸ë£¹ì— ëŒ€í•´ ìš”ì²­ ìƒì„±
                            for group in solution_groups:
                                correct_count = count_correct_solutions(group, ground_truth, math_verifier)
                                group_sizes_str = ",".join(str(len(g)) for g in solution_groups)
                                
                                aggregation_requests.append({
                                    "problem_id": problem_id,
                                    "problem_text": problem_text,
                                    "ground_truth": ground_truth,
                                    "solutions": group,
                                    "correct_count": correct_count,
                                    "group_sizes": group_sizes_str
                                })
                        
                        if aggregation_requests:
                            logger.info(f"ì´ {len(aggregation_requests)}ê°œ aggregation ìš”ì²­ ì¤€ë¹„ ì™„ë£Œ")
                            
                            # ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (confidence í¬í•¨)
                            formatted_prompts, prompt_texts = prepare_aggregation_prompts_batch(
                                aggllm_tokenizer,
                                aggregation_requests,
                                aggregation_prompt_template,
                                enable_thinking,
                                include_confidence=True
                            )
                            
                            # SamplingParams ì„¤ì •
                            sampling_params = SamplingParams(
                                temperature=eval_config.temperature,
                                max_tokens=eval_config.max_tokens,
                                top_p=eval_config.get("top_p", 0.8),
                                top_k=eval_config.get("top_k", 20),
                                min_p=eval_config.get("min_p", 0.0),
                            )
                            
                            # ë°°ì¹˜ë¡œ ìƒì„±
                            logger.info("ë°°ì¹˜ ìƒì„± ì‹œì‘...")
                            outputs = aggllm_llm.generate(formatted_prompts, sampling_params)
                            logger.info("ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for idx, output in enumerate(outputs):
                                req = aggregation_requests[idx]
                                agg_text = output.outputs[0].text
                                parsed_content = extract_content(agg_text)
                                agg_answer = math_verifier.extract_final_answer_from_content(parsed_content)
                                
                                aggregation_results["baseline_to_aggllm_aggregation"].append({
                                    "problem_id": req["problem_id"],
                                    "problem_text": req["problem_text"],
                                    "ground_truth": req["ground_truth"],
                                    "prompt_text": prompt_texts[idx],
                                    "num_solutions": len(req["solutions"]),
                                    "correct_solutions_count": req["correct_count"],
                                    "generated_text": agg_text,
                                    "parsed_content": parsed_content,
                                    "final_answer": agg_answer,
                                    "is_correct": math_verifier.verify_answer(agg_answer, req["ground_truth"]) if do_evaluation else None,
                                    "solution_group_sizes": req["group_sizes"]
                                })
                            
                            # Baseline â†’ AggLLM Aggregation (without confidence)
                            logger.info("Baseline â†’ AggLLM Aggregation (without confidence) ìƒì„± ì¤‘...")
                            
                            # ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (confidence ì œì™¸)
                            formatted_prompts, prompt_texts = prepare_aggregation_prompts_batch(
                                aggllm_tokenizer,
                                aggregation_requests,
                                aggregation_prompt_template,
                                enable_thinking,
                                include_confidence=False
                            )
                            
                            # ë°°ì¹˜ë¡œ ìƒì„±
                            logger.info("ë°°ì¹˜ ìƒì„± ì‹œì‘...")
                            outputs = aggllm_llm.generate(formatted_prompts, sampling_params)
                            logger.info("ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for idx, output in enumerate(outputs):
                                req = aggregation_requests[idx]
                                agg_text = output.outputs[0].text
                                parsed_content = extract_content(agg_text)
                                agg_answer = math_verifier.extract_final_answer_from_content(parsed_content)
                                
                                aggregation_results["baseline_to_aggllm_aggregation_without_confidence"].append({
                                    "problem_id": req["problem_id"],
                                    "problem_text": req["problem_text"],
                                    "ground_truth": req["ground_truth"],
                                    "prompt_text": prompt_texts[idx],
                                    "num_solutions": len(req["solutions"]),
                                    "correct_solutions_count": req["correct_count"],
                                    "generated_text": agg_text,
                                    "parsed_content": parsed_content,
                                    "final_answer": agg_answer,
                                    "is_correct": math_verifier.verify_answer(agg_answer, req["ground_truth"]) if do_evaluation else None,
                                    "solution_group_sizes": req["group_sizes"]
                                })
                    
                    # AggLLM â†’ AggLLM Aggregation
                    if aggllm_data:
                        logger.info(f"AggLLM â†’ AggLLM Aggregation ìƒì„± ì¤‘ (ê·¸ë£¹ í¬ê¸°: {group_size})...")
                        
                        # ë¨¼ì € ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ í•„í„°ë§ ë° ê·¸ë£¹í™”
                        aggregation_requests = []
                        for problem_data in aggllm_data["generated_solutions"]:
                            problem_text = problem_data["problem_text"]
                            solutions = problem_data["solutions"]
                            ground_truth = problem_data["ground_truth"]
                            problem_id = problem_data["problem_id"]
                            
                            # ì „ì²´ 16ê°œë¥¼ ë¨¼ì € í•„í„°ë§
                            filtered_solutions, _ = filter_solutions_by_token_length(
                                solutions,
                                aggllm_tokenizer,
                                max_tokens=eval_config.filter_max_tokens
                            )
                            
                            # ê·¸ë£¹í™” (group_sizeê°œì”©, ìµœëŒ€ max_groupsê°œ)
                            solution_groups = group_solutions_for_aggregation(
                                filtered_solutions, 
                                target_group_size=group_size,
                                max_groups=max_groups
                            )
                            
                            # ê° ê·¸ë£¹ì— ëŒ€í•´ ìš”ì²­ ìƒì„±
                            for group in solution_groups:
                                correct_count = count_correct_solutions(group, ground_truth, math_verifier)
                                group_sizes_str = ",".join(str(len(g)) for g in solution_groups)
                                
                                aggregation_requests.append({
                                    "problem_id": problem_id,
                                    "problem_text": problem_text,
                                    "ground_truth": ground_truth,
                                    "solutions": group,
                                    "correct_count": correct_count,
                                    "group_sizes": group_sizes_str
                                })
                        
                        if aggregation_requests:
                            logger.info(f"ì´ {len(aggregation_requests)}ê°œ aggregation ìš”ì²­ ì¤€ë¹„ ì™„ë£Œ")
                            
                            # ë°°ì¹˜ë¡œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
                            formatted_prompts, prompt_texts = prepare_aggregation_prompts_batch(
                                aggllm_tokenizer,
                                aggregation_requests,
                                aggregation_prompt_template,
                                enable_thinking,
                                include_confidence=True
                            )
                            
                            # SamplingParams ì„¤ì •
                            sampling_params = SamplingParams(
                                temperature=eval_config.temperature,
                                max_tokens=eval_config.max_tokens,
                                top_p=eval_config.get("top_p", 0.8),
                                top_k=eval_config.get("top_k", 20),
                                min_p=eval_config.get("min_p", 0.0),
                            )
                            
                            # ë°°ì¹˜ë¡œ ìƒì„±
                            logger.info("ë°°ì¹˜ ìƒì„± ì‹œì‘...")
                            outputs = aggllm_llm.generate(formatted_prompts, sampling_params)
                            logger.info("ë°°ì¹˜ ìƒì„± ì™„ë£Œ")
                            
                            # ê²°ê³¼ ì²˜ë¦¬
                            for idx, output in enumerate(outputs):
                                req = aggregation_requests[idx]
                                agg_text = output.outputs[0].text
                                parsed_content = extract_content(agg_text)
                                agg_answer = math_verifier.extract_final_answer_from_content(parsed_content)
                                
                                aggregation_results["aggllm_to_aggllm_aggregation"].append({
                                    "problem_id": req["problem_id"],
                                    "problem_text": req["problem_text"],
                                    "ground_truth": req["ground_truth"],
                                    "prompt_text": prompt_texts[idx],
                                    "num_solutions": len(req["solutions"]),
                                    "correct_solutions_count": req["correct_count"],
                                    "generated_text": agg_text,
                                    "parsed_content": parsed_content,
                                    "final_answer": agg_answer,
                                    "is_correct": math_verifier.verify_answer(agg_answer, req["ground_truth"]) if do_evaluation else None,
                                    "solution_group_sizes": req["group_sizes"]
                                })
                    
                    # AggLLM ê´€ë ¨ ê²°ê³¼ë¥¼ ê¸°ì¡´ ê²°ê³¼ì— ì¶”ê°€
                    for key in ["baseline_to_aggllm_aggregation", "baseline_to_aggllm_aggregation_without_confidence",
                               "aggllm_to_aggllm_aggregation"]:
                        results = aggregation_results.get(key, [])
                        if results:
                            grouped = group_results_by_problem_id(results)
                            formatted_results[key] = grouped
                    
                    # Evaluation ë‹¨ê³„ (ì´ ê·¸ë£¹ í¬ê¸°ì— ëŒ€í•´)
                    if do_evaluation:
                        # í‰ê°€ ìˆ˜í–‰ (is_correctê°€ Noneì¸ ê²½ìš°ì—ë§Œ)
                        for key in ["baseline_to_baseline_aggregation", "baseline_to_baseline_aggregation_without_confidence",
                                   "baseline_to_aggllm_aggregation", "baseline_to_aggllm_aggregation_without_confidence",
                                   "aggllm_to_aggllm_aggregation"]:
                            results_dict = formatted_results.get(key, {})
                            if isinstance(results_dict, dict):
                                for problem_id, problem_data in results_dict.items():
                                    prompts = problem_data.get("prompts", [])
                                    for prompt in prompts:
                                        if prompt.get("is_correct") is None:
                                            final_answer = prompt.get("final_answer", "")
                                            if final_answer:
                                                prompt["is_correct"] = math_verifier.verify_answer(
                                                    final_answer,
                                                    problem_data["ground_truth"]
                                                )
                    
                    # ì •í™•ë„ ê³„ì‚° ë° ìµœì¢… ì €ì¥
                    summary = {}
                    for key in ["baseline_to_baseline_aggregation", "baseline_to_baseline_aggregation_without_confidence",
                               "baseline_to_aggllm_aggregation", "baseline_to_aggllm_aggregation_without_confidence",
                               "aggllm_to_aggllm_aggregation"]:
                        results_dict = formatted_results.get(key, {})
                        if isinstance(results_dict, dict):
                            total = 0
                            correct = 0
                            for problem_id, problem_data in results_dict.items():
                                prompts = problem_data.get("prompts", [])
                                for prompt in prompts:
                                    total += 1
                                    if prompt.get("is_correct", False):
                                        correct += 1
                            summary[key] = {
                                "correct": correct,
                                "total": total,
                                "accuracy": correct / total if total > 0 else 0.0
                            }
                    
                    formatted_results["summary"] = summary
                    
                    with open(aggregation_path, 'w', encoding='utf-8') as f:
                        json.dump(formatted_results, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"ê·¸ë£¹ í¬ê¸° {group_size} Aggregation ê²°ê³¼ ì €ì¥: {aggregation_path}")
                    logger.info(f"ê·¸ë£¹ í¬ê¸° {group_size} ìš”ì•½: {summary}")
                    all_group_results[group_size] = summary
                
                # AggLLM ëª¨ë¸ unload
                logger.info("AggLLM ëª¨ë¸ unload ì¤‘...")
                del aggllm_llm
                torch.cuda.empty_cache()
                logger.info("AggLLM ëª¨ë¸ unload ì™„ë£Œ")
        
        # Evaluation ë‹¨ê³„ (ì €ì¥ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‰ê°€) - ê¸°ì¡´ í˜•ì‹ ì§€ì›
        if do_evaluation and not do_generation:
            logger.info("=" * 60)
            logger.info("Evaluation ë‹¨ê³„ ì‹œì‘")
            logger.info("=" * 60)
            
            # ì €ì¥ëœ aggregation ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë¡œë“œ
            aggregation_path = os.path.join(
                results_dir,
                f"{dataset_safe_name}_aggregation_results.json"
            )
            
            if os.path.exists(aggregation_path) and not do_generation:
                # Generation ì—†ì´ Evaluationë§Œ í•˜ëŠ” ê²½ìš°
                with open(aggregation_path, 'r', encoding='utf-8') as f:
                    aggregation_results = json.load(f)
                logger.info(f"ì €ì¥ëœ Aggregation ê²°ê³¼ ë¡œë“œ: {aggregation_path}")
            
            # ê¸°ì¡´ í˜•ì‹ì˜ ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ í‰ê°€ (ê·¸ë£¹ í¬ê¸°ë³„ë¡œ)
            for group_size in group_sizes:
                group_results_dir = os.path.join(results_dir, f"group_size_{group_size}")
                aggregation_path = os.path.join(
                    group_results_dir,
                    f"{dataset_safe_name}_aggregation_results.json"
                )
                
                if os.path.exists(aggregation_path):
                    with open(aggregation_path, 'r', encoding='utf-8') as f:
                        formatted_results = json.load(f)
                    
                    # í‰ê°€ ìˆ˜í–‰ (is_correctê°€ Noneì¸ ê²½ìš°ì—ë§Œ)
                    for key in ["baseline_to_baseline_aggregation", "baseline_to_baseline_aggregation_without_confidence",
                               "baseline_to_aggllm_aggregation", "baseline_to_aggllm_aggregation_without_confidence",
                               "aggllm_to_aggllm_aggregation"]:
                        results_dict = formatted_results.get(key, {})
                        if isinstance(results_dict, dict):
                            for problem_id, problem_data in results_dict.items():
                                prompts = problem_data.get("prompts", [])
                                for prompt in prompts:
                                    if prompt.get("is_correct") is None:
                                        final_answer = prompt.get("final_answer", "")
                                        if final_answer:
                                            prompt["is_correct"] = math_verifier.verify_answer(
                                                final_answer,
                                                problem_data["ground_truth"]
                                            )
                    
                    # ì •í™•ë„ ì¬ê³„ì‚°
                    summary = {}
                    for key in ["baseline_to_baseline_aggregation", "baseline_to_baseline_aggregation_without_confidence",
                               "baseline_to_aggllm_aggregation", "baseline_to_aggllm_aggregation_without_confidence",
                               "aggllm_to_aggllm_aggregation"]:
                        results_dict = formatted_results.get(key, {})
                        if isinstance(results_dict, dict):
                            total = 0
                            correct = 0
                            for problem_id, problem_data in results_dict.items():
                                prompts = problem_data.get("prompts", [])
                                for prompt in prompts:
                                    total += 1
                                    if prompt.get("is_correct", False):
                                        correct += 1
                            summary[key] = {
                                "correct": correct,
                                "total": total,
                                "accuracy": correct / total if total > 0 else 0.0
                            }
                    
                    formatted_results["summary"] = summary
                    
                    with open(aggregation_path, 'w', encoding='utf-8') as f:
                        json.dump(formatted_results, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"ê·¸ë£¹ í¬ê¸° {group_size} í‰ê°€ ì™„ë£Œ ë° ì €ì¥: {aggregation_path}")
    
    logger.info("=" * 60)
    logger.info("âœ… Stage 4-3: Aggregation í‰ê°€ ì™„ë£Œ")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()

