"""
Stage 4: ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""
import os
import sys
from pathlib import Path
import hydra
from omegaconf import DictConfig
import logging
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from src.evaluation.benchmark import BenchmarkEvaluator
from src.data.confidence import ConfidenceCalculator
from src.evaluation.math_verifier import MathVerifier
from src.utils.logging import setup_logging

logger = logging.getLogger(__name__)


@hydra.main(version_base=None, config_path="../config", config_name="config")
def main(cfg: DictConfig) -> None:
    """Stage 4 ë©”ì¸ í•¨ìˆ˜"""
    
    # ë¡œê¹… ì„¤ì •
    log_file = os.path.join(cfg.paths.log_dir, "stage4_evaluate.log")
    setup_logging(
        log_level=cfg.experiment.get("log_level", "INFO"),
        log_file=log_file,
        wandb_enabled=cfg.experiment.wandb.enabled,
        wandb_project=cfg.experiment.wandb.project,
        wandb_tags=cfg.experiment.wandb.tags
    )
    
    logger.info("ğŸš€ Stage 4: ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹œì‘")
    logger.info(f"ì„¤ì •: {cfg.evaluation.benchmarks}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(cfg.paths.output_dir, exist_ok=True)
    os.makedirs(os.path.join(cfg.paths.output_dir, "results"), exist_ok=True)
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = os.path.join(cfg.paths.model_dir, "checkpoint-final")
    if not os.path.exists(model_path):
        logger.error(f"í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        logger.error("ë¨¼ì € Stage 3ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”: python scripts/stage3_train.py")
        return
    
    # í‰ê°€ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
    logger.info("í‰ê°€ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì¤‘...")
    
    confidence_calculator = ConfidenceCalculator(
        group_size=cfg.data.raw_dataset.confidence.group_size
    )
    
    math_verifier = MathVerifier(
        timeout=cfg.evaluation.benchmarks.evaluation.timeout
    )
    
    # ë²¤ì¹˜ë§ˆí¬ í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = BenchmarkEvaluator(
        model_path=model_path,
        base_model_name=cfg.model.base_model,
        confidence_calculator=confidence_calculator,
        math_verifier=math_verifier,
        num_candidates=cfg.evaluation.benchmarks.evaluation.num_candidates,
        temperature=cfg.evaluation.benchmarks.evaluation.temperature,
        max_tokens=cfg.evaluation.benchmarks.evaluation.max_tokens
    )
    
    # ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰
    logger.info("ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹¤í–‰ ì¤‘...")
    
    benchmark_configs = cfg.evaluation.benchmarks.datasets
    output_dir = os.path.join(cfg.paths.output_dir, "results")
    
    summary_results = evaluator.evaluate_all_benchmarks(
        benchmark_configs=benchmark_configs,
        output_dir=output_dir
    )
    
    # ê²°ê³¼ ì¶œë ¥
    logger.info("=" * 50)
    logger.info("ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    logger.info("=" * 50)
    
    for benchmark_name, results in summary_results["benchmarks"].items():
        logger.info(f"{benchmark_name}:")
        logger.info(f"  - Pass@1: {results['pass_at_1']:.3f}")
        logger.info(f"  - Pass@5: {results['pass_at_5']:.3f}")
        logger.info(f"  - Pass@10: {results['pass_at_10']:.3f}")
        logger.info(f"  - ì‹ ë¢°ë„ ìƒê´€ê´€ê³„: {results['confidence_correlation']:.3f}")
        logger.info(f"  - ì´ ë¬¸ì œ ìˆ˜: {results['total_problems']}")
        logger.info(f"  - ì •ë‹µ ìˆ˜: {results['correct_predictions']}")
        logger.info("")
    
    logger.info("ì „ì²´ í‰ê· :")
    logger.info(f"  - Pass@1: {summary_results['overall_pass_at_1']:.3f}")
    logger.info(f"  - Pass@5: {summary_results['overall_pass_at_5']:.3f}")
    logger.info(f"  - Pass@10: {summary_results['overall_pass_at_10']:.3f}")
    logger.info(f"  - ì‹ ë¢°ë„ ìƒê´€ê´€ê³„: {summary_results['overall_confidence_correlation']:.3f}")
    
    logger.info("âœ… Stage 4 ì™„ë£Œ")
    logger.info(f"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main()

