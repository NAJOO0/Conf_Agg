/mnt/data1/projects/Conf_Agg/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info
flash-attn package: 2.7.4.post1
INFO 11-14 06:47:21 [__init__.py:216] Automatically detected platform cuda.
vLLM uses FlashAttentionBackend
Backend location: /mnt/data1/projects/Conf_Agg/.venv/lib/python3.12/site-packages/vllm/attention/backends/flash_attn.py
âœ“ Using flash_attn_varlen_func (Flash Attention 2)
