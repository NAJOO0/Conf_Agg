"""
vLLM ê°„ë‹¨ í…ŒìŠ¤íŠ¸ - ìƒ˜í”Œ 4ê°œ, GPU ì „ì²´ ì‚¬ìš©, logprob í™•ì¸
"""
import torch
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import os

def main():
    # multiprocessing ì´ìŠˆ ë°©ì§€
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    
    # GPU í™•ì¸
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    
    # ìƒ˜í”Œ ë¬¸ì œ 4ê°œ (ê°„ë‹¨í•œ ìˆ˜í•™ ë¬¸ì œ)
    sample_problems = [
        "What is 2 + 2?",
        "What is 5 * 3?",
        "What is 10 - 4?",
        "What is 12 / 3?",
    ]
    
    # vLLM ëª¨ë¸ ë¡œë“œ (ë‹¨ì¼ GPUë¡œ ì‹¤í–‰í•˜ì—¬ multiprocessing ë¬¸ì œ íšŒí”¼)
    model_name = "Qwen/Qwen3-1.7B"
    print(f"\nğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    llm = LLM(
        model=model_name,
        tensor_parallel_size=1,  # ë‹¨ì¼ GPU (multiprocessing ì´ìŠˆ íšŒí”¼)
        gpu_memory_utilization=0.9,
        max_model_len=4096,
        dtype="float16",
        trust_remote_code=True,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    
    # í† í°í™” ë° í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompts = []
    for problem in sample_problems:
        messages = [{"role": "user", "content": problem}]
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        prompts.append(prompt)
    
    # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì • (logprob=5ëŠ” top-5 logprob ì €ì¥)
    sampling_params = SamplingParams(
        temperature=0.7,
        max_tokens=32768,
        logprobs=5,  # top-5 logprob ì €ì¥
    )
    
    # ì¶”ë¡  ì‹¤í–‰
    print("ğŸš€ ì¶”ë¡  ì‹œì‘...")
    outputs = llm.generate(prompts, sampling_params)
    print(f"âœ… ì¶”ë¡  ì™„ë£Œ: {len(outputs)}ê°œ ê²°ê³¼\n")
    
    # ê²°ê³¼ í™•ì¸
    print("=" * 80)
    print("ê²°ê³¼ ìƒì„¸ ë¶„ì„")
    print("=" * 80)
    
    for idx, output in enumerate(outputs):
        print(f"\n[ë¬¸ì œ {idx + 1}] {sample_problems[idx]}")
        print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {output.outputs[0].text}")
        
        # logprob êµ¬ì¡° ë¶„ì„
        if hasattr(output.outputs[0], 'logprobs') and output.outputs[0].logprobs:
            logprobs = output.outputs[0].logprobs
            print(f"\nğŸ“Š Logprobs êµ¬ì¡°:")
            print(f"   - Logprobs íƒ€ì…: {type(logprobs)}")
            print(f"   - ì´ í† í° ìˆ˜: {len(logprobs)}ê°œ")
            
            # ì²« ë²ˆì§¸ í† í°ì˜ logprob ìƒì„¸ í™•ì¸
            if len(logprobs) > 0:
                first_token_logprobs = logprobs[0]
                print(f"\n   [ì²« ë²ˆì§¸ í† í°ì˜ logprob]")
                print(f"   - íƒ€ì…: {type(first_token_logprobs)}")
                
                if isinstance(first_token_logprobs, dict):
                    print(f"   - í•­ëª© ìˆ˜: {len(first_token_logprobs)}ê°œ")
                    
                    # top-k í™•ì¸
                    count = 0
                    for token_id, logprob_entry in list(first_token_logprobs.items())[:10]:
                        count += 1
                        if isinstance(logprob_entry, dict):
                            logprob_value = logprob_entry.get('logprob', 'N/A')
                            token = logprob_entry.get('decoded_token', 'N/A')
                            print(f"   - Token ID {token_id}: logprob={logprob_value:.4f}, token={token}")
                        else:
                            print(f"   - Token ID {token_id}: {logprob_entry}")
                    
                    print(f"\n   âœ… ì²« ë²ˆì§¸ í† í°ì—ì„œ {len(first_token_logprobs)}ê°œì˜ logprob ì €ì¥ë¨")
                    
            # ë§ˆì§€ë§‰ í† í°ì˜ logprob í™•ì¸
            if len(logprobs) > 1:
                last_token_logprobs = logprobs[-1]
                if isinstance(last_token_logprobs, dict):
                    print(f"\n   [ë§ˆì§€ë§‰ í† í°ì˜ logprob]")
                    print(f"   - í•­ëª© ìˆ˜: {len(last_token_logprobs)}ê°œ")
            
            # í† í°ë³„ logprob ê°œìˆ˜ ìš”ì•½
            token_logprob_counts = []
            for i, token_logprobs in enumerate(logprobs):
                if isinstance(token_logprobs, dict):
                    token_logprob_counts.append(len(token_logprobs))
            
            if token_logprob_counts:
                print(f"\n   ğŸ“ˆ í† í°ë³„ logprob ê°œìˆ˜:")
                print(f"   - í‰ê· : {sum(token_logprob_counts) / len(token_logprob_counts):.1f}ê°œ")
                print(f"   - ìµœì†Œ: {min(token_logprob_counts)}ê°œ")
                print(f"   - ìµœëŒ€: {max(token_logprob_counts)}ê°œ")
            
            # ìƒ˜í”Œ ì¶œë ¥: ì²˜ìŒ 3ê°œ í† í°ì˜ logprob ê°œìˆ˜
            print(f"\n   ğŸ“‹ ì²˜ìŒ 3ê°œ í† í°ì˜ logprob ê°œìˆ˜:")
            for i in range(min(3, len(logprobs))):
                if isinstance(logprobs[i], dict):
                    print(f"   - í† í° {i+1}: {len(logprobs[i])}ê°œ logprob")
        else:
            print("   âš ï¸  logprobì´ ì—†ìŠµë‹ˆë‹¤")
    
    print("\n" + "=" * 80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 80)

if __name__ == '__main__':
    main()